@inproceedings{Grover2016,
   abstract = {Prediction tasks over nodes and edges in networks require careful effort in engineering features used by learning algorithms. Recent research in the broader field of representation learning has led to significant progress in automating prediction by learning the features themselves. However, present feature learning approaches are not expressive enough to capture the diversity of connectivity patterns observed in networks. Here we propose node2vec, an algorithmic framework for learning continuous feature representations for nodes in networks. In node2vec, we learn a mapping of nodes to a low-dimensional space of features that maximizes the likelihood of preserving network neighborhoods of nodes. We define a flexible notion of a node's network neighborhood and design a biased random walk procedure, which efficiently explores diverse neighborhoods. Our algorithm generalizes prior work which is based on rigid notions of network neighborhoods, and we argue that the added flexibility in exploring neighborhoods is the key to learning richer representations.We demonstrate the efficacy of node2vec over existing state-of-the-art techniques on multi-label classification and link prediction in several real-world networks from diverse domains. Taken together, our work represents a new way for efficiently learning state-of-the-art task-independent representations in complex networks.},
   author = {Aditya Grover and Jure Leskovec},
   city = {New York, NY, USA},
   doi = {10.1145/2939672.2939754},
   isbn = {9781450342322},
   booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
   keywords = {feature learning,graph representations,information networks,node embeddings},
   month = {8},
   pages = {855-864},
   publisher = {ACM},
   title = {Node2vec: Scalable Feature Learning for Networks},
   url = {https://dl.acm.org/doi/10.1145/2939672.2939754},
   year = {2016},
}


@misc{google_word2vec,
    title = {Google code archive - long-term storage for google code project hosting.},
    url = {https://code.google.com/archive/p/word2vec/},
    publisher = {Google}
}

@inproceedings{Kipf2017,
   author = {Thomas N. Kipf and Max Welling},
   booktitle = {International Conference on Learning Representations},
   month = {9},
   title = {Semi-Supervised Classification with Graph Convolutional Networks},
   url = {https://openreview.net/forum?id=SJU4ayYgl},
   year = {2017},
}

@inproceedings{Defferrard2016,
   abstract = {In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words' embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs.},
   author = {Michaël Defferrard and Xavier Bresson and Pierre Vandergheynst},
   city = {Red Hook, NY, USA},
   isbn = {9781510838819},
   booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
   pages = {3844-3852},
   publisher = {Curran Associates Inc.},
   title = {Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering},
   year = {2016},
}

@inproceedings{Morris2019,
   abstract = {<p>In recent years, graph neural networks (GNNs) have emerged as a powerful neural architecture to learn vector representations of nodes and graphs in a supervised, end-to-end fashion. Up to now, GNNs have only been evaluated empirically—showing promising results. The following work investigates GNNs from a theoretical point of view and relates them to the 1-dimensional Weisfeiler-Leman graph isomorphism heuristic (1-WL). We show that GNNs have the same expressiveness as the 1-WL in terms of distinguishing non-isomorphic (sub-)graphs. Hence, both algorithms also have the same shortcomings. Based on this, we propose a generalization of GNNs, so-called k-dimensional GNNs (k-GNNs), which can take higher-order graph structures at multiple scales into account. These higher-order structures play an essential role in the characterization of social networks and molecule graphs. Our experimental evaluation confirms our theoretical findings as well as confirms that higher-order information is useful in the task of graph classification and regression.</p>},
   author = {Christopher Morris and Martin Ritzert and Matthias Fey and William L. Hamilton and Jan Eric Lenssen and Gaurav Rattan and Martin Grohe},
   doi = {10.1609/aaai.v33i01.33014602},
   issn = {2374-3468},
   booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
   month = {7},
   pages = {4602-4609},
   title = {Weisfeiler and Leman Go Neural: Higher-Order Graph Neural Networks},
   volume = {33},
   url = {https://aaai.org/ojs/index.php/AAAI/article/view/4384},
   year = {2019},
}

@inproceedings{GAT2018,
   abstract = {We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).},
   author = {Petar Veličković and Guillem Cucurull and Arantxa Casanova and Adriana Romero and Pietro Liò and Yoshua Bengio},
   booktitle = {The 6th International Conference on Learning Representations},
   month = {10},
   title = {Graph Attention Networks},
   url = {https://openreview.net/forum?id=rJXMpikCZ},
   year = {2018},
}

@inproceedings{Brody2022,
   abstract = {Graph Attention Networks (GATs) are one of the most popular GNN architectures and are considered as the state-of-the-art architecture for representation learning with graphs. In GAT, every node attends to its neighbors given its own representation as the query. However, in this paper we show that GAT computes a very limited kind of attention: the ranking of the attention scores is unconditioned on the query node. We formally define this restricted kind of attention as static attention and distinguish it from a strictly more expressive dynamic attention. Because GATs use a static attention mechanism, there are simple graph problems that GAT cannot express: in a controlled problem, we show that static attention hinders GAT from even fitting the training data. To remove this limitation, we introduce a simple fix by modifying the order of operations and propose GATv2: a dynamic graph attention variant that is strictly more expressive than GAT. We perform an extensive evaluation and show that GATv2 outperforms GAT across 12 OGB and other benchmarks while we match their parametric costs. Our code is available at https://github.com/tech-srl/how_attentive_are_ gats.},
   author = {Shaked Brody and Uri Alon and Eran Yahav},
   booktitle = {International Conference on Learning Representations},
   title = {HOW ATTENTIVE ARE GRAPH ATTENTION NETWORKS?},
   url = {https://openreview.net/forum?id=F72ximsx7C1},
   year = {2022},
}

@inproceedings{Li2016,
   author = {Yujia Li and Daniel Tarlow and Marc Brockschmidt and Richard Zemel},
   booktitle = {International Conference on Learning Representations},
   month = {11},
   title = {Gated Graph Sequence Neural Networks},
   url = {https://arxiv.org/abs/1511.05493},
   year = {2016},
}

@article{Wang2019,
   abstract = {<p> Point clouds provide a flexible geometric representation suitable for countless applications in computer graphics; they also comprise the raw output of most 3D data acquisition devices. While hand-designed features on point clouds have long been proposed in graphics and vision, however, the recent overwhelming success of convolutional neural networks (CNNs) for image analysis suggests the value of adapting insight from CNN to the point cloud world. Point clouds inherently lack topological information, so designing a model to recover topology can enrich the representation power of point clouds. To this end, we propose a new neural network module dubbed <italic>EdgeConv</italic> suitable for CNN-based high-level tasks on point clouds, including classification and segmentation. EdgeConv acts on graphs dynamically computed in each layer of the network. It is differentiable and can be plugged into existing architectures. Compared to existing modules operating in extrinsic space or treating each point independently, EdgeConv has several appealing properties: It incorporates local neighborhood information; it can be stacked applied to learn global shape properties; and in multi-layer systems affinity in feature space captures semantic characteristics over potentially long distances in the original embedding. We show the performance of our model on standard benchmarks, including ModelNet40, ShapeNetPart, and S3DIS. </p>},
   author = {Yue Wang and Yongbin Sun and Ziwei Liu and Sanjay E. Sarma and Michael M. Bronstein and Justin M. Solomon},
   doi = {10.1145/3326362},
   issn = {0730-0301},
   issue = {5},
   journal = {ACM Transactions on Graphics},
   keywords = {Classification,Point cloud,Segmentation},
   month = {11},
   pages = {1-12},
   publisher = {Association for Computing Machinery},
   title = {Dynamic Graph CNN for Learning on Point Clouds},
   volume = {38},
   url = {https://dl.acm.org/doi/10.1145/3326362},
   year = {2019},
}

@inproceedings{Xu2019,
   abstract = {Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.},
   author = {Keyulu Xu and Weihua Hu and Jure Leskovec and Stefanie Jegelka},
   booktitle = {International Conference on Learning Representations},
   title = {HOW POWERFUL ARE GRAPH NEURAL NETWORKS?},
   url = {https://openreview.net/forum?id=ryGs6iA5Km},
   year = {2019},
}

@article{Xie2018,
   author = {Tian Xie and Jeffrey C. Grossman},
   doi = {10.1103/PhysRevLett.120.145301},
   issn = {0031-9007},
   issue = {14},
   journal = {Physical Review Letters},
   month = {4},
   pages = {145301},
   title = {Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable Prediction of Material Properties},
   volume = {120},
   url = {https://link.aps.org/doi/10.1103/PhysRevLett.120.145301},
   year = {2018},
}

@inproceedings{Kipf2016,
   abstract = {We introduce the variational graph auto-encoder (VGAE), a framework for unsupervised learning on graph-structured data based on the variational auto-encoder (VAE). This model makes use of latent variables and is capable of learning interpretable latent representations for undirected graphs. We demonstrate this model using a graph convolutional network (GCN) encoder and a simple inner product decoder. Our model achieves competitive results on a link prediction task in citation networks. In contrast to most existing models for unsupervised learning on graph-structured data and link prediction, our model can naturally incorporate node features, which significantly improves predictive performance on a number of benchmark datasets.},
   author = {Thomas N. Kipf and Max Welling},
   doi = {10.48550/arxiv.1611.07308},
   booktitle = {Neural Information Processing Systems},
   month = {11},
   title = {Variational Graph Auto-Encoders},
   url = {http://arxiv.org/abs/1611.07308},
   year = {2016},
}

@inproceedings{Zaheer2017,
   author = {Manzil Zaheer and Satwik Kottur and Siamak Ravanbakhsh and Barnabas Poczos and Russ R Salakhutdinov and Alexander J Smola},
   editor = {I Guyon and U V Luxburg and S Bengio and H Wallach and R Fergus and S Vishwanathan and R Garnett},
   booktitle = {Advances in Neural Information Processing Systems},
   publisher = {Curran Associates, Inc.},
   title = {Deep Sets},
   volume = {30},
   url = {https://proceedings.neurips.cc/paper/2017/file/f22e4747da1aa27e363d86d40ff442fe-Paper.pdf},
   year = {2017},
}

@inproceedings{Gao2019,
   abstract = {We consider the problem of representation learning for graph data. Convolutional neural networks can naturally operate on images, but have significant challenges in dealing with graph data. Given images are special cases of graphs with nodes lie on 2D lattices, graph embedding tasks have a natural correspondence with image pixel-wise prediction tasks such as segmentation. While encoder-decoder architectures like U-Nets have been successfully applied on many image pixel-wise prediction tasks, similar methods are lacking for graph data. This is due to the fact that pooling and up-sampling operations are not natural on graph data. To address these challenges, we propose novel graph pooling (gPool) and un-pooling (gUnpool) operations in this work. The gPool layer adaptively selects some nodes to form a smaller graph based on their scalar projection values on a trainable projection vector. We further propose the gUnpool layer as the inverse operation of the gPool layer. The gUnpool layer restores the graph into its original structure using the position information of nodes selected in the corresponding gPool layer. Based on our proposed gPool and gUnpool layers, we develop an encoder-decoder model on graph, known as the graph U-Nets. Our experimental results on node classification and graph classification tasks demonstrate that our methods achieve consistently better performance than previous models.},
   author = {Hongyang Gao and Shuiwang Ji},
   booktitle = {International Conference on Machine Learning},
   pages = {2083-2092},
   title = {Graph U-Nets},
   year = {2019},
}
