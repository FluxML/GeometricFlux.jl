@inproceedings{Grover2016,
   abstract = {Prediction tasks over nodes and edges in networks require careful effort in engineering features used by learning algorithms. Recent research in the broader field of representation learning has led to significant progress in automating prediction by learning the features themselves. However, present feature learning approaches are not expressive enough to capture the diversity of connectivity patterns observed in networks. Here we propose node2vec, an algorithmic framework for learning continuous feature representations for nodes in networks. In node2vec, we learn a mapping of nodes to a low-dimensional space of features that maximizes the likelihood of preserving network neighborhoods of nodes. We define a flexible notion of a node's network neighborhood and design a biased random walk procedure, which efficiently explores diverse neighborhoods. Our algorithm generalizes prior work which is based on rigid notions of network neighborhoods, and we argue that the added flexibility in exploring neighborhoods is the key to learning richer representations.We demonstrate the efficacy of node2vec over existing state-of-the-art techniques on multi-label classification and link prediction in several real-world networks from diverse domains. Taken together, our work represents a new way for efficiently learning state-of-the-art task-independent representations in complex networks.},
   author = {Aditya Grover and Jure Leskovec},
   city = {New York, NY, USA},
   doi = {10.1145/2939672.2939754},
   isbn = {9781450342322},
   booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
   keywords = {feature learning,graph representations,information networks,node embeddings},
   month = {8},
   pages = {855-864},
   publisher = {ACM},
   title = {Node2vec: Scalable Feature Learning for Networks},
   url = {https://dl.acm.org/doi/10.1145/2939672.2939754},
   year = {2016},
}


@misc{google_word2vec,
    title = {Google code archive - long-term storage for google code project hosting.},
    url = {https://code.google.com/archive/p/word2vec/},
    publisher = {Google}
}

@inproceedings{Kipf2017,
   author = {Thomas N. Kipf and Max Welling},
   booktitle = {International Conference on Learning Representations},
   month = {9},
   title = {Semi-Supervised Classification with Graph Convolutional Networks},
   url = {https://openreview.net/forum?id=SJU4ayYgl},
   year = {2017},
}

@inproceedings{Defferrard2016,
   abstract = {In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words' embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs.},
   author = {Michaël Defferrard and Xavier Bresson and Pierre Vandergheynst},
   city = {Red Hook, NY, USA},
   isbn = {9781510838819},
   booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
   pages = {3844-3852},
   publisher = {Curran Associates Inc.},
   title = {Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering},
   year = {2016},
}

@inproceedings{Morris2019,
   abstract = {<p>In recent years, graph neural networks (GNNs) have emerged as a powerful neural architecture to learn vector representations of nodes and graphs in a supervised, end-to-end fashion. Up to now, GNNs have only been evaluated empirically—showing promising results. The following work investigates GNNs from a theoretical point of view and relates them to the 1-dimensional Weisfeiler-Leman graph isomorphism heuristic (1-WL). We show that GNNs have the same expressiveness as the 1-WL in terms of distinguishing non-isomorphic (sub-)graphs. Hence, both algorithms also have the same shortcomings. Based on this, we propose a generalization of GNNs, so-called k-dimensional GNNs (k-GNNs), which can take higher-order graph structures at multiple scales into account. These higher-order structures play an essential role in the characterization of social networks and molecule graphs. Our experimental evaluation confirms our theoretical findings as well as confirms that higher-order information is useful in the task of graph classification and regression.</p>},
   author = {Christopher Morris and Martin Ritzert and Matthias Fey and William L. Hamilton and Jan Eric Lenssen and Gaurav Rattan and Martin Grohe},
   doi = {10.1609/aaai.v33i01.33014602},
   issn = {2374-3468},
   booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
   month = {7},
   pages = {4602-4609},
   title = {Weisfeiler and Leman Go Neural: Higher-Order Graph Neural Networks},
   volume = {33},
   url = {https://aaai.org/ojs/index.php/AAAI/article/view/4384},
   year = {2019},
}

@inproceedings{GAT2018,
   abstract = {We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).},
   author = {Petar Veličković and Guillem Cucurull and Arantxa Casanova and Adriana Romero and Pietro Liò and Yoshua Bengio},
   booktitle = {The 6th International Conference on Learning Representations},
   month = {10},
   title = {Graph Attention Networks},
   url = {https://openreview.net/forum?id=rJXMpikCZ},
   year = {2018},
}

@inproceedings{Brody2022,
   abstract = {Graph Attention Networks (GATs) are one of the most popular GNN architectures and are considered as the state-of-the-art architecture for representation learning with graphs. In GAT, every node attends to its neighbors given its own representation as the query. However, in this paper we show that GAT computes a very limited kind of attention: the ranking of the attention scores is unconditioned on the query node. We formally define this restricted kind of attention as static attention and distinguish it from a strictly more expressive dynamic attention. Because GATs use a static attention mechanism, there are simple graph problems that GAT cannot express: in a controlled problem, we show that static attention hinders GAT from even fitting the training data. To remove this limitation, we introduce a simple fix by modifying the order of operations and propose GATv2: a dynamic graph attention variant that is strictly more expressive than GAT. We perform an extensive evaluation and show that GATv2 outperforms GAT across 12 OGB and other benchmarks while we match their parametric costs. Our code is available at https://github.com/tech-srl/how_attentive_are_ gats.},
   author = {Shaked Brody and Uri Alon and Eran Yahav},
   booktitle = {International Conference on Learning Representations},
   title = {HOW ATTENTIVE ARE GRAPH ATTENTION NETWORKS?},
   url = {https://openreview.net/forum?id=F72ximsx7C1},
   year = {2022},
}

@inproceedings{Li2016,
   author = {Yujia Li and Daniel Tarlow and Marc Brockschmidt and Richard Zemel},
   booktitle = {International Conference on Learning Representations},
   month = {11},
   title = {Gated Graph Sequence Neural Networks},
   url = {https://arxiv.org/abs/1511.05493},
   year = {2016},
}

@article{Wang2019,
   abstract = {<p> Point clouds provide a flexible geometric representation suitable for countless applications in computer graphics; they also comprise the raw output of most 3D data acquisition devices. While hand-designed features on point clouds have long been proposed in graphics and vision, however, the recent overwhelming success of convolutional neural networks (CNNs) for image analysis suggests the value of adapting insight from CNN to the point cloud world. Point clouds inherently lack topological information, so designing a model to recover topology can enrich the representation power of point clouds. To this end, we propose a new neural network module dubbed <italic>EdgeConv</italic> suitable for CNN-based high-level tasks on point clouds, including classification and segmentation. EdgeConv acts on graphs dynamically computed in each layer of the network. It is differentiable and can be plugged into existing architectures. Compared to existing modules operating in extrinsic space or treating each point independently, EdgeConv has several appealing properties: It incorporates local neighborhood information; it can be stacked applied to learn global shape properties; and in multi-layer systems affinity in feature space captures semantic characteristics over potentially long distances in the original embedding. We show the performance of our model on standard benchmarks, including ModelNet40, ShapeNetPart, and S3DIS. </p>},
   author = {Yue Wang and Yongbin Sun and Ziwei Liu and Sanjay E. Sarma and Michael M. Bronstein and Justin M. Solomon},
   doi = {10.1145/3326362},
   issn = {0730-0301},
   issue = {5},
   journal = {ACM Transactions on Graphics},
   keywords = {Classification,Point cloud,Segmentation},
   month = {11},
   pages = {1-12},
   publisher = {Association for Computing Machinery},
   title = {Dynamic Graph CNN for Learning on Point Clouds},
   volume = {38},
   url = {https://dl.acm.org/doi/10.1145/3326362},
   year = {2019},
}

@inproceedings{Xu2019,
   abstract = {Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.},
   author = {Keyulu Xu and Weihua Hu and Jure Leskovec and Stefanie Jegelka},
   booktitle = {International Conference on Learning Representations},
   title = {HOW POWERFUL ARE GRAPH NEURAL NETWORKS?},
   url = {https://openreview.net/forum?id=ryGs6iA5Km},
   year = {2019},
}

@article{Xie2018,
   author = {Tian Xie and Jeffrey C. Grossman},
   doi = {10.1103/PhysRevLett.120.145301},
   issn = {0031-9007},
   issue = {14},
   journal = {Physical Review Letters},
   month = {4},
   pages = {145301},
   title = {Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable Prediction of Material Properties},
   volume = {120},
   url = {https://link.aps.org/doi/10.1103/PhysRevLett.120.145301},
   year = {2018},
}

@inproceedings{Kipf2016,
   abstract = {We introduce the variational graph auto-encoder (VGAE), a framework for unsupervised learning on graph-structured data based on the variational auto-encoder (VAE). This model makes use of latent variables and is capable of learning interpretable latent representations for undirected graphs. We demonstrate this model using a graph convolutional network (GCN) encoder and a simple inner product decoder. Our model achieves competitive results on a link prediction task in citation networks. In contrast to most existing models for unsupervised learning on graph-structured data and link prediction, our model can naturally incorporate node features, which significantly improves predictive performance on a number of benchmark datasets.},
   author = {Thomas N. Kipf and Max Welling},
   doi = {10.48550/arxiv.1611.07308},
   booktitle = {Neural Information Processing Systems},
   month = {11},
   title = {Variational Graph Auto-Encoders},
   url = {http://arxiv.org/abs/1611.07308},
   year = {2016},
}

@inproceedings{Zaheer2017,
   author = {Manzil Zaheer and Satwik Kottur and Siamak Ravanbakhsh and Barnabas Poczos and Russ R Salakhutdinov and Alexander J Smola},
   editor = {I Guyon and U V Luxburg and S Bengio and H Wallach and R Fergus and S Vishwanathan and R Garnett},
   booktitle = {Advances in Neural Information Processing Systems},
   publisher = {Curran Associates, Inc.},
   title = {Deep Sets},
   volume = {30},
   url = {https://proceedings.neurips.cc/paper/2017/file/f22e4747da1aa27e363d86d40ff442fe-Paper.pdf},
   year = {2017},
}

@inproceedings{Gao2019,
   abstract = {We consider the problem of representation learning for graph data. Convolutional neural networks can naturally operate on images, but have significant challenges in dealing with graph data. Given images are special cases of graphs with nodes lie on 2D lattices, graph embedding tasks have a natural correspondence with image pixel-wise prediction tasks such as segmentation. While encoder-decoder architectures like U-Nets have been successfully applied on many image pixel-wise prediction tasks, similar methods are lacking for graph data. This is due to the fact that pooling and up-sampling operations are not natural on graph data. To address these challenges, we propose novel graph pooling (gPool) and un-pooling (gUnpool) operations in this work. The gPool layer adaptively selects some nodes to form a smaller graph based on their scalar projection values on a trainable projection vector. We further propose the gUnpool layer as the inverse operation of the gPool layer. The gUnpool layer restores the graph into its original structure using the position information of nodes selected in the corresponding gPool layer. Based on our proposed gPool and gUnpool layers, we develop an encoder-decoder model on graph, known as the graph U-Nets. Our experimental results on node classification and graph classification tasks demonstrate that our methods achieve consistently better performance than previous models.},
   author = {Hongyang Gao and Shuiwang Ji},
   booktitle = {International Conference on Machine Learning},
   pages = {2083-2092},
   title = {Graph U-Nets},
   year = {2019},
}

@article{Wang2019,
   abstract = {<p> Point clouds provide a flexible geometric representation suitable for countless applications in computer graphics; they also comprise the raw output of most 3D data acquisition devices. While hand-designed features on point clouds have long been proposed in graphics and vision, however, the recent overwhelming success of convolutional neural networks (CNNs) for image analysis suggests the value of adapting insight from CNN to the point cloud world. Point clouds inherently lack topological information, so designing a model to recover topology can enrich the representation power of point clouds. To this end, we propose a new neural network module dubbed <italic>EdgeConv</italic> suitable for CNN-based high-level tasks on point clouds, including classification and segmentation. EdgeConv acts on graphs dynamically computed in each layer of the network. It is differentiable and can be plugged into existing architectures. Compared to existing modules operating in extrinsic space or treating each point independently, EdgeConv has several appealing properties: It incorporates local neighborhood information; it can be stacked applied to learn global shape properties; and in multi-layer systems affinity in feature space captures semantic characteristics over potentially long distances in the original embedding. We show the performance of our model on standard benchmarks, including ModelNet40, ShapeNetPart, and S3DIS. </p>},
   author = {Yue Wang and Yongbin Sun and Ziwei Liu and Sanjay E. Sarma and Michael M. Bronstein and Justin M. Solomon},
   doi = {10.1145/3326362},
   issn = {0730-0301},
   issue = {5},
   journal = {ACM Transactions on Graphics},
   keywords = {Classification,Point cloud,Segmentation},
   month = {11},
   pages = {1-12},
   publisher = {Association for Computing Machinery},
   title = {Dynamic Graph CNN for Learning on Point Clouds},
   volume = {38},
   url = {https://dl.acm.org/doi/10.1145/3326362},
   year = {2019},
}

@inproceedings{Hamilton2017,
   abstract = {Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.},
   author = {William L Hamilton and Rex Ying and Jure Leskovec},
   city = {Red Hook, NY, USA},
   isbn = {9781510860964},
   booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
   pages = {1025-1035},
   publisher = {Curran Associates Inc.},
   title = {Inductive Representation Learning on Large Graphs},
   year = {2017},
}

@inproceedings{Satorras2021,
   abstract = {This paper introduces a new model to learn graph neural networks equivariant to rotations, translations, reflections and permutations called E(n)-Equivariant Graph Neural Networks (EGNNs). In contrast with existing methods, our work does not require computationally expensive higher-order representations in intermediate layers while it still achieves competitive or better performance. In addition, whereas existing methods are limited to equivariance on 3 dimensional spaces, our model is easily scaled to higher-dimensional spaces. We demonstrate the effectiveness of our method on dynamical systems modelling, representation learning in graph autoencoders and predicting molecular properties.},
   author = {Victor Garcia Satorras and Emiel Hoogeboom and Max Welling},
   editor = {Marina Meila and Tong Zhang},
   booktitle = {Proceedings of the 38th International Conference on Machine Learning},
   month = {2},
   pages = {9323-9332},
   publisher = {PMLR},
   title = {E(n) Equivariant Graph Neural Networks},
   volume = {139},
   url = {http://arxiv.org/abs/2102.09844},
   year = {2021},
}

@article{Dwivedi2021,
   abstract = {Graph neural networks (GNNs) have become the standard learning architectures for graphs. GNNs have been applied to numerous domains ranging from quantum chemistry, recommender systems to knowledge graphs and natural language processing. A major issue with arbitrary graphs is the absence of canonical positional information of nodes, which decreases the representation power of GNNs to distinguish e.g. isomorphic nodes and other graph symmetries. An approach to tackle this issue is to introduce Positional Encoding (PE) of nodes, and inject it into the input layer, like in Transformers. Possible graph PE are Laplacian eigenvectors. In this work, we propose to decouple structural and positional representations to make easy for the network to learn these two essential properties. We introduce a novel generic architecture which we call LSPE (Learnable Structural and Positional Encodings). We investigate several sparse and fully-connected (Transformer-like) GNNs, and observe a performance increase for molecular datasets, from 2.87% up to 64.14% when considering learnable PE for both GNN classes.},
   author = {Vijay Prakash Dwivedi and Anh Tuan Luu and Thomas Laurent and Yoshua Bengio and Xavier Bresson},
   journal = {ArXiv},
   month = {10},
   title = {Graph Neural Networks with Learnable Structural and Positional Representations},
   url = {http://arxiv.org/abs/2110.07875},
   year = {2021},
}

@article{Battaglia2018,
   abstract = {Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between "hand-engineering" and "end-to-end" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.},
   author = {Peter W. Battaglia and Jessica B. Hamrick and Victor Bapst and Alvaro Sanchez-Gonzalez and Vinicius Zambaldi and Mateusz Malinowski and Andrea Tacchetti and David Raposo and Adam Santoro and Ryan Faulkner and Caglar Gulcehre and Francis Song and Andrew Ballard and Justin Gilmer and George Dahl and Ashish Vaswani and Kelsey Allen and Charles Nash and Victoria Langston and Chris Dyer and Nicolas Heess and Daan Wierstra and Pushmeet Kohli and Matt Botvinick and Oriol Vinyals and Yujia Li and Razvan Pascanu},
   journal = {ArXiv},
   month = {6},
   title = {Relational inductive biases, deep learning, and graph networks},
   url = {http://arxiv.org/abs/1806.01261},
   year = {2018},
}

@inproceedings{Gilmer2017,
   abstract = {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.},
   author = {Justin Gilmer and Samuel S. Schoenholz and Patrick F. Riley and Oriol Vinyals and George E. Dahl},
   booktitle = {ICML 2017},
   month = {4},
   title = {Neural Message Passing for Quantum Chemistry},
   url = {http://arxiv.org/abs/1704.01212},
   year = {2017},
}

@misc{Bresson2017,
  doi = {10.48550/ARXIV.1711.07553},
  url = {https://arxiv.org/abs/1711.07553},
  author = {Bresson, Xavier and Laurent, Thomas},
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Residual Gated Graph ConvNets},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
