var documenterSearchIndex = {"docs":
[{"location":"tutorials/semisupervised_gcn/#Semi-supervised-Learning-with-Graph-Convolution-Networks-(GCN)","page":"Semi-Supervised Learning with GCN","title":"Semi-supervised Learning with Graph Convolution Networks (GCN)","text":"","category":"section"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"Graph convolution networks (GCN) have been considered as the first step to graph neural networks (GNN). This example will go through how to train a vanilla GCN.","category":"page"},{"location":"tutorials/semisupervised_gcn/#Semi-supervised-Learning-in-Graph-Neural-Networks","page":"Semi-Supervised Learning with GCN","title":"Semi-supervised Learning in Graph Neural Networks","text":"","category":"section"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"The semi-supervised learning task defines a learning by given features and labels for only partial nodes in a graph. We train features and labels for partial nodes, and test the model for another partial nodes in graph.","category":"page"},{"location":"tutorials/semisupervised_gcn/#Node-Classification-task","page":"Semi-Supervised Learning with GCN","title":"Node Classification task","text":"","category":"section"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"In this task, we learn a node classification task which learns a model to predict labels for each node in a graph. In GCN network, node features are given and the model outputs node labels.","category":"page"},{"location":"tutorials/semisupervised_gcn/#Step-1:-Load-Dataset","page":"Semi-Supervised Learning with GCN","title":"Step 1: Load Dataset","text":"","category":"section"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"GeometricFlux provides planetoid dataset in GeometricFlux.Datasets, which is provided by GraphMLDatasets. Planetoid dataset has three sub-datasets: Cora, Citeseer, PubMed. We demonstrate Cora dataset in this example. traindata provides the functionality for loading training data from various kinds of datasets. Dataset can be specified by the first argument, and the second for sub-datasets.","category":"page"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"using GeometricFlux.Datasets\n\ntrain_X, train_y = traindata(Planetoid(), :cora)","category":"page"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"traindata returns a pre-defined training features and labels. These features are node features.","category":"page"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"train_X, train_y = map(x->Matrix(x), traindata(Planetoid(), :cora))","category":"page"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"We can load graph from graphdata, and the graph is preprocessed into SimpleGraph type, which is provided by Graphs.","category":"page"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"g = graphdata(Planetoid(), :cora)\ntrain_idx = train_indices(Planetoid(), :cora)","category":"page"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"We need node indices to index a subgraph from original graph. train_indices gives node indices for training.","category":"page"},{"location":"tutorials/semisupervised_gcn/#Step-2:-Wrapping-Graph-and-Features-into-FeaturedGraph","page":"Semi-Supervised Learning with GCN","title":"Step 2: Wrapping Graph and Features into FeaturedGraph","text":"","category":"section"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"FeaturedGraph is a container for holding a graph, node features, edge features and global features. It is provided by GraphSignals. To wrap graph and node features into FeaturedGraph, graph g should be placed as the first argument and nf is to specify node features.","category":"page"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"using GraphSignals\n\nFeaturedGraph(g, nf=train_X)","category":"page"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"If we want to get a subgraph from a FeaturedGraph object, we call subgraph and provide node indices train_idx as second argument.","category":"page"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"subgraph(FeaturedGraph(g, nf=train_X), train_idx)","category":"page"},{"location":"tutorials/semisupervised_gcn/#Step-3:-Build-a-GCN-model","page":"Semi-Supervised Learning with GCN","title":"Step 3: Build a GCN model","text":"","category":"section"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"A GCn model is composed of two layers of GCNConv and the activation function for first layer is relu. In the middle, a Dropout layer is placed. We need a GraphParallel to integrate with regular Flux layer, and it specifies node features go to node_layer=Dropout(0.5).","category":"page"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"model = Chain(\n    GCNConv(input_dim=>hidden_dim, relu),\n    GraphParallel(node_layer=Dropout(0.5)),\n    GCNConv(hidden_dim=>target_dim),\n    node_feature,\n)","category":"page"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"Since the model input is a FeaturedGraph object, the model output a FeaturedGraph object as well. In the end of model, we get node features out from a FeaturedGraph object using node_feature.","category":"page"},{"location":"tutorials/semisupervised_gcn/#Step-4:-Loss-Functions-and-Accuracy","page":"Semi-Supervised Learning with GCN","title":"Step 4: Loss Functions and Accuracy","text":"","category":"section"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"Then, since it is a node classification task, we define the model loss by logitcrossentropy, and a L2 regularization is used. In the vanilla GCN, only first layer is applied to L2 regularization and can be adjusted by hyperparameter λ.","category":"page"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"l2norm(x) = sum(abs2, x)\n\nfunction model_loss(model, λ, batch)\n    loss = 0.f0\n    for (x, y) in batch\n        loss += logitcrossentropy(model(x), y)\n        loss += λ*sum(l2norm, Flux.params(model[1]))\n    end\n    return loss\nend","category":"page"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"Accuracy for a batch and for data loader are provided.","category":"page"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"function accuracy(model, batch::AbstractVector)\n    return mean(mean(onecold(softmax(cpu(model(x)))) .== onecold(cpu(y))) for (x, y) in batch)\nend\n\naccuracy(model, loader::DataLoader, device) = mean(accuracy(model, batch |> device) for batch in loader)","category":"page"},{"location":"tutorials/semisupervised_gcn/#Step-5:-Training-GCN-Model","page":"Semi-Supervised Learning with GCN","title":"Step 5: Training GCN Model","text":"","category":"section"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"We train the model with the same process as training a Flux model.","category":"page"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"train_loader, test_loader = load_data(:cora, args.batch_size)\n\n# optimizer\nopt = ADAM(args.η)\n    \n# parameters\nps = Flux.params(model)\n\n# training\ntrain_steps = 0\n@info \"Start Training, total $(args.epochs) epochs\"\nfor epoch = 1:args.epochs\n    @info \"Epoch $(epoch)\"\n\n    for batch in train_loader\n        grad = gradient(() -> model_loss(model, args.λ, batch |> device), ps)\n        Flux.Optimise.update!(opt, ps, grad)\n        train_steps += 1\n    end\nend","category":"page"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"So far, we complete a basic tutorial for training a GCN model!","category":"page"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"For the complete example, please check the script examples/semisupervised_gcn.jl.","category":"page"},{"location":"tutorials/semisupervised_gcn/#Acceleration-by-Pre-computing-Normalized-Adjacency-Matrix","page":"Semi-Supervised Learning with GCN","title":"Acceleration by Pre-computing Normalized Adjacency Matrix","text":"","category":"section"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"The training process can be slow in this example. Since we place the graph and features together in FeaturedGraph object, GCNConv will need to compute a normalized adjacency matrix in the training process. This behavior will lead to long training time. We can accelerate training process by pre-compute normalized adjacency matrix for all FeaturedGraph objects. To do so, we can call the following function and it will compute normalized adjacency matrix for fg before training. This will reduce the training time.","category":"page"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"GraphSignals.normalized_adjacency_matrix!(fg)","category":"page"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"Since the normalized adjacency matrix is used in GCNConv, we could pre-compute normalized adjacency matrix for it. If a layer doesn't require a normalized adjacency matrix, this step will lead to error.","category":"page"},{"location":"references/#References","page":"References","title":"References","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"","category":"page"},{"location":"basics/passgraph/#Graph-Passing-Strategy","page":"Graph Passing","title":"Graph Passing Strategy","text":"","category":"section"},{"location":"basics/passgraph/","page":"Graph Passing","title":"Graph Passing","text":"Graph is an input data structure for graph neural network. Passing a graph into a GNN layer can have different behaviors. If the graph remains fixed across samples, that is, all samples utilize the same graph structure, a static graph is used. Or, graphs can be carried within FeaturedGraph to provide variable graphs to GNN layer. Users have the flexibility to pick an adequate approach for their own needs.","category":"page"},{"location":"basics/passgraph/#Variable-Graph-Strategy","page":"Graph Passing","title":"Variable Graph Strategy","text":"","category":"section"},{"location":"basics/passgraph/","page":"Graph Passing","title":"Graph Passing","text":"Variable graphs are supported through FeaturedGraph, which contains both the graph information and the features. Each FeaturedGraph can contain a distinct graph structure and its features. Data of FeaturedGraph are fed directly to graph convolutional layer or graph neural network to let each feature be learned on different graph structures. A adjacency matrix adj_mat is given to construct a FeaturedGraph as follows:","category":"page"},{"location":"basics/passgraph/","page":"Graph Passing","title":"Graph Passing","text":"fg = FeaturedGraph(adj_mat, features)\nlayer = GCNConv(feat=>h1, relu)","category":"page"},{"location":"basics/passgraph/","page":"Graph Passing","title":"Graph Passing","text":"Simple(Di)Graph, SimpleWeighted(Di)Graph or Meta(Di)Graph provided by the packages Graphs, SimpleWeightedGraphs and MetaGraphs, respectively, are acceptable for constructing a FeaturedGraph. An adjacency list is also accepted, too.","category":"page"},{"location":"basics/passgraph/#[FeaturedGraph](@ref)-in,-[FeaturedGraph](@ref)-out","page":"Graph Passing","title":"FeaturedGraph in, FeaturedGraph out","text":"","category":"section"},{"location":"basics/passgraph/","page":"Graph Passing","title":"Graph Passing","text":"Since a variable graph is provided from data, a FeaturedGraph object or a set of FeaturedGraph objects should be fed in a GNN model. The FeaturedGraph object should contain a graph and sufficient features that a GNN model needed. After operations, a FeaturedGraph object is given as output.","category":"page"},{"location":"basics/passgraph/","page":"Graph Passing","title":"Graph Passing","text":"fg = FeaturedGraph(g, nf=X)\ngc = GCNConv(in_channel=>out_channel)\nnew_fg = gc(fg)","category":"page"},{"location":"basics/passgraph/#Static-Graph-Strategy","page":"Graph Passing","title":"Static Graph Strategy","text":"","category":"section"},{"location":"basics/passgraph/","page":"Graph Passing","title":"Graph Passing","text":"A static graph is used to reduce redundant computation during passing through layers. A static graph can be set in graph convolutional layers such that this graph is shared for computations across those layers. An adjacency matrix adj_mat is given to represent a graph and is provided to a graph convolutional layer as follows:","category":"page"},{"location":"basics/passgraph/","page":"Graph Passing","title":"Graph Passing","text":"fg = FeaturedGraph(adj_mat)\nlayer = WithGraph(fg, GCNConv(feat=>h1, relu))","category":"page"},{"location":"basics/passgraph/","page":"Graph Passing","title":"Graph Passing","text":"Simple(Di)Graph, SimpleWeighted(Di)Graph or Meta(Di)Graph provided by the packages Graphs, SimpleWeightedGraphs and MetaGraphs, respectively, are valid arguments for passing as a static graph to this layer. An adjacency list is also accepted in the type of Vector{Vector} is also accepted.","category":"page"},{"location":"basics/passgraph/#Cached-Graph-in-Layers","page":"Graph Passing","title":"Cached Graph in Layers","text":"","category":"section"},{"location":"basics/passgraph/","page":"Graph Passing","title":"Graph Passing","text":"While a variable graph is given by FeaturedGraph, a GNN layer doesn't need a static graph anymore. A cache mechanism is designed to cache static graph to reduce computation time. A cached graph is retrieved from WithGraph layer and operation is then performed. For each time, it will assign current computed graph back to layer.","category":"page"},{"location":"basics/passgraph/#Array-in,-Array-out","page":"Graph Passing","title":"Array in, Array out","text":"","category":"section"},{"location":"basics/passgraph/","page":"Graph Passing","title":"Graph Passing","text":"Since a static graph is provided from WithGraph layer, it doesn't accept a FeaturedGraph object anymore. Instead, it accepts a regular array as input, and outputs an array back.","category":"page"},{"location":"basics/passgraph/","page":"Graph Passing","title":"Graph Passing","text":"fg = FeaturedGraph(g)\nlayer = WithGraph(fg, GCNConv(in_channel=>out_channel))\nH = layer(X)","category":"page"},{"location":"basics/passgraph/#What-you-feed-is-what-you-get","page":"Graph Passing","title":"What you feed is what you get","text":"","category":"section"},{"location":"basics/passgraph/","page":"Graph Passing","title":"Graph Passing","text":"In GeometricFlux, there are are two APIs which allow different input/output types for GNN layers. For example, GCNConv layer provides the following two APIs:","category":"page"},{"location":"basics/passgraph/","page":"Graph Passing","title":"Graph Passing","text":"(g::WithGraph{<:GCNConv})(X::AbstractArray) -> AbstractArray\n(g::GCNConv)(fg::FeaturedGraph) -> FeaturedGraph","category":"page"},{"location":"basics/passgraph/","page":"Graph Passing","title":"Graph Passing","text":"If your feed a GCNConv layer with a Array, it will return you a Array. If you feed a GCNConv layer with a FeaturedGraph, it will return you a FeaturedGraph. These APIs ensure the consistency between input and output types.","category":"page"},{"location":"basics/neighborhood_graph/#Neighborhood-Graphs","page":"Neighborhood graphs","title":"Neighborhood Graphs","text":"","category":"section"},{"location":"basics/neighborhood_graph/","page":"Neighborhood graphs","title":"Neighborhood graphs","text":"In machine learning, it is often that using a neighborhood graph to approach manifold in high dimensional space. The construction of neighborhood graph is the essential step for machine learning algorithms on graph/manifold, especially manifold learning.","category":"page"},{"location":"basics/neighborhood_graph/","page":"Neighborhood graphs","title":"Neighborhood graphs","text":"The k-nearest neighbor (kNN) method is the most frequent use to construct a neighborhood graph. We provide kneighbors_graph to generate a kNN graph from a set of nodes/points.","category":"page"},{"location":"basics/neighborhood_graph/","page":"Neighborhood graphs","title":"Neighborhood graphs","text":"We prepare 1,024 10-dimensional data points.","category":"page"},{"location":"basics/neighborhood_graph/","page":"Neighborhood graphs","title":"Neighborhood graphs","text":"X = rand(Float32, 10, 1024)","category":"page"},{"location":"basics/neighborhood_graph/","page":"Neighborhood graphs","title":"Neighborhood graphs","text":"Then, we can generate a kNN graph with k=7, which means a data point should be linked to their top-7 nearest neighbor points.","category":"page"},{"location":"basics/neighborhood_graph/","page":"Neighborhood graphs","title":"Neighborhood graphs","text":"fg = kneighbors_graph(nf, 7)","category":"page"},{"location":"basics/neighborhood_graph/","page":"Neighborhood graphs","title":"Neighborhood graphs","text":"The default distance metric would be Euclidean distance from Distance.jl package. If one wants to customize kneighbors_graph by using different distance metric, you can just use the distance objects from Distance.jl package directly, and pass it to kneighbors_graph.","category":"page"},{"location":"basics/neighborhood_graph/","page":"Neighborhood graphs","title":"Neighborhood graphs","text":"using Distances\n\nfg = kneighbors_graph(nf, 7, Cityblock())","category":"page"},{"location":"manual/models/#Models","page":"Models","title":"Models","text":"","category":"section"},{"location":"manual/models/#Autoencoders","page":"Models","title":"Autoencoders","text":"","category":"section"},{"location":"manual/models/#Graph-Autoencoder","page":"Models","title":"Graph Autoencoder","text":"","category":"section"},{"location":"manual/models/","page":"Models","title":"Models","text":"Z = enc(X A) \nhatA = sigma (ZZ^T)","category":"page"},{"location":"manual/models/","page":"Models","title":"Models","text":"where A denotes the adjacency matrix.","category":"page"},{"location":"manual/models/","page":"Models","title":"Models","text":"GeometricFlux.GAE","category":"page"},{"location":"manual/models/#GeometricFlux.GAE","page":"Models","title":"GeometricFlux.GAE","text":"GAE(enc, [σ=identity])\n\nGraph autoencoder.\n\nArguments\n\nenc: encoder. It can be any graph convolutional layer.\nσ: Activation function for decoder.\n\nEncoder is specified by user and decoder will be InnerProductDecoder layer.\n\n\n\n\n\n","category":"type"},{"location":"manual/models/","page":"Models","title":"Models","text":"Reference: Thomas N. Kipf, Max Welling (2016)","category":"page"},{"location":"manual/models/","page":"Models","title":"Models","text":"","category":"page"},{"location":"manual/models/#Variational-Graph-Autoencoder","page":"Models","title":"Variational Graph Autoencoder","text":"","category":"section"},{"location":"manual/models/","page":"Models","title":"Models","text":"H = enc(X A) \nZ_mu Z_logσ = GCN_mu(H A) GCN_sigma(H A) \nhatA = sigma (ZZ^T)","category":"page"},{"location":"manual/models/","page":"Models","title":"Models","text":"where A denotes the adjacency matrix, X denotes node features.","category":"page"},{"location":"manual/models/","page":"Models","title":"Models","text":"GeometricFlux.VGAE","category":"page"},{"location":"manual/models/#GeometricFlux.VGAE","page":"Models","title":"GeometricFlux.VGAE","text":"VGAE(enc[, σ])\n\nVariational graph autoencoder.\n\nArguments\n\nenc: encoder. It can be any graph convolutional layer.\n\nEncoder is specified by user and decoder will be InnerProductDecoder layer.\n\n\n\n\n\n","category":"type"},{"location":"manual/models/","page":"Models","title":"Models","text":"Reference: Thomas N. Kipf, Max Welling (2016)","category":"page"},{"location":"manual/models/","page":"Models","title":"Models","text":"","category":"page"},{"location":"manual/models/#DeepSet","page":"Models","title":"DeepSet","text":"","category":"section"},{"location":"manual/models/","page":"Models","title":"Models","text":"Z = rho ( sum_x_i in mathcalV phi (x_i) )","category":"page"},{"location":"manual/models/","page":"Models","title":"Models","text":"where phi and rho denote two neural networks and x_i is the node feature for node i.","category":"page"},{"location":"manual/models/","page":"Models","title":"Models","text":"GeometricFlux.DeepSet","category":"page"},{"location":"manual/models/#GeometricFlux.DeepSet","page":"Models","title":"GeometricFlux.DeepSet","text":"DeepSet(ϕ, ρ, aggr=+)\n\nDeep set model.\n\nArguments\n\nϕ: Neural network layer for each input before aggregation.\nρ: Neural network layer after aggregation.\naggr: An aggregate function applied to the result of message function. +, -,\n\n*, /, max, min and mean are available.\n\nExamples\n\njulia> ϕ = Dense(64, 16)\nDense(64 => 16)     # 1_040 parameters\n\njulia> ρ = Dense(16, 4)\nDense(16 => 4)      # 68 parameters\n\njulia> DeepSet(ϕ, ρ)\nDeepSet(Dense(64 => 16), Dense(16 => 4), aggr=+)\n\njulia> DeepSet(ϕ, ρ, aggr=max)\nDeepSet(Dense(64 => 16), Dense(16 => 4), aggr=max)\n\nSee also WithGraph for training layer with static graph.\n\n\n\n\n\n","category":"type"},{"location":"manual/models/","page":"Models","title":"Models","text":"Reference: Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, Alexander J Smola (2017)","category":"page"},{"location":"manual/models/","page":"Models","title":"Models","text":"","category":"page"},{"location":"manual/models/#Special-Layers","page":"Models","title":"Special Layers","text":"","category":"section"},{"location":"manual/models/#Inner-product-Decoder","page":"Models","title":"Inner-product Decoder","text":"","category":"section"},{"location":"manual/models/","page":"Models","title":"Models","text":"hatA = sigma (ZZ^T)","category":"page"},{"location":"manual/models/","page":"Models","title":"Models","text":"where Z denotes the input matrix from encoder.","category":"page"},{"location":"manual/models/","page":"Models","title":"Models","text":"GeometricFlux.InnerProductDecoder","category":"page"},{"location":"manual/models/#GeometricFlux.InnerProductDecoder","page":"Models","title":"GeometricFlux.InnerProductDecoder","text":"InnerProductDecoder(σ)\n\nInner-product decoder layer.\n\nArguments\n\nσ: activation function.\n\n\n\n\n\n","category":"type"},{"location":"manual/models/","page":"Models","title":"Models","text":"Reference: Thomas N. Kipf, Max Welling (2016)","category":"page"},{"location":"manual/models/","page":"Models","title":"Models","text":"","category":"page"},{"location":"manual/models/#Variational-Graph-Encoder","page":"Models","title":"Variational Graph Encoder","text":"","category":"section"},{"location":"manual/models/","page":"Models","title":"Models","text":"H = enc(X A) \nZ_mu Z_logσ = GCN_mu(H A) GCN_sigma(H A)","category":"page"},{"location":"manual/models/","page":"Models","title":"Models","text":"GeometricFlux.VariationalGraphEncoder","category":"page"},{"location":"manual/models/#GeometricFlux.VariationalGraphEncoder","page":"Models","title":"GeometricFlux.VariationalGraphEncoder","text":"VariationalGraphEncoder(nn, h_dim, z_dim)\n\nVariational graph encoder layer.\n\nArguments\n\nnn: neural network. It can be any graph convolutional layer.\nh_dim: dimension of hidden layer. This should fit the output dimension of nn.\nz_dim: dimension of latent variable layer. This will be parametrized into μ and logσ.\n\nEncoder can be any graph convolutional layer.\n\n\n\n\n\n","category":"type"},{"location":"manual/models/","page":"Models","title":"Models","text":"Reference: Thomas N. Kipf, Max Welling (2016)","category":"page"},{"location":"tutorials/vgae/#Variational-Graph-Autoencoder","page":"Variational Graph Autoencoder","title":"Variational Graph Autoencoder","text":"","category":"section"},{"location":"tutorials/vgae/","page":"Variational Graph Autoencoder","title":"Variational Graph Autoencoder","text":"Variational Graph Autoencoder (VGAE) is a unsupervised generative model. It takes node features and graph structure and predicts the edge link in the graph. A link preidction task is defined for this model.","category":"page"},{"location":"tutorials/vgae/#Step-1:-Load-Dataset","page":"Variational Graph Autoencoder","title":"Step 1: Load Dataset","text":"","category":"section"},{"location":"tutorials/vgae/","page":"Variational Graph Autoencoder","title":"Variational Graph Autoencoder","text":"We load dataset from Planetoid dataset. Here cora dataset is used.","category":"page"},{"location":"tutorials/vgae/","page":"Variational Graph Autoencoder","title":"Variational Graph Autoencoder","text":"train_X, _ = map(x -> Matrix(x), alldata(Planetoid(), dataset))","category":"page"},{"location":"tutorials/vgae/","page":"Variational Graph Autoencoder","title":"Variational Graph Autoencoder","text":"Notably, a link prediction task will output a graph in the form of adjacency matrix, so an adjacency matrix is needed as label for this task.","category":"page"},{"location":"tutorials/vgae/","page":"Variational Graph Autoencoder","title":"Variational Graph Autoencoder","text":"g = graphdata(Planetoid(), dataset)\nfg = FeaturedGraph(g)\nA = GraphSignals.adjacency_matrix(fg)","category":"page"},{"location":"tutorials/vgae/#Step-2:-Batch-up-Features-and-Labels","page":"Variational Graph Autoencoder","title":"Step 2: Batch up Features and Labels","text":"","category":"section"},{"location":"tutorials/vgae/","page":"Variational Graph Autoencoder","title":"Variational Graph Autoencoder","text":"Just batch up features as usual.","category":"page"},{"location":"tutorials/vgae/","page":"Variational Graph Autoencoder","title":"Variational Graph Autoencoder","text":"data = (repeat(X, outer=(1,1,train_repeats)), repeat(A, outer=(1,1,train_repeats)))\nloader = DataLoader(data, batchsize=batch_size, shuffle=true)","category":"page"},{"location":"tutorials/vgae/#Step-3:-Build-a-VGAE-model","page":"Variational Graph Autoencoder","title":"Step 3: Build a VGAE model","text":"","category":"section"},{"location":"tutorials/vgae/","page":"Variational Graph Autoencoder","title":"Variational Graph Autoencoder","text":"A VGAE model is composed of an encoder and a decoder. A VariationalGraphEncoder is used as an graph encoder and it contains a neural network to encode node features. A InnerProductDecoder is the decoder to predict links in a graph. Actually, it gives an adjacency matrix. Finally, we build VGAE model with encoder and decoder.","category":"page"},{"location":"tutorials/vgae/","page":"Variational Graph Autoencoder","title":"Variational Graph Autoencoder","text":"encoder = VariationalGraphEncoder(\n    WithGraph(fg, GCNConv(args.input_dim=>args.h_dim, relu)),\n    WithGraph(fg, GCNConv(args.h_dim=>args.z_dim)),\n    WithGraph(fg, GCNConv(args.h_dim=>args.z_dim)),\n    args.z_dim\n)\n\ndecoder = InnerProductDecoder(σ)\n\nmodel = VGAE(encoder, decoder) |> device","category":"page"},{"location":"tutorials/vgae/#Step-4:-Loss-Functions-and-Link-Prediction","page":"Variational Graph Autoencoder","title":"Step 4: Loss Functions and Link Prediction","text":"","category":"section"},{"location":"tutorials/vgae/","page":"Variational Graph Autoencoder","title":"Variational Graph Autoencoder","text":"Since a VGAE is a VAE model, its loss function is composed of a KL divergence and a log P.","category":"page"},{"location":"tutorials/vgae/","page":"Variational Graph Autoencoder","title":"Variational Graph Autoencoder","text":"function kldivergence(model, X::AbstractArray{T}) where {T}\n    μ̂, logσ̂ = GeometricFlux.summarize(model.encoder, X)\n    return -T(0.5) * sum(one(T) .+ T(2).*logσ̂ .- μ̂.^2 .- exp.(T(2).*logσ̂))\nend\n\nfunction logp(model, X, Y)\n    Z = model.encoder(X)\n    return -logitbinarycrossentropy(model.decoder(Z), Y)\nend\n\nfunction model_loss(model, X, Y, β)\n    kl_q_p = kldivergence(model, X)\n    logp_y_z = logp(model, X, Y)\n    return -logp_y_z + β*kl_q_p\nend","category":"page"},{"location":"tutorials/vgae/","page":"Variational Graph Autoencoder","title":"Variational Graph Autoencoder","text":"Precision metric is used to measure the existence of edges to be predicted from a model.","category":"page"},{"location":"tutorials/vgae/","page":"Variational Graph Autoencoder","title":"Variational Graph Autoencoder","text":"function precision(model, X::AbstractArray, A::AbstractArray)\n    ŷ = cpu(Flux.flatten(model(X))) .≥ 0.5\n    y = cpu(Flux.flatten(A))\n    return sum(y .* ŷ) / sum(ŷ)\nend","category":"page"},{"location":"tutorials/vgae/#Step-5:-Training-VGAE-Model","page":"Variational Graph Autoencoder","title":"Step 5: Training VGAE Model","text":"","category":"section"},{"location":"tutorials/vgae/","page":"Variational Graph Autoencoder","title":"Variational Graph Autoencoder","text":"# ADAM optimizer\nopt = ADAM(args.η)\n\n# parameters\nps = Flux.params(model)\n\n# training\n@info \"Start Training, total $(args.epochs) epochs\"\nfor epoch = 1:args.epochs\n    @info \"Epoch $(epoch)\"\n\n    for (X, A) in loader\n        loss, back = Flux.pullback(ps) do\n            model_loss(model, X |> device, A |> device, args.β)\n        end\n        prec = precision(model, loader, device)\n        grad = back(1f0)\n        Flux.Optimise.update!(opt, ps, grad)\n    end\nend","category":"page"},{"location":"tutorials/vgae/","page":"Variational Graph Autoencoder","title":"Variational Graph Autoencoder","text":"For a complete example, please check examples/vgae.jl.","category":"page"},{"location":"basics/batch/#Batch-Learning","page":"Batch Learning","title":"Batch Learning","text":"","category":"section"},{"location":"basics/batch/#Batch-Learning-for-Variable-Graph-Strategy","page":"Batch Learning","title":"Batch Learning for Variable Graph Strategy","text":"","category":"section"},{"location":"basics/batch/","page":"Batch Learning","title":"Batch Learning","text":"Batch learning for variable graph strategy can be prepared as follows:","category":"page"},{"location":"basics/batch/","page":"Batch Learning","title":"Batch Learning","text":"train_data = [(FeaturedGraph(g, nf=train_X), train_y) for _ in 1:N]\ntrain_batch = Flux.batch(train_data)","category":"page"},{"location":"basics/batch/","page":"Batch Learning","title":"Batch Learning","text":"It batches up FeaturedGraph objects into specified mini-batch. A batch is passed to a GNN model and trained/inferred one by one. It is hard for FeaturedGraph objects to train or infer in real batch for GPU.","category":"page"},{"location":"basics/batch/#Batch-Learning-for-Static-Graph-Strategy","page":"Batch Learning","title":"Batch Learning for Static Graph Strategy","text":"","category":"section"},{"location":"basics/batch/","page":"Batch Learning","title":"Batch Learning","text":"A efficient batch learning should use static graph strategy. Batch learning for static graph strategy can be prepared as follows:","category":"page"},{"location":"basics/batch/","page":"Batch Learning","title":"Batch Learning","text":"train_data = (repeat(train_X, outer=(1,1,N)), repeat(train_y, outer=(1,1,N)))\ntrain_loader = DataLoader(train_data, batchsize=batch_size, shuffle=true)","category":"page"},{"location":"basics/batch/","page":"Batch Learning","title":"Batch Learning","text":"An efficient batch learning should feed array to a GNN model. In the example, the mini-batch dimension is the third dimension for train_X array. The train_X array is split by DataLoader into mini-batches and feed a mini-batch to GNN model at a time. This strategy leverages the advantage of GPU training by accelerating training GNN model in a real batch learning.","category":"page"},{"location":"manual/embedding/#Embeddings","page":"Embeddings","title":"Embeddings","text":"","category":"section"},{"location":"manual/embedding/#Node2vec","page":"Embeddings","title":"Node2vec","text":"","category":"section"},{"location":"manual/embedding/","page":"Embeddings","title":"Embeddings","text":"GeometricFlux.node2vec","category":"page"},{"location":"manual/embedding/#GeometricFlux.node2vec","page":"Embeddings","title":"GeometricFlux.node2vec","text":"node2vec(g; walks_per_node, len, p, q, dims)\n\nReturns an embedding matrix with size of (nv(g), dims). It computes node embeddings on graph g. It performs biased random walks on the graph, then computes word embeddings by treating those random walks as sentences.\n\nArguments\n\ng::FeaturedGraph: The graph to perform random walk on.\nwalks_per_node::Int: Number of walks starting on each node, total number of walks is nv(g) * walks_per_node\nlen::Int: Length of random walks\np::Real: Return parameter. It controls the likelihood of immediately revisiting a node in the walk\nq::Real: In-out parameter. It allows the search to differentiate between inward and outward nodes.\ndims::Int: Number of vector dimensions\n\n\n\n\n\n","category":"function"},{"location":"manual/embedding/","page":"Embeddings","title":"Embeddings","text":"Reference: Aditya Grover, Jure Leskovec (2016)","category":"page"},{"location":"basics/conv/#Graph-Convolutions","page":"Graph Convolutions","title":"Graph Convolutions","text":"","category":"section"},{"location":"basics/conv/","page":"Graph Convolutions","title":"Graph Convolutions","text":"Graph convolution can be classified into spectral-based graph convolution and spatial-based graph convolution. Spectral-based graph convolution, such as GCNConv and ChebConv, performs operation on features of whole graph at one time. Spatial-based graph convolution, such as GraphConv and GATConv, performs operation on features of local subgraph instead. Message-passing scheme is an abstraction for spatial-based graph convolutional layers. Any spatial-based graph convolutional layer can be implemented under the framework of message-passing scheme.","category":"page"},{"location":"basics/subgraph/#Subgraph","page":"Subgraph","title":"Subgraph","text":"","category":"section"},{"location":"basics/subgraph/#Subgraph-of-[FeaturedGraph](@ref)","page":"Subgraph","title":"Subgraph of FeaturedGraph","text":"","category":"section"},{"location":"basics/subgraph/","page":"Subgraph","title":"Subgraph","text":"A FeaturedGraph object can derive a subgraph from a selected subset of the vertices of the graph.","category":"page"},{"location":"basics/subgraph/","page":"Subgraph","title":"Subgraph","text":"train_idx = train_indices(Planetoid(), :cora)\nfg = FeaturedGraph(g)\nfsg = subgraph(fg, train_idx)","category":"page"},{"location":"basics/subgraph/","page":"Subgraph","title":"Subgraph","text":"A FeaturedSubgraph object is returned from subgraph by selected vertices train_idx.","category":"page"},{"location":"manual/featuredgraph/#FeaturedGraph","page":"FeaturedGraph","title":"FeaturedGraph","text":"","category":"section"},{"location":"manual/featuredgraph/","page":"FeaturedGraph","title":"FeaturedGraph","text":"GraphSignals.FeaturedGraph\nGraphSignals.graph\nGraphSignals.node_feature\nGraphSignals.has_node_feature\nGraphSignals.edge_feature\nGraphSignals.has_edge_feature\nGraphSignals.global_feature\nGraphSignals.has_global_feature\nGraphSignals.subgraph\nGraphSignals.ConcreteFeaturedGraph","category":"page"},{"location":"manual/featuredgraph/#GraphSignals.FeaturedGraph","page":"FeaturedGraph","title":"GraphSignals.FeaturedGraph","text":"FeaturedGraph(g, [mt]; directed=:auto, nf, ef, gf, T, N, E)\n\nA type representing a graph structure and storing also arrays  that contain features associated to nodes, edges, and the whole graph. \n\nA FeaturedGraph can be constructed out of different objects g representing the connections inside the graph. When constructed from another featured graph fg, the internal graph representation is preserved and shared.\n\nArguments\n\ng: Data representing the graph topology. Possible type are \nAn adjacency matrix.\nAn adjacency list.\nA Graphs' graph, i.e. SimpleGraph, SimpleDiGraph from Graphs, or SimpleWeightedGraph,   SimpleWeightedDiGraph from SimpleWeightedGraphs.\nAn AbstractFeaturedGraph object.\nmt::Symbol: Matrix type for g in matrix form. if graph is in matrix form, mt is recorded as one of :adjm,   :normedadjm, :laplacian, :normalized or :scaled.\ndirected: It specify that direction of a graph. It can be :auto, :directed and :undirected.   Default value is :auto, which infers direction automatically.\nnf: Node features.\nef: Edge features.\ngf: Global features.\nT: It specifies the element type of graph. Default value is the element type of g.\nN: Number of nodes for g.\nE: Number of edges for g.\n\nUsage\n\nusing GraphSignals, CUDA\n\n# Construct from adjacency list representation\ng = [[2,3], [1,4,5], [1], [2,5], [2,4]]\nfg = FeaturedGraph(g)\n\n# Number of nodes and edges\nnv(fg)  # 5\nne(fg)  # 10\n\n# From a Graphs' graph\nfg = FeaturedGraph(erdos_renyi(100, 20))\n\n# Copy featured graph while also adding node features\nfg = FeaturedGraph(fg, nf=rand(100, 5))\n\n# Send to gpu\nfg = fg |> cu\n\nSee also graph, node_feature, edge_feature, and global_feature.\n\n\n\n\n\n","category":"type"},{"location":"manual/featuredgraph/#GraphSignals.graph","page":"FeaturedGraph","title":"GraphSignals.graph","text":"graph(fg)\n\nGet referenced graph in fg.\n\nArguments\n\nfg::AbstractFeaturedGraph: A concrete object of AbstractFeaturedGraph type.\n\n\n\n\n\n","category":"function"},{"location":"manual/featuredgraph/#GraphSignals.node_feature","page":"FeaturedGraph","title":"GraphSignals.node_feature","text":"node_feature(fg)\n\nGet node feature attached to fg.\n\nArguments\n\nfg::AbstractFeaturedGraph: A concrete object of AbstractFeaturedGraph type.\n\n\n\n\n\n","category":"function"},{"location":"manual/featuredgraph/#GraphSignals.has_node_feature","page":"FeaturedGraph","title":"GraphSignals.has_node_feature","text":"has_node_feature(::AbstractFeaturedGraph)\n\nCheck if node_feature is available or not for fg.\n\nArguments\n\nfg::AbstractFeaturedGraph: A concrete object of AbstractFeaturedGraph type.\n\n\n\n\n\n","category":"function"},{"location":"manual/featuredgraph/#GraphSignals.edge_feature","page":"FeaturedGraph","title":"GraphSignals.edge_feature","text":"edge_feature(fg)\n\nGet edge feature attached to fg.\n\nArguments\n\nfg::AbstractFeaturedGraph: A concrete object of AbstractFeaturedGraph type.\n\n\n\n\n\n","category":"function"},{"location":"manual/featuredgraph/#GraphSignals.has_edge_feature","page":"FeaturedGraph","title":"GraphSignals.has_edge_feature","text":"has_edge_feature(::AbstractFeaturedGraph)\n\nCheck if edge_feature is available or not for fg.\n\nArguments\n\nfg::AbstractFeaturedGraph: A concrete object of AbstractFeaturedGraph type.\n\n\n\n\n\n","category":"function"},{"location":"manual/featuredgraph/#GraphSignals.global_feature","page":"FeaturedGraph","title":"GraphSignals.global_feature","text":"global_feature(fg)\n\nGet global feature attached to fg.\n\nArguments\n\nfg::AbstractFeaturedGraph: A concrete object of AbstractFeaturedGraph type.\n\n\n\n\n\n","category":"function"},{"location":"manual/featuredgraph/#GraphSignals.has_global_feature","page":"FeaturedGraph","title":"GraphSignals.has_global_feature","text":"has_global_feature(::AbstractFeaturedGraph)\n\nCheck if global_feature is available or not for fg.\n\nArguments\n\nfg::AbstractFeaturedGraph: A concrete object of AbstractFeaturedGraph type.\n\n\n\n\n\n","category":"function"},{"location":"manual/featuredgraph/#GraphSignals.subgraph","page":"FeaturedGraph","title":"GraphSignals.subgraph","text":"subgraph(fg, nodes)\n\nReturns a subgraph of type FeaturedSubgraph from a given featured graph fg. It constructs a subgraph by reserving nodes in a graph.\n\nArguments\n\nfg::AbstractFeaturedGraph: A base featured graph to construct a subgraph.\nnodes::AbstractVector: It specifies nodes to be reserved from fg.\n\n\n\n\n\n","category":"function"},{"location":"manual/featuredgraph/#GraphSignals.ConcreteFeaturedGraph","page":"FeaturedGraph","title":"GraphSignals.ConcreteFeaturedGraph","text":"ConcreteFeaturedGraph(fg; kwargs...)\n\nThis is a syntax sugar for construction for FeaturedGraph and FeaturedSubgraph object. It is an idempotent operation, which gives the same type of object as inputs. It wraps input fg again but reconfigures with kwargs.\n\nArguments\n\nfg: FeaturedGraph and FeaturedSubgraph object.\n\nUsage\n\njulia> using GraphSignals\n\njulia> adjm = [0 1 1 1;\n               1 0 1 0;\n               1 1 0 1;\n               1 0 1 0];\n\njulia> nf = rand(10, 4);\n\njulia> fg = FeaturedGraph(adjm; nf=nf)\nFeaturedGraph:\n\tUndirected graph with (#V=4, #E=5) in adjacency matrix\n\tNode feature:\tℝ^10 <Matrix{Float64}>\n\njulia> ConcreteFeaturedGraph(fg, nf=rand(7, 4))\nFeaturedGraph:\n    Undirected graph with (#V=4, #E=5) in adjacency matrix\n    Node feature:\tℝ^7 <Matrix{Float64}>    \n\n\n\n\n\n","category":"function"},{"location":"tutorials/gcn_fixed_graph/#GCN-with-Fixed-Graph","page":"GCN with Fixed Graph","title":"GCN with Fixed Graph","text":"","category":"section"},{"location":"tutorials/gcn_fixed_graph/","page":"GCN with Fixed Graph","title":"GCN with Fixed Graph","text":"In the tutorial for semi-supervised learning with GCN, variable graphs are provided to GNN from FeaturedGraph, which contains a graph and node features. Each FeaturedGraph object can contain different graph and different node features, and can be train on the same GNN model. However, variable graph doesn't have the proper form of graph structure with respect to GNN layers and this lead to inefficient training/inference process. Fixed graph strategy can be used to train a GNN model with the same graph structure in GeometricFlux.","category":"page"},{"location":"tutorials/gcn_fixed_graph/#Fixed-Graph","page":"GCN with Fixed Graph","title":"Fixed Graph","text":"","category":"section"},{"location":"tutorials/gcn_fixed_graph/","page":"GCN with Fixed Graph","title":"GCN with Fixed Graph","text":"A fixed graph is given to a layer by WithGraph syntax. WithGraph wrap a FeaturedGraph object and a GNN layer as first and second arguments, respectively.","category":"page"},{"location":"tutorials/gcn_fixed_graph/","page":"GCN with Fixed Graph","title":"GCN with Fixed Graph","text":"fg = FeaturedGraph(graph)\nWithGraph(fg, GCNConv(1024=>256, relu))","category":"page"},{"location":"tutorials/gcn_fixed_graph/","page":"GCN with Fixed Graph","title":"GCN with Fixed Graph","text":"This way, we can customize by binding different graph to certain layer and the layer will specialize graph to a required form. For example, a GCNConv layer requires graph in the form of normalized adjacency matrix. Once the graph is bound to a GCNConv layer, it transforms graph into normalized adjacency matrix and stores in WithGraph object. It accelerates training or inference by avoiding calculating transformations. The features in FeaturedGraph object in WithGraph are not used in any layer or model training or inference.","category":"page"},{"location":"tutorials/gcn_fixed_graph/#Array-in,-Array-out","page":"GCN with Fixed Graph","title":"Array in, Array out","text":"","category":"section"},{"location":"tutorials/gcn_fixed_graph/","page":"GCN with Fixed Graph","title":"GCN with Fixed Graph","text":"With this approach, a GNN layer accepts features in array. It takes an array as input and outputs array. Thus, a GNN layer wrapped with WithGraph should accept a feature array, just like regular deep learning model.","category":"page"},{"location":"tutorials/gcn_fixed_graph/#Batch-Learning","page":"GCN with Fixed Graph","title":"Batch Learning","text":"","category":"section"},{"location":"tutorials/gcn_fixed_graph/","page":"GCN with Fixed Graph","title":"GCN with Fixed Graph","text":"Since features are in the form of array, they can be batched up for batched learning. We will demonstrate how to achieve these goals.","category":"page"},{"location":"tutorials/gcn_fixed_graph/#Step-1:-Load-Dataset","page":"GCN with Fixed Graph","title":"Step 1: Load Dataset","text":"","category":"section"},{"location":"tutorials/gcn_fixed_graph/","page":"GCN with Fixed Graph","title":"GCN with Fixed Graph","text":"Different from loading datasets in semi-supervised learning example, we use alldata for supervised learning here and padding=true is added in order to padding features from partial nodes to pseudo-full nodes. A padded features contains zeros in the nodes that are not supposed to be train on.","category":"page"},{"location":"tutorials/gcn_fixed_graph/","page":"GCN with Fixed Graph","title":"GCN with Fixed Graph","text":"train_X, train_y = map(x -> Matrix(x), alldata(Planetoid(), dataset, padding=true))","category":"page"},{"location":"tutorials/gcn_fixed_graph/","page":"GCN with Fixed Graph","title":"GCN with Fixed Graph","text":"We need graph and node indices for training as well.","category":"page"},{"location":"tutorials/gcn_fixed_graph/","page":"GCN with Fixed Graph","title":"GCN with Fixed Graph","text":"g = graphdata(Planetoid(), dataset)\ntrain_idx = 1:size(train_X, 2)","category":"page"},{"location":"tutorials/gcn_fixed_graph/#Step-2:-Batch-up-Features-and-Labels","page":"GCN with Fixed Graph","title":"Step 2: Batch up Features and Labels","text":"","category":"section"},{"location":"tutorials/gcn_fixed_graph/","page":"GCN with Fixed Graph","title":"GCN with Fixed Graph","text":"In order to make batch learning available, we separate graph and node features. We don't subgraph here. Node features are batched up by repeating node features here for demonstration, since planetoid dataset doesn't have batched settings. Different repeat numbers can be specified by train_repeats and train_repeats.","category":"page"},{"location":"tutorials/gcn_fixed_graph/","page":"GCN with Fixed Graph","title":"GCN with Fixed Graph","text":"fg = FeaturedGraph(g)\ntrain_data = (repeat(train_X, outer=(1,1,train_repeats)), repeat(train_y, outer=(1,1,train_repeats)))","category":"page"},{"location":"tutorials/gcn_fixed_graph/#Step-3:-Build-a-GCN-model","page":"GCN with Fixed Graph","title":"Step 3: Build a GCN model","text":"","category":"section"},{"location":"tutorials/gcn_fixed_graph/","page":"GCN with Fixed Graph","title":"GCN with Fixed Graph","text":"Here comes to building a GCN model. We build a model as building a regular Flux model but just wrap GCNConv layer with WithGraph.","category":"page"},{"location":"tutorials/gcn_fixed_graph/","page":"GCN with Fixed Graph","title":"GCN with Fixed Graph","text":"model = Chain(\n    WithGraph(fg, GCNConv(args.input_dim=>args.hidden_dim, relu)),\n    Dropout(0.5),\n    WithGraph(fg, GCNConv(args.hidden_dim=>args.target_dim)),\n)","category":"page"},{"location":"tutorials/gcn_fixed_graph/#Step-4:-Loss-Functions-and-Accuracy","page":"GCN with Fixed Graph","title":"Step 4: Loss Functions and Accuracy","text":"","category":"section"},{"location":"tutorials/gcn_fixed_graph/","page":"GCN with Fixed Graph","title":"GCN with Fixed Graph","text":"Almost all codes are the same as in semi-supervised learning example, except that indices for subgraphing are needed to get partial features out for calculating loss.","category":"page"},{"location":"tutorials/gcn_fixed_graph/","page":"GCN with Fixed Graph","title":"GCN with Fixed Graph","text":"l2norm(x) = sum(abs2, x)\n\nfunction model_loss(model, λ, X, y, idx)\n    loss = logitcrossentropy(model(X)[:,idx,:], y[:,idx,:])\n    loss += λ*sum(l2norm, Flux.params(model[1]))\n    return loss\nend","category":"page"},{"location":"tutorials/gcn_fixed_graph/","page":"GCN with Fixed Graph","title":"GCN with Fixed Graph","text":"And the accuracy measurement also needs indices.","category":"page"},{"location":"tutorials/gcn_fixed_graph/","page":"GCN with Fixed Graph","title":"GCN with Fixed Graph","text":"function accuracy(model, X::AbstractArray, y::AbstractArray, idx)\n    return mean(onecold(softmax(cpu(model(X))[:,idx,:])) .== onecold(cpu(y)[:,idx,:]))\nend\n\naccuracy(model, loader::DataLoader, device, idx) = mean(accuracy(model, X |> device, y |> device, idx) for (X, y) in loader)","category":"page"},{"location":"tutorials/gcn_fixed_graph/#Step-5:-Training-GCN-Model","page":"GCN with Fixed Graph","title":"Step 5: Training GCN Model","text":"","category":"section"},{"location":"tutorials/gcn_fixed_graph/","page":"GCN with Fixed Graph","title":"GCN with Fixed Graph","text":"train_loader, test_loader, fg, train_idx, test_idx = load_data(:cora, args.batch_size)\n\n# optimizer\nopt = ADAM(args.η)\n\n# parameters\nps = Flux.params(model)\n\n# training\ntrain_steps = 0\n@info \"Start Training, total $(args.epochs) epochs\"\nfor epoch = 1:args.epochs\n    @info \"Epoch $(epoch)\"\n\n    for (X, y) in train_loader\n        grad = gradient(() -> model_loss(model, args.λ, X |> device, y |> device, train_idx |> device), ps)\n        Flux.Optimise.update!(opt, ps, grad)\n        train_steps += 1\n    end\nend","category":"page"},{"location":"tutorials/gcn_fixed_graph/","page":"GCN with Fixed Graph","title":"GCN with Fixed Graph","text":"Now we could just train the GCN model directly!","category":"page"},{"location":"abstractions/msgpass/#Message-passing-scheme","page":"Message passing scheme","title":"Message passing scheme","text":"","category":"section"},{"location":"abstractions/msgpass/","page":"Message passing scheme","title":"Message passing scheme","text":"Message passing scheme is a popular GNN scheme in many frameworks. It adapts the property of connectivity of neighbors and form a general approach for spatial graph convolutional neural network. It comprises two user-defined functions and one aggregate function. A message function is defined to process information from edge states and node states from neighbors and itself. Messages from each node are obtained and aggregated by aggregate function to provide node-level information for update function. Update function takes current node state and aggregated message and gives a new node state.","category":"page"},{"location":"abstractions/msgpass/","page":"Message passing scheme","title":"Message passing scheme","text":"Message passing scheme is realized into a abstract type MessagePassing. Any subtype of MessagePassing is a message passing layer which utilize default message and update functions:","category":"page"},{"location":"abstractions/msgpass/","page":"Message passing scheme","title":"Message passing scheme","text":"message(mp, x_i, x_j, e_ij) = x_j\nupdate(mp, m, x) = m","category":"page"},{"location":"abstractions/msgpass/","page":"Message passing scheme","title":"Message passing scheme","text":"mp denotes a message passing layer. message accepts node state x_i for node i and its neighbor state x_j for node j, as well as corresponding edge state e_ij for edge (i,j). The default message function gives all the neighbor state x_j for neighbor of node i. update takes aggregated message m and current node state x, and then outputs m.","category":"page"},{"location":"abstractions/msgpass/","page":"Message passing scheme","title":"Message passing scheme","text":"GeometricFlux.MessagePassing","category":"page"},{"location":"abstractions/msgpass/#GeometricFlux.MessagePassing","page":"Message passing scheme","title":"GeometricFlux.MessagePassing","text":"MessagePassing\n\nAn abstract type for message-passing scheme.\n\nSee also message and update.\n\n\n\n\n\n","category":"type"},{"location":"abstractions/msgpass/#Message-function","page":"Message passing scheme","title":"Message function","text":"","category":"section"},{"location":"abstractions/msgpass/","page":"Message passing scheme","title":"Message passing scheme","text":"A message function accepts feature vector representing node state x_i, feature vectors for neighbor state x_j and corresponding edge state e_ij. A vector is expected to output from message for message. User can override message for customized message passing layer to provide desired behavior.","category":"page"},{"location":"abstractions/msgpass/","page":"Message passing scheme","title":"Message passing scheme","text":"GeometricFlux.message","category":"page"},{"location":"abstractions/msgpass/#GeometricFlux.message","page":"Message passing scheme","title":"GeometricFlux.message","text":"message(mp::MessagePassing, x_i, x_j, e_ij)\n\nMessage function for the message-passing scheme, returning the message from node j to node i . In the message-passing scheme. the incoming messages  from the neighborhood of i will later be aggregated in order to update the features of node i.\n\nBy default, the function returns x_j. Layers subtyping MessagePassing should  specialize this method with custom behavior.\n\nArguments\n\nmp: message-passing layer.\nx_i: the features of node i.\nx_j: the features of the nighbor j of node i.\ne_ij: the features of edge (i, j).\n\nSee also update.\n\n\n\n\n\n","category":"function"},{"location":"abstractions/msgpass/#Aggregate-messages","page":"Message passing scheme","title":"Aggregate messages","text":"","category":"section"},{"location":"abstractions/msgpass/","page":"Message passing scheme","title":"Message passing scheme","text":"Messages from message function are aggregated by an aggregate function. An aggregated message is passed to update function for node-level computation. An aggregate function is given by the following:","category":"page"},{"location":"abstractions/msgpass/","page":"Message passing scheme","title":"Message passing scheme","text":"propagate(mp, fg::FeaturedGraph, aggr::Symbol=:add)","category":"page"},{"location":"abstractions/msgpass/","page":"Message passing scheme","title":"Message passing scheme","text":"propagate function calls the whole message passing layer. fg acts as an input for message passing layer and aggr represents assignment of aggregate function to propagate function. :add represents an aggregate function of addition of all messages.","category":"page"},{"location":"abstractions/msgpass/","page":"Message passing scheme","title":"Message passing scheme","text":"The following aggr are available aggregate functions:","category":"page"},{"location":"abstractions/msgpass/","page":"Message passing scheme","title":"Message passing scheme","text":":add: sum over all messages :sub: negative of sum over all messages :mul: multiplication over all messages :div: inverse of multiplication over all messages :max: the maximum of all messages :min: the minimum of all messages :mean: the average of all messages","category":"page"},{"location":"abstractions/msgpass/#Update-function","page":"Message passing scheme","title":"Update function","text":"","category":"section"},{"location":"abstractions/msgpass/","page":"Message passing scheme","title":"Message passing scheme","text":"An update function takes aggregated message m and current node state x as arguments. An output vector is expected to be the new node state for next layer. User can override update for customized message passing layer to provide desired behavior.","category":"page"},{"location":"abstractions/msgpass/","page":"Message passing scheme","title":"Message passing scheme","text":"GeometricFlux.update","category":"page"},{"location":"abstractions/msgpass/#GeometricFlux.update","page":"Message passing scheme","title":"GeometricFlux.update","text":"update(mp::MessagePassing, m, x)\n\nUpdate function for the message-passing scheme, returning a new set of node features x′ based on old  features x and the incoming message from the neighborhood aggregation m.\n\nBy default, the function returns m. Layers subtyping MessagePassing should  specialize this method with custom behavior.\n\nArguments\n\nmp: message-passing layer.\nm: the aggregated edge messages from the message function.\nx: the node features to be updated.\n\nSee also message.\n\n\n\n\n\n","category":"function"},{"location":"manual/pool/#Graph-Pooling-Layers","page":"Graph Pooling Layers","title":"Graph Pooling Layers","text":"","category":"section"},{"location":"manual/pool/#Global-Pooling-Layer","page":"Graph Pooling Layers","title":"Global Pooling Layer","text":"","category":"section"},{"location":"manual/pool/","page":"Graph Pooling Layers","title":"Graph Pooling Layers","text":"GlobalPool","category":"page"},{"location":"manual/pool/#GeometricFlux.GlobalPool","page":"Graph Pooling Layers","title":"GeometricFlux.GlobalPool","text":"GlobalPool(aggr, dim...)\n\nGlobal pooling layer.\n\nIt pools all features with aggr operation.\n\nArguments\n\naggr: An aggregate function applied to pool all features.\n\n\n\n\n\n","category":"type"},{"location":"manual/pool/","page":"Graph Pooling Layers","title":"Graph Pooling Layers","text":"","category":"page"},{"location":"manual/pool/#Local-Pooling-Layer","page":"Graph Pooling Layers","title":"Local Pooling Layer","text":"","category":"section"},{"location":"manual/pool/","page":"Graph Pooling Layers","title":"Graph Pooling Layers","text":"LocalPool","category":"page"},{"location":"manual/pool/#GeometricFlux.LocalPool","page":"Graph Pooling Layers","title":"GeometricFlux.LocalPool","text":"LocalPool(aggr, cluster)\n\nLocal pooling layer.\n\nIt pools features with aggr operation accroding to cluster. It is implemented with scatter operation.\n\nArguments\n\naggr: An aggregate function applied to pool all features.\ncluster: An index structure which indicates what features to aggregate with.\n\n\n\n\n\n","category":"type"},{"location":"manual/pool/","page":"Graph Pooling Layers","title":"Graph Pooling Layers","text":"","category":"page"},{"location":"manual/pool/#Top-k-Pooling-Layer","page":"Graph Pooling Layers","title":"Top-k Pooling Layer","text":"","category":"section"},{"location":"manual/pool/","page":"Graph Pooling Layers","title":"Graph Pooling Layers","text":"TopKPool","category":"page"},{"location":"manual/pool/#GeometricFlux.TopKPool","page":"Graph Pooling Layers","title":"GeometricFlux.TopKPool","text":"TopKPool(adj, k, in_channel)\n\nTop-k pooling layer.\n\nArguments\n\nadj: Adjacency matrix  of a graph.\nk: Top-k nodes are selected to pool together.\nin_channel: The dimension of input channel.\n\n\n\n\n\n","category":"type"},{"location":"manual/pool/","page":"Graph Pooling Layers","title":"Graph Pooling Layers","text":"Reference: Hongyang Gao, Shuiwang Ji (2019)","category":"page"},{"location":"manual/conv/#Graph-Convolutional-Layers","page":"Graph Convolutional Layers","title":"Graph Convolutional Layers","text":"","category":"section"},{"location":"manual/conv/#Graph-Convolutional-Layer","page":"Graph Convolutional Layers","title":"Graph Convolutional Layer","text":"","category":"section"},{"location":"manual/conv/","page":"Graph Convolutional Layers","title":"Graph Convolutional Layers","text":"X = sigma(hatD^-12 hatA hatD^-12 X Theta)","category":"page"},{"location":"manual/conv/","page":"Graph Convolutional Layers","title":"Graph Convolutional Layers","text":"where hatA = A + I, A denotes the adjacency matrix, and hatD = hatd_ij = sum_j=0 hata_ij is degree matrix.","category":"page"},{"location":"manual/conv/","page":"Graph Convolutional Layers","title":"Graph Convolutional Layers","text":"GCNConv","category":"page"},{"location":"manual/conv/#GeometricFlux.GCNConv","page":"Graph Convolutional Layers","title":"GeometricFlux.GCNConv","text":"GCNConv(in => out, σ=identity; bias=true, init=glorot_uniform)\n\nGraph convolutional layer. The input to the layer is a node feature array X of size (num_features, num_nodes).\n\nArguments\n\nin: The dimension of input features.\nout: The dimension of output features.\nσ: Activation function.\nbias: Add learnable bias.\ninit: Weights' initializer.\n\nExamples\n\njulia> gc = GCNConv(1024=>256, relu)\nGCNConv(1024 => 256, relu)\n\nSee also WithGraph for training layer with static graph.\n\n\n\n\n\n","category":"type"},{"location":"manual/conv/","page":"Graph Convolutional Layers","title":"Graph Convolutional Layers","text":"Reference: Thomas N. Kipf, Max Welling (2017)","category":"page"},{"location":"manual/conv/","page":"Graph Convolutional Layers","title":"Graph Convolutional Layers","text":"","category":"page"},{"location":"manual/conv/#Chebyshev-Spectral-Graph-Convolutional-Layer","page":"Graph Convolutional Layers","title":"Chebyshev Spectral Graph Convolutional Layer","text":"","category":"section"},{"location":"manual/conv/","page":"Graph Convolutional Layers","title":"Graph Convolutional Layers","text":"X = sum^K-1_k=0 Z^(k) Theta^(k)","category":"page"},{"location":"manual/conv/","page":"Graph Convolutional Layers","title":"Graph Convolutional Layers","text":"where Z^(k) is the k-th term of Chebyshev polynomials, and can be calculated by the following recursive form:","category":"page"},{"location":"manual/conv/","page":"Graph Convolutional Layers","title":"Graph Convolutional Layers","text":"Z^(0) = X \nZ^(1) = hatL X \nZ^(k) = 2 hatL Z^(k-1) - Z^(k-2)","category":"page"},{"location":"manual/conv/","page":"Graph Convolutional Layers","title":"Graph Convolutional Layers","text":"and hatL = frac2lambda_max L - I.","category":"page"},{"location":"manual/conv/","page":"Graph Convolutional Layers","title":"Graph Convolutional Layers","text":"ChebConv","category":"page"},{"location":"manual/conv/#GeometricFlux.ChebConv","page":"Graph Convolutional Layers","title":"GeometricFlux.ChebConv","text":"ChebConv(in=>out, k; bias=true, init=glorot_uniform)\n\nChebyshev spectral graph convolutional layer.\n\nArguments\n\nin: The dimension of input features.\nout: The dimension of output features.\nk: The order of Chebyshev polynomial.\nbias: Add learnable bias.\ninit: Weights' initializer.\n\nExamples\n\njulia> cc = ChebConv(1024=>256, 5, relu)\nChebConv(1024 => 256, k=5, relu)\n\nSee also WithGraph for training layer with static graph.\n\n\n\n\n\n","category":"type"},{"location":"manual/conv/","page":"Graph Convolutional Layers","title":"Graph Convolutional Layers","text":"Reference: Michaël Defferrard, Xavier Bresson, Pierre Vandergheynst (2016)","category":"page"},{"location":"manual/conv/","page":"Graph Convolutional Layers","title":"Graph Convolutional Layers","text":"","category":"page"},{"location":"manual/conv/#Graph-Neural-Network-Layer","page":"Graph Convolutional Layers","title":"Graph Neural Network Layer","text":"","category":"section"},{"location":"manual/conv/","page":"Graph Convolutional Layers","title":"Graph Convolutional Layers","text":"textbfx_i = sigma (Theta_1 textbfx_i + sum_j in mathcalN(i) Theta_2 textbfx_j)","category":"page"},{"location":"manual/conv/","page":"Graph Convolutional Layers","title":"Graph Convolutional Layers","text":"GraphConv","category":"page"},{"location":"manual/conv/#GeometricFlux.GraphConv","page":"Graph Convolutional Layers","title":"GeometricFlux.GraphConv","text":"GraphConv(in => out, σ=identity, aggr=+; bias=true, init=glorot_uniform)\n\nGraph neural network layer.\n\nArguments\n\nin: The dimension of input features.\nout: The dimension of output features.\nσ: Activation function.\naggr: An aggregate function applied to the result of message function. +, -,\n\n*, /, max, min and mean are available.\n\nbias: Add learnable bias.\ninit: Weights' initializer.\n\nExamples\n\njulia> GraphConv(1024=>256, relu)\nGraphConv(1024 => 256, relu, aggr=+)\n\njulia> GraphConv(1024=>256, relu, *)\nGraphConv(1024 => 256, relu, aggr=*)\n\nSee also WithGraph for training layer with static graph.\n\n\n\n\n\n","category":"type"},{"location":"manual/conv/","page":"Graph Convolutional Layers","title":"Graph Convolutional Layers","text":"Reference: Christopher Morris, Martin Ritzert, Matthias Fey, William L. Hamilton, Jan Eric Lenssen, Gaurav Rattan, Martin Grohe (2019)","category":"page"},{"location":"manual/conv/","page":"Graph Convolutional Layers","title":"Graph Convolutional Layers","text":"","category":"page"},{"location":"manual/conv/#Graph-Attentional-Layer","page":"Graph Convolutional Layers","title":"Graph Attentional Layer","text":"","category":"section"},{"location":"manual/conv/","page":"Graph Convolutional Layers","title":"Graph Convolutional Layers","text":"textbfx_i = alpha_ii Theta textbfx_i + sum_j in mathcalN(i) alpha_ij Theta textbfx_j","category":"page"},{"location":"manual/conv/","page":"Graph Convolutional Layers","title":"Graph Convolutional Layers","text":"where the attention coefficient alpha_ij can be calculated from","category":"page"},{"location":"manual/conv/","page":"Graph Convolutional Layers","title":"Graph Convolutional Layers","text":"alpha_ij = fracexp(LeakyReLU(textbfa^T Theta textbfx_i  Theta textbfx_j))sum_k in mathcalN(i) cup i exp(LeakyReLU(textbfa^T Theta textbfx_i  Theta textbfx_k))","category":"page"},{"location":"manual/conv/","page":"Graph Convolutional Layers","title":"Graph Convolutional Layers","text":"GATConv","category":"page"},{"location":"manual/conv/#GeometricFlux.GATConv","page":"Graph Convolutional Layers","title":"GeometricFlux.GATConv","text":"GATConv(in => out, σ=identity; heads=1, concat=true,\n        init=glorot_uniform, bias=true, negative_slope=0.2)\n\nGraph attentional layer.\n\nArguments\n\nin: The dimension of input features.\nout: The dimension of output features.\nbias::Bool: Keyword argument, whether to learn the additive bias.\nσ: Activation function.\nheads: Number attention heads \nconcat: Concatenate layer output or not. If not, layer output is averaged.\nnegative_slope::Real: Keyword argument, the parameter of LeakyReLU.\n\nExamples\n\njulia> GATConv(1024=>256, relu)\nGATConv(1024=>256, relu, heads=1, concat=true, LeakyReLU(λ=0.2))\n\njulia> GATConv(1024=>256, relu, heads=4)\nGATConv(1024=>1024, relu, heads=4, concat=true, LeakyReLU(λ=0.2))\n\njulia> GATConv(1024=>256, relu, heads=4, concat=false)\nGATConv(1024=>1024, relu, heads=4, concat=false, LeakyReLU(λ=0.2))\n\njulia> GATConv(1024=>256, relu, negative_slope=0.1f0)\nGATConv(1024=>256, relu, heads=1, concat=true, LeakyReLU(λ=0.1))\n\nSee also WithGraph for training layer with static graph.\n\n\n\n\n\n","category":"type"},{"location":"manual/conv/","page":"Graph Convolutional Layers","title":"Graph Convolutional Layers","text":"Reference: Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, Yoshua Bengio (2018)","category":"page"},{"location":"manual/conv/","page":"Graph Convolutional Layers","title":"Graph Convolutional Layers","text":"","category":"page"},{"location":"manual/conv/#Graph-Attentional-Layer-v2","page":"Graph Convolutional Layers","title":"Graph Attentional Layer v2","text":"","category":"section"},{"location":"manual/conv/","page":"Graph Convolutional Layers","title":"Graph Convolutional Layers","text":"GATv2Conv","category":"page"},{"location":"manual/conv/#GeometricFlux.GATv2Conv","page":"Graph Convolutional Layers","title":"GeometricFlux.GATv2Conv","text":"GATv2Conv(in => out, σ=identity; heads=1, concat=true,\n          init=glorot_uniform, negative_slope=0.2)\n\nGraph attentional layer v2.\n\nArguments\n\nin: The dimension of input features.\nout: The dimension of output features.\nσ: Activation function.\nheads: Number attention heads \nconcat: Concatenate layer output or not. If not, layer output is averaged.\nnegative_slope::Real: Keyword argument, the parameter of LeakyReLU.\n\nExamples\n\njulia> GATv2Conv(1024=>256, relu)\nGATv2Conv(1024=>256, relu, heads=1, concat=true, LeakyReLU(λ=0.2))\n\njulia> GATv2Conv(1024=>256, relu, heads=4)\nGATv2Conv(1024=>1024, relu, heads=4, concat=true, LeakyReLU(λ=0.2))\n\njulia> GATv2Conv(1024=>256, relu, heads=4, concat=false)\nGATv2Conv(1024=>1024, relu, heads=4, concat=false, LeakyReLU(λ=0.2))\n\njulia> GATv2Conv(1024=>256, relu, negative_slope=0.1f0)\nGATv2Conv(1024=>256, relu, heads=1, concat=true, LeakyReLU(λ=0.1))\n\nSee also WithGraph for training layer with static graph.\n\n\n\n\n\n","category":"type"},{"location":"manual/conv/","page":"Graph Convolutional Layers","title":"Graph Convolutional Layers","text":"Reference: Shaked Brody, Uri Alon, Eran Yahav (2022)","category":"page"},{"location":"manual/conv/","page":"Graph Convolutional Layers","title":"Graph Convolutional Layers","text":"","category":"page"},{"location":"manual/conv/#Gated-Graph-Convolution-Layer","page":"Graph Convolutional Layers","title":"Gated Graph Convolution Layer","text":"","category":"section"},{"location":"manual/conv/","page":"Graph Convolutional Layers","title":"Graph Convolutional Layers","text":"textbfh^(0)_i = textbfx_i  textbf0 \ntextbfh^(l)_i = GRU(textbfh^(l-1)_i sum_j in mathcalN(i) Theta textbfh^(l-1)_j)","category":"page"},{"location":"manual/conv/","page":"Graph Convolutional Layers","title":"Graph Convolutional Layers","text":"where textbfh^(l)_i denotes the l-th hidden variables passing through GRU. The dimension of input textbfx_i needs to be less or equal to out.","category":"page"},{"location":"manual/conv/","page":"Graph Convolutional Layers","title":"Graph Convolutional Layers","text":"GatedGraphConv","category":"page"},{"location":"manual/conv/#GeometricFlux.GatedGraphConv","page":"Graph Convolutional Layers","title":"GeometricFlux.GatedGraphConv","text":"GatedGraphConv([fg,] out, num_layers; aggr=+, init=glorot_uniform)\n\nGated graph convolution layer.\n\nArguments\n\nout: The dimension of output features.\nnum_layers: The number of gated recurrent unit.\naggr: An aggregate function applied to the result of message function. +, -,\n\n*, /, max, min and mean are available.\n\nExamples\n\njulia> GatedGraphConv(256, 4)\nGatedGraphConv((256 => 256)^4, aggr=+)\n\njulia> GatedGraphConv(256, 4, aggr=*)\nGatedGraphConv((256 => 256)^4, aggr=*)\n\nSee also WithGraph for training layer with static graph.\n\n\n\n\n\n","category":"type"},{"location":"manual/conv/","page":"Graph Convolutional Layers","title":"Graph Convolutional Layers","text":"Reference: Yujia Li, Daniel Tarlow, Marc Brockschmidt, Richard Zemel (2016)","category":"page"},{"location":"manual/conv/","page":"Graph Convolutional Layers","title":"Graph Convolutional Layers","text":"","category":"page"},{"location":"manual/conv/#Edge-Convolutional-Layer","page":"Graph Convolutional Layers","title":"Edge Convolutional Layer","text":"","category":"section"},{"location":"manual/conv/","page":"Graph Convolutional Layers","title":"Graph Convolutional Layers","text":"textbfx_i = sum_j in mathcalN(i) f_Theta(textbfx_i  textbfx_j - textbfx_i)","category":"page"},{"location":"manual/conv/","page":"Graph Convolutional Layers","title":"Graph Convolutional Layers","text":"where f_Theta denotes a neural network parametrized by Theta, i.e., a MLP.","category":"page"},{"location":"manual/conv/","page":"Graph Convolutional Layers","title":"Graph Convolutional Layers","text":"EdgeConv","category":"page"},{"location":"manual/conv/#GeometricFlux.EdgeConv","page":"Graph Convolutional Layers","title":"GeometricFlux.EdgeConv","text":"EdgeConv(nn; aggr=max)\n\nEdge convolutional layer.\n\nArguments\n\nnn: A neural network (e.g. a Dense layer or a MLP).\naggr: An aggregate function applied to the result of message function.\n\n+, max and mean are available.\n\nExamples\n\njulia> EdgeConv(Dense(1024, 256, relu))\nEdgeConv(Dense(1024 => 256, relu), aggr=max)\n\njulia> EdgeConv(Dense(1024, 256, relu), aggr=+)\nEdgeConv(Dense(1024 => 256, relu), aggr=+)\n\nSee also WithGraph for training layer with static graph.\n\n\n\n\n\n","category":"type"},{"location":"manual/conv/","page":"Graph Convolutional Layers","title":"Graph Convolutional Layers","text":"Reference: Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein, Justin M. Solomon (2019)","category":"page"},{"location":"manual/conv/","page":"Graph Convolutional Layers","title":"Graph Convolutional Layers","text":"","category":"page"},{"location":"manual/conv/#Graph-Isomorphism-Network","page":"Graph Convolutional Layers","title":"Graph Isomorphism Network","text":"","category":"section"},{"location":"manual/conv/","page":"Graph Convolutional Layers","title":"Graph Convolutional Layers","text":"textbfx_i = f_Thetaleft((1 + varepsilon) dot textbfx_i + sum_j in mathcalN(i) textbfx_j right)","category":"page"},{"location":"manual/conv/","page":"Graph Convolutional Layers","title":"Graph Convolutional Layers","text":"where f_Theta denotes a neural network parametrized by Theta, i.e., a MLP.","category":"page"},{"location":"manual/conv/","page":"Graph Convolutional Layers","title":"Graph Convolutional Layers","text":"GINConv","category":"page"},{"location":"manual/conv/#GeometricFlux.GINConv","page":"Graph Convolutional Layers","title":"GeometricFlux.GINConv","text":"GINConv(nn, [eps=0])\n\nGraph Isomorphism Network.\n\nArguments\n\nnn: A neural network/layer.\neps: Weighting factor.\n\nExamples\n\njulia> GINConv(Dense(1024, 256, relu))\nGINConv(Dense(1024 => 256, relu), ϵ=0.0)\n\njulia> GINConv(Dense(1024, 256, relu), 1.f-6)\nGINConv(Dense(1024 => 256, relu), ϵ=1.0e-6)\n\nSee also WithGraph for training layer with static graph.\n\n\n\n\n\n","category":"type"},{"location":"manual/conv/","page":"Graph Convolutional Layers","title":"Graph Convolutional Layers","text":"Reference: Keyulu Xu, Weihua Hu, Jure Leskovec, Stefanie Jegelka (2019)","category":"page"},{"location":"manual/conv/","page":"Graph Convolutional Layers","title":"Graph Convolutional Layers","text":"","category":"page"},{"location":"manual/conv/#Crystal-Graph-Convolutional-Network","page":"Graph Convolutional Layers","title":"Crystal Graph Convolutional Network","text":"","category":"section"},{"location":"manual/conv/","page":"Graph Convolutional Layers","title":"Graph Convolutional Layers","text":"textbfx_i = textbfx_i + sum_j in mathcalN(i) sigmaleft( textbfz_ij textbfW_f + textbfb_f right) odot textsoftplusleft(textbfz_ij textbfW_s + textbfb_s right)","category":"page"},{"location":"manual/conv/","page":"Graph Convolutional Layers","title":"Graph Convolutional Layers","text":"where textbfz_ij = textbfx_i textbfx_j textbfe_ij denotes the concatenation of node features, neighboring node features, and edge features. The operation odot represents elementwise multiplication, and sigma denotes the sigmoid function.","category":"page"},{"location":"manual/conv/","page":"Graph Convolutional Layers","title":"Graph Convolutional Layers","text":"CGConv","category":"page"},{"location":"manual/conv/#GeometricFlux.CGConv","page":"Graph Convolutional Layers","title":"GeometricFlux.CGConv","text":"CGConv((node_dim, edge_dim), init, bias=true)\n\nCrystal Graph Convolutional network. Uses both node and edge features.\n\nArguments\n\nnode_dim: Dimensionality of the input node features. Also is necessarily the output dimensionality.\nedge_dim: Dimensionality of the input edge features.\ninit: Initialization algorithm for each of the weight matrices\nbias: Whether or not to learn an additive bias parameter.\n\nExamples\n\njulia> CGConv((128, 32))\nCGConv(node dim=128, edge dim=32)\n\nSee also WithGraph for training layer with static graph.\n\n\n\n\n\n","category":"type"},{"location":"manual/conv/","page":"Graph Convolutional Layers","title":"Graph Convolutional Layers","text":"Reference: Tian Xie, Jeffrey C. Grossman (2018)","category":"page"},{"location":"manual/conv/","page":"Graph Convolutional Layers","title":"Graph Convolutional Layers","text":"","category":"page"},{"location":"manual/conv/#SAmple-and-aggreGatE-(GraphSAGE)-Network","page":"Graph Convolutional Layers","title":"SAmple and aggreGatE (GraphSAGE) Network","text":"","category":"section"},{"location":"manual/conv/","page":"Graph Convolutional Layers","title":"Graph Convolutional Layers","text":"hattextbfx_j = sample(textbfx_j) forall j in mathcalN(i) \ntextbfm_i = aggregate(hattextbfx_j) \ntextbfx_i = sigma (Theta_1 textbfx_i + Theta_2 textbfm_i)","category":"page"},{"location":"manual/conv/","page":"Graph Convolutional Layers","title":"Graph Convolutional Layers","text":"SAGEConv\nMeanAggregator\nMeanPoolAggregator\nMaxPoolAggregator\nLSTMAggregator","category":"page"},{"location":"manual/conv/#GeometricFlux.SAGEConv","page":"Graph Convolutional Layers","title":"GeometricFlux.SAGEConv","text":"SAGEConv(in => out, σ=identity, aggr=mean; normalize=true, project=false,\n         bias=true, num_sample=10, init=glorot_uniform)\n\nSAmple and aggreGatE convolutional layer for GraphSAGE network.\n\nArguments\n\nin: The dimension of input features.\nout: The dimension of output features.\nσ: Activation function.\naggr: An aggregate function applied to the result of message function. mean, max,\n\nLSTM and GCNConv are available.\n\nnormalize::Bool: Whether to normalize features across all nodes or not.\nproject::Bool: Whether to project, i.e. Dense(in, in), before aggregation.\nbias: Add learnable bias.\nnum_sample::Int: Number of samples for each node from their neighbors.\ninit: Weights' initializer.\n\nExamples\n\njulia> SAGEConv(1024=>256, relu)\nSAGEConv(1024 => 256, relu, aggr=mean, normalize=true, #sample=10)\n\njulia> SAGEConv(1024=>256, relu, num_sample=5)\nSAGEConv(1024 => 256, relu, aggr=mean, normalize=true, #sample=5)\n\njulia> MeanAggregator(1024=>256, relu, normalize=false)\nSAGEConv(1024 => 256, relu, aggr=mean, normalize=false, #sample=10)\n\njulia> MeanPoolAggregator(1024=>256, relu)\nSAGEConv(1024 => 256, relu, project=Dense(1024 => 1024), aggr=mean, normalize=true, #sample=10)\n\njulia> MaxPoolAggregator(1024=>256, relu)\nSAGEConv(1024 => 256, relu, project=Dense(1024 => 1024), aggr=max, normalize=true, #sample=10)\n\njulia> LSTMAggregator(1024=>256, relu)\nSAGEConv(1024 => 256, relu, aggr=LSTMCell(1024 => 1024), normalize=true, #sample=10)\n\nSee also WithGraph for training layer with static graph and MeanAggregator, MeanPoolAggregator, MaxPoolAggregator and LSTMAggregator.\n\n\n\n\n\n","category":"type"},{"location":"manual/conv/#GeometricFlux.MeanAggregator","page":"Graph Convolutional Layers","title":"GeometricFlux.MeanAggregator","text":"MeanAggregator(in => out, σ=identity; normalize=true, project=false,\n               bias=true, num_sample=10, init=glorot_uniform)\n\nSAGEConv with mean aggregator.\n\nSee also SAGEConv.\n\n\n\n\n\n","category":"function"},{"location":"manual/conv/#GeometricFlux.MeanPoolAggregator","page":"Graph Convolutional Layers","title":"GeometricFlux.MeanPoolAggregator","text":"MeanAggregator(in => out, σ=identity; normalize=true,\n               bias=true, num_sample=10, init=glorot_uniform)\n\nSAGEConv with meanpool aggregator.\n\nSee also SAGEConv.\n\n\n\n\n\n","category":"function"},{"location":"manual/conv/#GeometricFlux.MaxPoolAggregator","page":"Graph Convolutional Layers","title":"GeometricFlux.MaxPoolAggregator","text":"MeanAggregator(in => out, σ=identity; normalize=true,\n               bias=true, num_sample=10, init=glorot_uniform)\n\nSAGEConv with maxpool aggregator.\n\nSee also SAGEConv.\n\n\n\n\n\n","category":"function"},{"location":"manual/conv/#GeometricFlux.LSTMAggregator","page":"Graph Convolutional Layers","title":"GeometricFlux.LSTMAggregator","text":"LSTMAggregator(in => out, σ=identity; normalize=true, project=false,\n               bias=true, num_sample=10, init=glorot_uniform)\n\nSAGEConv with LSTM aggregator.\n\nSee also SAGEConv.\n\n\n\n\n\n","category":"function"},{"location":"manual/conv/","page":"Graph Convolutional Layers","title":"Graph Convolutional Layers","text":"Reference: William L Hamilton, Rex Ying, Jure Leskovec (2017) and GraphSAGE website","category":"page"},{"location":"tutorials/deepset/#Predicting-Digits-Sum-from-DeepSet-model","page":"DeepSet for Digit Sum","title":"Predicting Digits Sum from DeepSet model","text":"","category":"section"},{"location":"tutorials/deepset/","page":"DeepSet for Digit Sum","title":"DeepSet for Digit Sum","text":"Digits sum is a task of summing up digits in images or text. This example demonstrates summing up digits in arbitrary number of MNIST images. To accomplish such task, DeepSet model is suitable for this task. DeepSet model is excellent at the task which takes a set of objects and reduces them into single object.","category":"page"},{"location":"tutorials/deepset/#Step-1:-Load-MNIST-Dataset","page":"DeepSet for Digit Sum","title":"Step 1: Load MNIST Dataset","text":"","category":"section"},{"location":"tutorials/deepset/","page":"DeepSet for Digit Sum","title":"DeepSet for Digit Sum","text":"Since a DeepSet model predicts the summation from a set of images, we have to prepare training dataset composed of a random-sized set of images and a summed result.","category":"page"},{"location":"tutorials/deepset/","page":"DeepSet for Digit Sum","title":"DeepSet for Digit Sum","text":"First, the whole dataset is loaded from MLDatasets.jl and then shuffled before generating training dataset.","category":"page"},{"location":"tutorials/deepset/","page":"DeepSet for Digit Sum","title":"DeepSet for Digit Sum","text":"train_X, train_y = MLDatasets.MNIST.traindata(Float32)\ntrain_X, train_y = shuffle_data(train_X, train_y)","category":"page"},{"location":"tutorials/deepset/","page":"DeepSet for Digit Sum","title":"DeepSet for Digit Sum","text":"The generate_featuredgraphs here generates a set of pairs which contains a FeaturedGraph and a summed number for prediction target. In a FeaturedGraph, an arbitrary number of MNIST images are collected as node features and corresponding nodes are collected in a graph without edges.","category":"page"},{"location":"tutorials/deepset/","page":"DeepSet for Digit Sum","title":"DeepSet for Digit Sum","text":"train_data = generate_featuredgraphs(train_X, train_y, num_train_examples, 1:train_max_length)","category":"page"},{"location":"tutorials/deepset/","page":"DeepSet for Digit Sum","title":"DeepSet for Digit Sum","text":"num_train_examples is the parameter for assigning how many training example to generate. 1:train_max_length specifies the range of number of images to contained in one example.","category":"page"},{"location":"tutorials/deepset/#Step-2:-Build-a-DeepSet-model","page":"DeepSet for Digit Sum","title":"Step 2: Build a DeepSet model","text":"","category":"section"},{"location":"tutorials/deepset/","page":"DeepSet for Digit Sum","title":"DeepSet for Digit Sum","text":"A DeepSet takes a set of objects and outputs single object. To make a model accept a set of objects, the model input must be invariant to permutation. The DeepSet model is simply composed of two parts: phi network and rho network. ","category":"page"},{"location":"tutorials/deepset/","page":"DeepSet for Digit Sum","title":"DeepSet for Digit Sum","text":"Z = rho ( sum_x_i in mathcalV phi (x_i) )","category":"page"},{"location":"tutorials/deepset/","page":"DeepSet for Digit Sum","title":"DeepSet for Digit Sum","text":"phi network embeds every images and they are summed up to be a single embedding. Permutation invariance comes from the use of summation. In general, a commutative binary operator can be used to reduce a set of embeddings into one embedding. Finally, rho network decodes the embedding to a number.","category":"page"},{"location":"tutorials/deepset/","page":"DeepSet for Digit Sum","title":"DeepSet for Digit Sum","text":"ϕ = Chain(\n    Dense(args.input_dim, args.hidden_dims[1], tanh),\n    Dense(args.hidden_dims[1], args.hidden_dims[2], tanh),\n    Dense(args.hidden_dims[2], args.hidden_dims[3], tanh),\n)\nρ = Dense(args.hidden_dims[3], args.target_dim)\nmodel = DeepSet(ϕ, ρ) |> device","category":"page"},{"location":"tutorials/deepset/#Step-3:-Loss-Functions","page":"DeepSet for Digit Sum","title":"Step 3: Loss Functions","text":"","category":"section"},{"location":"tutorials/deepset/","page":"DeepSet for Digit Sum","title":"DeepSet for Digit Sum","text":"Mean absolute error is used as the loss function. Since the model outputs a FeaturedGraph, the prediction is placed as a global feature in FeaturedGraph.","category":"page"},{"location":"tutorials/deepset/","page":"DeepSet for Digit Sum","title":"DeepSet for Digit Sum","text":"function model_loss(model, batch)\n    ŷ = vcat(map(x -> global_feature(model(x[1])), batch)...)\n    y = vcat(map(x -> x[2], batch)...)\n    return mae(ŷ, y)\nend","category":"page"},{"location":"tutorials/deepset/#Step-4:-Training-DeepSet-Model","page":"DeepSet for Digit Sum","title":"Step 4: Training DeepSet Model","text":"","category":"section"},{"location":"tutorials/deepset/","page":"DeepSet for Digit Sum","title":"DeepSet for Digit Sum","text":"# optimizer\nopt = ADAM(args.η)\n\n# parameters\nps = Flux.params(model)\n\n# training\n@info \"Start Training, total $(args.epochs) epochs\"\nfor epoch = 1:args.epochs\n    @info \"Epoch $(epoch)\"\n\n    for batch in train_loader\n        train_loss, back = Flux.pullback(ps) do\n            model_loss(model, batch |> device)\n        end\n        test_loss = model_loss(model, test_loader, device)\n        grad = back(1f0)\n        Flux.Optimise.update!(opt, ps, grad)\n    end\nend","category":"page"},{"location":"tutorials/deepset/","page":"DeepSet for Digit Sum","title":"DeepSet for Digit Sum","text":"For a complete example, please check examples/digitsum_deepsets.jl.","category":"page"},{"location":"basics/random_graph/#Random-Graphs","page":"Random graphs","title":"Random Graphs","text":"","category":"section"},{"location":"basics/random_graph/#Random-Graph-Generation","page":"Random graphs","title":"Random Graph Generation","text":"","category":"section"},{"location":"basics/random_graph/","page":"Random graphs","title":"Random graphs","text":"A graph is needed as input for a GNN model. Random graph can be generated, which is provided by Graphs.jl package. A random graph can be generated by erdos_renyi model.","category":"page"},{"location":"basics/random_graph/","page":"Random graphs","title":"Random graphs","text":"julia> using Graphs\n\njulia> g = erdos_renyi(10, 30)\n{10, 30} undirected simple Int64 graph","category":"page"},{"location":"basics/random_graph/","page":"Random graphs","title":"Random graphs","text":"To construct a FeaturedGraph object, just put the graph object and its corresponding features into it.","category":"page"},{"location":"basics/random_graph/","page":"Random graphs","title":"Random graphs","text":"julia> X = rand(Float32, 5, 10);\n\njulia> fg = FeaturedGraph(g, nf=X)\nFeaturedGraph:\n\tUndirected graph with (#V=10, #E=30) in adjacency matrix\n\tNode feature:\tℝ^5 <Matrix{Float32}>","category":"page"},{"location":"basics/random_graph/","page":"Random graphs","title":"Random graphs","text":"Various random graph with different generating model can be used here.","category":"page"},{"location":"basics/random_graph/","page":"Random graphs","title":"Random graphs","text":"julia> barabasi_albert(10, 3)\n{10, 21} undirected simple Int64 graph\n\njulia> watts_strogatz(10, 4, 0.3)\n{10, 20} undirected simple Int64 graph","category":"page"},{"location":"basics/random_graph/","page":"Random graphs","title":"Random graphs","text":"barabasi_albert generates graphs from scale-free network model, while watts_strogatz generates graphs from small-world model.","category":"page"},{"location":"basics/random_graph/#Common-Graphs","page":"Random graphs","title":"Common Graphs","text":"","category":"section"},{"location":"basics/random_graph/","page":"Random graphs","title":"Random graphs","text":"There are commonly used graphs listed here.","category":"page"},{"location":"basics/random_graph/","page":"Random graphs","title":"Random graphs","text":"clique_graph(k, n)\ncomplete_graph(n)\ngrid(dims; periodic=false)\npath_digraph(n)\npath_graph(n)","category":"page"},{"location":"manual/linalg/#Linear-Algebra","page":"Linear Algebra","title":"Linear Algebra","text":"","category":"section"},{"location":"manual/linalg/","page":"Linear Algebra","title":"Linear Algebra","text":"GraphSignals.degrees\nGraphSignals.degree_matrix\nGraphSignals.normalized_adjacency_matrix\nGraphSignals.laplacian_matrix\nGraphSignals.normalized_laplacian\nGraphSignals.scaled_laplacian\nGraphSignals.random_walk_laplacian\nGraphSignals.signless_laplacian","category":"page"},{"location":"manual/linalg/#GraphSignals.degrees","page":"Linear Algebra","title":"GraphSignals.degrees","text":"degrees(g, [T]; dir=:out)\n\nDegree of each vertex. Return a vector which contains the degree of each vertex in graph g.\n\nArguments\n\ng: should be a adjacency matrix, SimpleGraph, SimpleDiGraph (from Graphs) or   SimpleWeightedGraph, SimpleWeightedDiGraph (from SimpleWeightedGraphs).\nT: result element type of degree vector; default is the element type of g (optional).\ndir: direction of degree; should be :in, :out, or :both (optional).\n\nExamples\n\njulia> using GraphSignals\n\njulia> m = [0 1 1; 1 0 0; 1 0 0];\n\njulia> GraphSignals.degrees(m)\n3-element Vector{Int64}:\n 2\n 1\n 1\n\n\n\n\n\n","category":"function"},{"location":"manual/linalg/#GraphSignals.degree_matrix","page":"Linear Algebra","title":"GraphSignals.degree_matrix","text":"degree_matrix(g, [T]; dir=:out)\n\nDegree matrix of graph g. Return a matrix which contains degrees of each vertex in its diagonal. The values other than diagonal are zeros.\n\nArguments\n\ng: should be a adjacency matrix, FeaturedGraph, SimpleGraph, SimpleDiGraph (from Graphs)   or SimpleWeightedGraph, SimpleWeightedDiGraph (from SimpleWeightedGraphs).\nT: result element type of degree vector; default is the element type of g (optional).\ndir: direction of degree; should be :in, :out, or :both (optional).\n\nExamples\n\njulia> using GraphSignals\n\njulia> m = [0 1 1; 1 0 0; 1 0 0];\n\njulia> GraphSignals.degree_matrix(m)\n3×3 LinearAlgebra.Diagonal{Int64, Vector{Int64}}:\n 2  ⋅  ⋅\n ⋅  1  ⋅\n ⋅  ⋅  1\n\n\n\n\n\n","category":"function"},{"location":"manual/linalg/#GraphSignals.normalized_adjacency_matrix","page":"Linear Algebra","title":"GraphSignals.normalized_adjacency_matrix","text":"normalized_adjacency_matrix(g, [T]; selfloop=false)\n\nNormalized adjacency matrix of graph g.\n\nArguments\n\ng: should be a adjacency matrix, FeaturedGraph, SimpleGraph, SimpleDiGraph (from Graphs)   or SimpleWeightedGraph, SimpleWeightedDiGraph (from SimpleWeightedGraphs).\nT: result element type of degree vector; default is the element type of g (optional).\nselfloop: adding self loop while calculating the matrix (optional).\n\n\n\n\n\n","category":"function"},{"location":"manual/linalg/#Graphs.LinAlg.laplacian_matrix","page":"Linear Algebra","title":"Graphs.LinAlg.laplacian_matrix","text":"laplacian_matrix(g, [T]; dir=:out)\n\nLaplacian matrix of graph g.\n\nArguments\n\ng: should be a adjacency matrix, FeaturedGraph, SimpleGraph, SimpleDiGraph (from Graphs)   or SimpleWeightedGraph, SimpleWeightedDiGraph (from SimpleWeightedGraphs).\nT: result element type of degree vector; default is the element type of g (optional).\ndir: direction of degree; should be :in, :out, or :both (optional).\n\n\n\n\n\n","category":"function"},{"location":"manual/linalg/#GraphSignals.normalized_laplacian","page":"Linear Algebra","title":"GraphSignals.normalized_laplacian","text":"normalized_laplacian(g, [T]; dir=:both, selfloop=false)\n\nNormalized Laplacian matrix of graph g.\n\nArguments\n\ng: should be a adjacency matrix, FeaturedGraph, SimpleGraph, SimpleDiGraph (from Graphs)   or SimpleWeightedGraph, SimpleWeightedDiGraph (from SimpleWeightedGraphs).\nT: result element type of degree vector; default is the element type of g (optional).\nselfloop: adding self loop while calculating the matrix (optional).\ndir: direction of graph; should be :in or :out (optional).\n\n\n\n\n\n","category":"function"},{"location":"manual/linalg/#GraphSignals.scaled_laplacian","page":"Linear Algebra","title":"GraphSignals.scaled_laplacian","text":"scaled_laplacian(g, [T])\n\nScaled Laplacien matrix of graph g, defined as hatL = frac2lambda_max L - I where L is the normalized Laplacian matrix.\n\nArguments\n\ng: should be a adjacency matrix, FeaturedGraph, SimpleGraph, SimpleDiGraph (from Graphs)   or SimpleWeightedGraph, SimpleWeightedDiGraph (from SimpleWeightedGraphs).\nT: result element type of degree vector; default is the element type of g (optional).\n\n\n\n\n\n","category":"function"},{"location":"manual/linalg/#GraphSignals.random_walk_laplacian","page":"Linear Algebra","title":"GraphSignals.random_walk_laplacian","text":"random_walk_laplacian(g, [T]; dir=:out)\n\nRandom walk normalized Laplacian matrix of graph g.\n\nArguments\n\ng: should be a adjacency matrix, FeaturedGraph, SimpleGraph, SimpleDiGraph (from Graphs)   or SimpleWeightedGraph, SimpleWeightedDiGraph (from SimpleWeightedGraphs).\nT: result element type of degree vector; default is the element type of g (optional).\ndir: direction of degree; should be :in, :out, or :both (optional).\n\n\n\n\n\n","category":"function"},{"location":"manual/linalg/#GraphSignals.signless_laplacian","page":"Linear Algebra","title":"GraphSignals.signless_laplacian","text":"signless_laplacian(g, [T]; dir=:out)\n\nSignless Laplacian matrix of graph g.\n\nArguments\n\ng: should be a adjacency matrix, FeaturedGraph, SimpleGraph, SimpleDiGraph (from Graphs)   or SimpleWeightedGraph, SimpleWeightedDiGraph (from SimpleWeightedGraphs).\nT: result element type of degree vector; default is the element type of g (optional).\ndir: direction of degree; should be :in, :out, or :both (optional).\n\n\n\n\n\n","category":"function"},{"location":"cooperate/#Cooperate-with-Flux-Layers","page":"Cooperate with Flux Layers","title":"Cooperate with Flux Layers","text":"","category":"section"},{"location":"cooperate/","page":"Cooperate with Flux Layers","title":"Cooperate with Flux Layers","text":"GeometricFlux is designed to be compatible with Flux layers. Flux layers usually have array input and array output. Since the mechanism of \"what you feed is what you get\", the API for array type is compatible directly with other Flux layers. However, the API for FeaturedGraph is not compatible directly.","category":"page"},{"location":"cooperate/#Fetching-Features-from-[FeaturedGraph](@ref)-and-Output-Compatible-Result-with-Flux-Layers","page":"Cooperate with Flux Layers","title":"Fetching Features from FeaturedGraph and Output Compatible Result with Flux Layers","text":"","category":"section"},{"location":"cooperate/","page":"Cooperate with Flux Layers","title":"Cooperate with Flux Layers","text":"With a layer outputs a FeaturedGraph, it is not compatible with Flux layers. Since Flux layers need single feature in array form as input, node features, edge features and global features can be selected by using FeaturedGraph APIs: node_feature, edge_feature or global_feature, respectively.","category":"page"},{"location":"cooperate/","page":"Cooperate with Flux Layers","title":"Cooperate with Flux Layers","text":"model = Chain(\n    GCNConv(1024=>256, relu),\n    node_feature,  # or edge_feature or global_feature\n    softmax\n)","category":"page"},{"location":"cooperate/","page":"Cooperate with Flux Layers","title":"Cooperate with Flux Layers","text":"In a multitask learning scenario, multiple outputs are required. A branching selection of features can be made as follows:","category":"page"},{"location":"cooperate/","page":"Cooperate with Flux Layers","title":"Cooperate with Flux Layers","text":"model = Chain(\n    GCNConv(1024=>256, relu),\n    x -> (node_feature(x), global_feature(x)),\n    (nf, gf) -> (softmax(nf), identity.(gf))\n)","category":"page"},{"location":"cooperate/#Branching-Different-Features-Through-Different-Layers","page":"Cooperate with Flux Layers","title":"Branching Different Features Through Different Layers","text":"","category":"section"},{"location":"cooperate/","page":"Cooperate with Flux Layers","title":"Cooperate with Flux Layers","text":"A GraphParallel construct is designed for passing each feature through different layers from a FeaturedGraph. An example is given as follow:","category":"page"},{"location":"cooperate/","page":"Cooperate with Flux Layers","title":"Cooperate with Flux Layers","text":"Flux.Chain(\n    ...\n    GraphParallel(\n        node_layer=Dropout(0.5),\n        edge_layer=Dense(1024, 256, relu),\n        global_layer=identity,\n    ),\n    ...\n)","category":"page"},{"location":"cooperate/","page":"Cooperate with Flux Layers","title":"Cooperate with Flux Layers","text":"GraphParallel will pass node feature to a Dropout layer and edge feature to a Dense layer. Meanwhile, a FeaturedGraph is decomposed and keep the graph in FeaturedGraph to the downstream layers. A new FeaturedGraph is constructed with processed node feature, edge feature and global feature. GraphParallel acts as a layer which accepts a FeaturedGraph and output a FeaturedGraph. Thus, it by pass the graph in a FeaturedGraph but pass different features to different layers.","category":"page"},{"location":"cooperate/","page":"Cooperate with Flux Layers","title":"Cooperate with Flux Layers","text":"GeometricFlux.GraphParallel","category":"page"},{"location":"cooperate/#GeometricFlux.GraphParallel","page":"Cooperate with Flux Layers","title":"GeometricFlux.GraphParallel","text":"GraphParallel(; node_layer=identity, edge_layer=identity, global_layer=identity)\n\nPassing features in FeaturedGraph in parallel. It takes FeaturedGraph as input and it can be specified by assigning layers for specific (node, edge and global) features.\n\nArguments\n\nnode_layer: A regular Flux layer for passing node features.\nedge_layer: A regular Flux layer for passing edge features.\nglobal_layer: A regular Flux layer for passing global features.\n\nExample\n\njulia> using Flux, GeometricFlux\n\njulia> l = GraphParallel(\n            node_layer=Dropout(0.5),\n            global_layer=Dense(10, 5)\n       )\nGraphParallel(node_layer=Dropout(0.5), edge_layer=identity, global_layer=Dense(10 => 5))\n\n\n\n\n\n","category":"type"},{"location":"dynamicgraph/#Dynamic-Graph-Update","page":"Dynamic Graph Update","title":"Dynamic Graph Update","text":"","category":"section"},{"location":"dynamicgraph/","page":"Dynamic Graph Update","title":"Dynamic Graph Update","text":"Dynamic graph update is a technique to generate a new graph within a graph convolutional layer proposed by Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein, Justin M. Solomon (2019).","category":"page"},{"location":"dynamicgraph/","page":"Dynamic Graph Update","title":"Dynamic Graph Update","text":"Most of manifold learning approaches aims to learn capture manifold structures in high dimensional space. They construct a graph to approximate the manifold and learn to reduce dimensions of space. The separation of capturing manifold and learning dimensional reduction limits the power of manifold learning. Thus, latent graph learning is proposed to learn the manifold and dimensional reduction simultaneously. The latent graph learning is also named as manifold learning 2.0 which leverages the power of graph neural network and learns latent graph structure within layers of a graph neural network.","category":"page"},{"location":"dynamicgraph/","page":"Dynamic Graph Update","title":"Dynamic Graph Update","text":"Latent graph learning learns the latent graph through training over point cloud, or a set of features. A fixed graph structure is not provided to a GNN model. Latent graph is dynamically constructed by constructing a neighborhood graph using features in graph convolutional layers. After construction of neighborhood graph, the neighborhood graph is fed as input with features into a graph convolutional layer.","category":"page"},{"location":"dynamicgraph/","page":"Dynamic Graph Update","title":"Dynamic Graph Update","text":"Currently, we support k-nearest neighbor method to construct a neighborhood graph. To use dynamic graph update, just replace the static graph strategy","category":"page"},{"location":"dynamicgraph/","page":"Dynamic Graph Update","title":"Dynamic Graph Update","text":"WithGraph(fg, EdgeConv(Dense(2*in_channel, out_channel)))","category":"page"},{"location":"dynamicgraph/","page":"Dynamic Graph Update","title":"Dynamic Graph Update","text":"as graph construction method.","category":"page"},{"location":"dynamicgraph/","page":"Dynamic Graph Update","title":"Dynamic Graph Update","text":"WithGraph(\n    EdgeConv(Dense(2*in_channel, out_channel)),\n    dynamic=X -> GraphSignals.kneighbors_graph(X, 3)\n)","category":"page"},{"location":"introduction/#Introduction-to-Graph-Neural-Networks-(GNN)","page":"Introduction","title":"Introduction to Graph Neural Networks (GNN)","text":"","category":"section"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"Graph neural networks act as standalone network architecture other than convolutional neural networks (CNN), recurrent neural networks (RNN). As its name implies, GNN needs a graph as training data. The problem setting requires at least a graph to train on.","category":"page"},{"location":"introduction/#What-is-Graph-Neural-Networks?","page":"Introduction","title":"What is Graph Neural Networks?","text":"","category":"section"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"Graph convolutional layers is the building block for GNN, and it extends from classic convolutional layer. Convolutional layer performs convolution operation over regular grid geometry, e.g. pixels in images arrange regularly along vertical and horizontal directions, while graph convolutional performs convolutional operation over irregular graph topology, e.g. graph is composed of a set of nodes which connect to each other with edges. In signal processing field, images are viewed as a kind of signals. Precisely, image can be represented as a function which maps from coordinates to color for each pixel. A matrix satisfies the definition and it maps image indices to a RGB value representing each pixel. Analogically, a graph signal can be defined as a function which maps from node/edge in a graph to certain value or features. We call them node features or edge features if the features correspond to node or edge, respectively. Graph convolutional layer maps features on nodes or edges to their embeddings.","category":"page"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"<figure>\n    <img src=\"../assets/geometry.svg\" width=\"50%\" alt=\"geometry.svg\" /><br>\n    <figcaption><em>Geometry for images and graphs.</em></figcaption>\n</figure>","category":"page"},{"location":"introduction/#What-is-the-Difference-between-Deep-Learning-and-GNN?","page":"Introduction","title":"What is the Difference between Deep Learning and GNN?","text":"","category":"section"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"Practically, GNN requires graph to be input in a certain form and features will be mapped according to the input graph, while classic deep learning architecture doesn't require a graph or a geometric object as input. In the design of GeometricFlux, the input graph can be two kinds: static graph or variable graph. A static graph is carried within a GNN layer, while a variable graph can be carried with features. The concept of a static graph defines the graph topology in the GNN layer and view it as a built-in static topology for a layer. The concept of variable graph is, totally different from static graph, to consider graph as a part of input data, which is more nature to most of people.","category":"page"},{"location":"introduction/#Features-for-GNNs","page":"Introduction","title":"Features for GNNs","text":"","category":"section"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"Graph signals include node signals, edge signals and global (or graph) signals. According to the problem setting, signals are further classified as features or labels. Features that can be used in GNN includes node features, edge features and global (or graph) features. Global (or graph) features are features that corresponds the whole graph and represents the status of a graph.","category":"page"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"<figure>\n    <img src=\"../assets/graph signals.svg\" width=\"70%\" alt=\"graph signals.svg\" /><br>\n    <figcaption><em>Signals and graph signals.</em></figcaption>\n</figure>","category":"page"},{"location":"introduction/#Variable-graph:-[FeaturedGraph](@ref)-as-Container-for-Graph-and-Features","page":"Introduction","title":"Variable graph: FeaturedGraph as Container for Graph and Features","text":"","category":"section"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"A GNN model accepts a graph and features as input. To this end, FeaturedGraph object is designed as a container for graph and various kinds of features. It can be passed to a GNN model directly.","category":"page"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"T = Float32\nfg = FeaturedGraph(g, nf=rand(10, 5), ef=rand(7, 11), gf=)","category":"page"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"It is worth noting that it is better to convert element type of graph to Float32 explicitly. It can avoid some issues when training or inferring a GNN model.","category":"page"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"train_data = [(FeaturedGraph(g, nf=train_X), train_y) for _ in 1:N]","category":"page"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"A set of FeaturedGraph can include different graph structures g and different features train_X and then pass into the same GNN model in order to train/infer on variable graphs.","category":"page"},{"location":"introduction/#Build-GNN-Model","page":"Introduction","title":"Build GNN Model","text":"","category":"section"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"model = Chain(\n    GCNConv(input_dim=>hidden_dim, relu),\n    GraphParallel(node_layer=Dropout(0.5)),\n    GCNConv(hidden_dim=>target_dim),\n    node_feature,\n)","category":"page"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"A GNN model can be built by stacking GNN layers with or without regular Flux layers. Regular Flux layers should be wrapped in GraphParallel and specified as node_layer which is applied to node features.","category":"page"},{"location":"basics/layers/#Building-Graph-Neural-Networks","page":"Building Layers","title":"Building Graph Neural Networks","text":"","category":"section"},{"location":"basics/layers/","page":"Building Layers","title":"Building Layers","text":"Building GNN is as simple as building neural network in Flux. The syntax here is the same as Flux. Chain is used to stack layers into a GNN. A simple example is shown here:","category":"page"},{"location":"basics/layers/","page":"Building Layers","title":"Building Layers","text":"model = Chain(\n    GCNConv(feat=>h1),\n    GCNConv(h1=>h2, relu),\n)","category":"page"},{"location":"basics/layers/","page":"Building Layers","title":"Building Layers","text":"In the example above, the feature dimension in first layer is mapped from feat to h1. In second layer, h1 is then mapped to h2. Default activation function is given as identity if it is not specified by users.","category":"page"},{"location":"basics/layers/","page":"Building Layers","title":"Building Layers","text":"The initialization function GCNConv(...) constructs a GCNConv layer. For most of the layer types in GeometricFlux, a layer can be initialized in two ways:","category":"page"},{"location":"basics/layers/","page":"Building Layers","title":"Building Layers","text":"GNN layer without graph: initializing without a predefined graph topology. This allows the layer to accept different graph topology.\nGNN layer with static graph: initializing with a predefined graph topology, e.g. graph wrapped in FeaturedGraph. This strategy is suitable for datasets where each input requires the same graph structure and it has better performance than variable graph strategy.","category":"page"},{"location":"basics/layers/","page":"Building Layers","title":"Building Layers","text":"The example above demonstrate the variable graph strategy. The equivalent GNN architecture but with static graph strategy is shown as following:","category":"page"},{"location":"basics/layers/","page":"Building Layers","title":"Building Layers","text":"model = Chain(\n    WithGraph(fg, GCNConv(feat=>h1)),\n    WithGraph(fg, GCNConv(h1=>h2, relu)),\n)","category":"page"},{"location":"basics/layers/","page":"Building Layers","title":"Building Layers","text":"GeometricFlux.WithGraph","category":"page"},{"location":"basics/layers/#GeometricFlux.WithGraph","page":"Building Layers","title":"GeometricFlux.WithGraph","text":"WithGraph([g], layer; dynamic=nothing)\n\nTrain GNN layers with static graph.\n\nArguments\n\ng: If a FeaturedGraph is given, a fixed graph is used to train with.\nlayer: A GNN layer.\ndynamic: If a function is given, it enables dynamic graph update by constructing\n\ndynamic graph through given function within layers.\n\nExample\n\njulia> using GraphSignals, GeometricFlux\n\njulia> adj = [0 1 0 1;\n              1 0 1 0;\n              0 1 0 1;\n              1 0 1 0];\n\njulia> fg = FeaturedGraph(adj);\n\njulia> gc = WithGraph(fg, GCNConv(1024=>256))  # graph preprocessed by adding self loops\nWithGraph(Graph(#V=4, #E=8), GCNConv(1024 => 256))\n\njulia> WithGraph(fg, Dense(10, 5))\nDense(10 => 5)      # 55 parameters\n\njulia> model = Chain(\n           GCNConv(32=>32),\n           gc,\n       );\n\njulia> WithGraph(fg, model)\nChain(\n  WithGraph(\n    GCNConv(32 => 32),                  # 1_056 parameters\n  ),\n  WithGraph(\n    GCNConv(1024 => 256),               # 262_400 parameters\n  ),\n)         # Total: 4 trainable arrays, 263_456 parameters,\n          # plus 2 non-trainable, 32 parameters, summarysize 1.006 MiB.\n\n\n\n\n\n","category":"type"},{"location":"basics/layers/#Applying-Layers","page":"Building Layers","title":"Applying Layers","text":"","category":"section"},{"location":"basics/layers/","page":"Building Layers","title":"Building Layers","text":"When using GNN layers, the general guidelines are:","category":"page"},{"location":"basics/layers/","page":"Building Layers","title":"Building Layers","text":"With static graph strategy: you should pass in a d times n times batch matrix for node features, and the layer maps node features mathbbR^d rightarrow mathbbR^k then the output will be in matrix with dimensions k times n times batch. The same ostensibly goes for edge features but as of now no layer type supports outputting new edge features.\nWith variable graph strategy: you should pass in a FeaturedGraph, the output will be also be a FeaturedGraph with modified node (and/or edge) features. Add node_feature as the following entry in the Flux chain (or simply call node_feature() on the output) if you wish to subsequently convert them to matrix form.","category":"page"},{"location":"basics/layers/#Define-Your-Own-GNN-Layer","page":"Building Layers","title":"Define Your Own GNN Layer","text":"","category":"section"},{"location":"basics/layers/","page":"Building Layers","title":"Building Layers","text":"Customizing your own GNN layers are the same as defining a layer in Flux. You may want to check Flux documentation first.","category":"page"},{"location":"basics/layers/","page":"Building Layers","title":"Building Layers","text":"To define a customized GNN layer, for example, we take a simple GCNConv layer as example here.","category":"page"},{"location":"basics/layers/","page":"Building Layers","title":"Building Layers","text":"struct GCNConv <: AbstractGraphLayer\n    weight\n    bias\n    σ\nend\n\n@functor GCNConv","category":"page"},{"location":"basics/layers/","page":"Building Layers","title":"Building Layers","text":"We first should define a GCNConv type and let it be the subtype of AbstractGraphLayer. In this type, it holds parameters that a layer operate on. Don't forget to add @functor macro to GCNConv type.","category":"page"},{"location":"basics/layers/","page":"Building Layers","title":"Building Layers","text":"(l::GCNConv)(Ã::AbstractMatrix, x::AbstractMatrix) = l.σ.(l.weight * x * Ã .+ l.bias)","category":"page"},{"location":"basics/layers/","page":"Building Layers","title":"Building Layers","text":"Then, we can define the operation for GCNConv layer.","category":"page"},{"location":"basics/layers/","page":"Building Layers","title":"Building Layers","text":"function (l::GCNConv)(fg::AbstractFeaturedGraph)\n    nf = node_feature(fg)\n    Ã = Zygote.ignore() do\n        GraphSignals.normalized_adjacency_matrix(fg, eltype(nf); selfloop=true)\n    end\n    return ConcreteFeaturedGraph(fg, nf = l(Ã, nf))\nend","category":"page"},{"location":"basics/layers/","page":"Building Layers","title":"Building Layers","text":"Here comes to the GNN-specific behaviors. A GNN layer should accept object of subtype of AbstractFeaturedGraph to support variable graph strategy. A variable graph strategy should fetch node/edge/global features from fg and transform graph in fg into required form for layer operation, e.g. GCNConv layer needs a normalized adjacency matrix with self loop. Then, normalized adjacency matrix Ã and node features nf are pass through GCNConv layer l(Ã, nf) to give a new node feature. Finally, a ConcreteFeaturedGraph wrap graph in fg and new node features into a new object of subtype of AbstractFeaturedGraph.","category":"page"},{"location":"basics/layers/","page":"Building Layers","title":"Building Layers","text":"layer = GCNConv(10=>5, relu)\nnew_fg = layer(fg)\ngradient(() -> sum(node_feature(layer(fg))), Flux.params(layer))","category":"page"},{"location":"basics/layers/","page":"Building Layers","title":"Building Layers","text":"Now we complete a simple version of GCNConv layer. One can test the forward pass and gradient if they work properly.","category":"page"},{"location":"basics/layers/","page":"Building Layers","title":"Building Layers","text":"GeometricFlux.AbstractGraphLayer","category":"page"},{"location":"basics/layers/#GeometricFlux.AbstractGraphLayer","page":"Building Layers","title":"GeometricFlux.AbstractGraphLayer","text":"AbstractGraphLayer\n\nAn abstract type of graph neural network layer for GeometricFlux.\n\n\n\n\n\n","category":"type"},{"location":"manual/neighborhood_graph/#Neighborhood-Graphs","page":"Neighborhood graphs","title":"Neighborhood Graphs","text":"","category":"section"},{"location":"manual/neighborhood_graph/","page":"Neighborhood graphs","title":"Neighborhood graphs","text":"GraphSignals.kneighbors_graph","category":"page"},{"location":"manual/neighborhood_graph/#GraphSignals.kneighbors_graph","page":"Neighborhood graphs","title":"GraphSignals.kneighbors_graph","text":"kneighbors_graph(X, k, metric; include_self=false, weighted=false)\nkneighbors_graph(X, k; include_self=false, weighted=false)\n\nGenerate k-nearest neighborhood (kNN) graph from their node features. It returns a FeaturedGraph object, which contains a kNN graph and node features X.\n\nArguments\n\nX::AbstractMatrix: The feature matrix for each node with size (feat_dim, num_nodes).\nk: Number of nearest neighbor for each node in kNN graph.\nmetric::Metric: Distance metric to measure distance between any two nodes.\n\nIt aceepts distance objects from Distances.\n\ninclude_self::Bool: Whether distance from node to itself is included in nearest neighbor.\nweighted::Bool: Whether distance could be the edge weight in kNN graph.\n\nUsage\n\njulia> using GraphSignals, Distances\n\njulia> nf = rand(Float32, 10, 1024);\n\njulia> fg = kneighbors_graph(nf, 5)\nFeaturedGraph:\n\tDirected graph with (#V=1024, #E=5120) in adjacency matrix\n\tNode feature:\tℝ^10 <Matrix{Float32}>\n\njulia> fg = kneighbors_graph(nf, 5, Cityblock())\nFeaturedGraph:\n    Directed graph with (#V=1024, #E=5120) in adjacency matrix\n    Node feature:\tℝ^10 <Matrix{Float32}>\n\njulia> nf = rand(Float32[0, 1], 10, 1024);\n\njulia> fg = kneighbors_graph(nf, 5, Jaccard(); include_self=true)\nFeaturedGraph:\n    Directed graph with (#V=1024, #E=5120) in adjacency matrix\n    Node feature:\tℝ^10 <Matrix{Float32}>\n\n\n\n\n\n","category":"function"},{"location":"tutorials/graph_embedding/#Graph-Embedding-Through-Node2vec-model","page":"Graph Embedding","title":"Graph Embedding Through Node2vec model","text":"","category":"section"},{"location":"#GeometricFlux:-The-Geometric-Deep-Learning-Library-in-Julia","page":"Home","title":"GeometricFlux: The Geometric Deep Learning Library in Julia","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Welcome to GeometricFlux package! GeometricFlux is a framework for geometric deep learning/machine learning. It provides classic graph neural network layers and some utility constructs.","category":"page"},{"location":"","page":"Home","title":"Home","text":"It extends Flux machine learning library for geometric deep learning.\nIt supports of CUDA GPU with CUDA.jl\nIt integrates with JuliaGraphs ecosystems.\nIt supports generic graph neural network architectures (i.g. message passing scheme and graph network block)\nIt contains built-in GNN benchmark datasets (provided by GraphMLDatasets)","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"] add GeometricFlux","category":"page"},{"location":"#Quick-start","page":"Home","title":"Quick start","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The basic graph convolutional network (GCN) is constructed as follow.","category":"page"},{"location":"","page":"Home","title":"Home","text":"fg = FeaturedGraph(adj_mat)\nmodel = Chain(\n    WithGraph(fg, GCNConv(num_features=>hidden, relu)),\n    WithGraph(fg, GCNConv(hidden=>target_dim)),\n    softmax\n)","category":"page"},{"location":"#Load-dataset","page":"Home","title":"Load dataset","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Load cora dataset from GeometricFlux.","category":"page"},{"location":"","page":"Home","title":"Home","text":"using GeometricFlux.Datasets\n\ntrain_X, train_y = traindata(Planetoid(), :cora)\ntest_X, test_y = testdata(Planetoid(), :cora)\ng = graphdata(Planetoid(), :cora)\ntrain_idx = train_indices(Planetoid(), :cora)\ntest_idx = test_indices(Planetoid(), :cora)","category":"page"},{"location":"#Training/testing-data","page":"Home","title":"Training/testing data","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Data is stored in sparse array, thus, we have to convert it into normal array.","category":"page"},{"location":"","page":"Home","title":"Home","text":"train_X = train_X |> Matrix\ntrain_y = train_y |> Matrix","category":"page"},{"location":"#Loss-function","page":"Home","title":"Loss function","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"loss(x, y) = logitcrossentropy(model(x), y)\naccuracy(x, y) = mean(onecold(model(x)) .== onecold(y))","category":"page"},{"location":"#Training","page":"Home","title":"Training","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"ps = Flux.params(model)\ntrain_data = [(train_X, train_y)]\nopt = ADAM()\nevalcb() = @show(accuracy(train_X, train_y))\n\n@epochs epochs Flux.train!(loss, ps, train_data, opt, cb=throttle(evalcb, 10))","category":"page"},{"location":"#Logs","page":"Home","title":"Logs","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"[ Info: Epoch 1\naccuracy(train_X, train_y) = 0.11669128508124077\n[ Info: Epoch 2\naccuracy(train_X, train_y) = 0.19608567208271788\n[ Info: Epoch 3\naccuracy(train_X, train_y) = 0.3098227474150665\n[ Info: Epoch 4\naccuracy(train_X, train_y) = 0.387370753323486\n[ Info: Epoch 5\naccuracy(train_X, train_y) = 0.44645494830132937\n[ Info: Epoch 6\naccuracy(train_X, train_y) = 0.46824224519940916\n[ Info: Epoch 7\naccuracy(train_X, train_y) = 0.48892171344165436\n[ Info: Epoch 8\naccuracy(train_X, train_y) = 0.5025849335302807\n[ Info: Epoch 9\naccuracy(train_X, train_y) = 0.5151403249630724\n[ Info: Epoch 10\naccuracy(train_X, train_y) = 0.5291728212703102\n[ Info: Epoch 11\naccuracy(train_X, train_y) = 0.543205317577548\n[ Info: Epoch 12\naccuracy(train_X, train_y) = 0.5550221565731167\n[ Info: Epoch 13\naccuracy(train_X, train_y) = 0.5638847858197932\n[ Info: Epoch 14\naccuracy(train_X, train_y) = 0.5657311669128509\n[ Info: Epoch 15\naccuracy(train_X, train_y) = 0.5749630723781388\n[ Info: Epoch 16\naccuracy(train_X, train_y) = 0.5834564254062038\n[ Info: Epoch 17\naccuracy(train_X, train_y) = 0.5919497784342689\n[ Info: Epoch 18\naccuracy(train_X, train_y) = 0.5978581979320532\n[ Info: Epoch 19\naccuracy(train_X, train_y) = 0.6019202363367799\n[ Info: Epoch 20\naccuracy(train_X, train_y) = 0.6067208271787297","category":"page"},{"location":"","page":"Home","title":"Home","text":"Check examples/semisupervised_gcn.jl for details.","category":"page"},{"location":"abstractions/gn/#Graph-network-block","page":"Graph network block","title":"Graph network block","text":"","category":"section"},{"location":"abstractions/gn/","page":"Graph network block","title":"Graph network block","text":"Graph network (GN) is a more generic model for graph neural network. It describes an update order: edge, node and then global. There are three corresponding update functions for edge, node and then global, respectively. Three update functions return their default values as follow:","category":"page"},{"location":"abstractions/gn/","page":"Graph network block","title":"Graph network block","text":"update_edge(gn, e, vi, vj, u) = e\nupdate_vertex(gn, ē, vi, u) = vi\nupdate_global(gn, ē, v̄, u) = u","category":"page"},{"location":"abstractions/gn/","page":"Graph network block","title":"Graph network block","text":"Information propagation between different levels are achieved by aggregate functions. Three aggregate functions aggregate_neighbors, aggregate_edges and aggregate_vertices are defined to aggregate states.","category":"page"},{"location":"abstractions/gn/","page":"Graph network block","title":"Graph network block","text":"GN block is realized into a abstract type GraphNet. User can make a subtype of GraphNet to customize GN block. Thus, a GN block is defined as a layer in GNN. MessagePassing is a subtype of GraphNet.","category":"page"},{"location":"abstractions/gn/#Update-functions","page":"Graph network block","title":"Update functions","text":"","category":"section"},{"location":"abstractions/gn/","page":"Graph network block","title":"Graph network block","text":"update_edge acts as the first update function to apply to edge states. It takes edge state e, node i state vi, node j state vj and global state u. It is expected to return a feature vector for new edge state. update_vertex updates nodes state by taking aggregated edge state ē, node i state vi and global state u. It is expected to return a feature vector for new node state. update_global updates global state with aggregated information from edge and node. It takes aggregated edge state ē, aggregated node state v̄ and global state u. It is expected to return a feature vector for new global state. User can define their own behavior by overriding update functions.","category":"page"},{"location":"abstractions/gn/#Aggregate-functions","page":"Graph network block","title":"Aggregate functions","text":"","category":"section"},{"location":"abstractions/gn/","page":"Graph network block","title":"Graph network block","text":"An aggregate function aggregate_neighbors aggregates edge states for edges incident to some node i into node-level information. Aggregate function aggregate_edges aggregates all edge states into global-level information. The last aggregate function aggregate_vertices aggregates all vertex states into global-level information. It is available for assigning aggregate function by assigning aggregate operations to propagate function.","category":"page"},{"location":"abstractions/gn/","page":"Graph network block","title":"Graph network block","text":"propagate(gn, fg::FeaturedGraph, naggr=nothing, eaggr=nothing, vaggr=nothing)","category":"page"},{"location":"abstractions/gn/","page":"Graph network block","title":"Graph network block","text":"naggr, eaggr and vaggr are arguments for aggregate_neighbors, aggregate_edges and aggregate_vertices, respectively. Available aggregate functions are assigned by following symbols to them: :add, :sub, :mul, :div, :max, :min and :mean.","category":"page"},{"location":"tutorials/gat/#Graph-Attention-Network","page":"Graph Attention Network","title":"Graph Attention Network","text":"","category":"section"},{"location":"tutorials/gat/","page":"Graph Attention Network","title":"Graph Attention Network","text":"Graph attention network (GAT) belongs to the message-passing network family, and it queries node feature over its neighbor features and generates result as layer output.","category":"page"},{"location":"tutorials/gat/#Step-1:-Load-Dataset","page":"Graph Attention Network","title":"Step 1: Load Dataset","text":"","category":"section"},{"location":"tutorials/gat/","page":"Graph Attention Network","title":"Graph Attention Network","text":"We load dataset from Planetoid dataset. Here cora dataset is used.","category":"page"},{"location":"tutorials/gat/","page":"Graph Attention Network","title":"Graph Attention Network","text":"train_X, train_y = map(x -> Matrix(x), alldata(Planetoid(), dataset, padding=true))","category":"page"},{"location":"tutorials/gat/#Step-2:-Batch-up-Features-and-Labels","page":"Graph Attention Network","title":"Step 2: Batch up Features and Labels","text":"","category":"section"},{"location":"tutorials/gat/","page":"Graph Attention Network","title":"Graph Attention Network","text":"Just batch up features as usual.","category":"page"},{"location":"tutorials/gat/","page":"Graph Attention Network","title":"Graph Attention Network","text":"add_all_self_loops!(g)\nfg = FeaturedGraph(g)\ntrain_data = (repeat(train_X, outer=(1,1,train_repeats)), repeat(train_y, outer=(1,1,train_repeats)))\ntrain_loader = DataLoader(train_data, batchsize=batch_size, shuffle=true)","category":"page"},{"location":"tutorials/gat/","page":"Graph Attention Network","title":"Graph Attention Network","text":"Notably, self loop for all nodes are needed for GAT model.","category":"page"},{"location":"tutorials/gat/#Step-3:-Build-a-GAT-model","page":"Graph Attention Network","title":"Step 3: Build a GAT model","text":"","category":"section"},{"location":"tutorials/gat/","page":"Graph Attention Network","title":"Graph Attention Network","text":"model = Chain(\n    WithGraph(fg, GATConv(args.input_dim=>args.hidden_dim, heads=args.heads)),\n    Dropout(0.6),\n    WithGraph(fg, GATConv(args.hidden_dim*args.heads=>args.target_dim, heads=args.heads, concat=false)),\n) |> device","category":"page"},{"location":"tutorials/gat/","page":"Graph Attention Network","title":"Graph Attention Network","text":"To note that a GATConv with concat=true will accumulates heads onto feature dimension. Thus, in the next layer, we should use args.hidden_dim*args.heads. In the final layer of a network, a GATConv layer should be assigned with concat=false to average over each heads.","category":"page"},{"location":"tutorials/gat/#Step-4:-Loss-Functions-and-Accuracy","page":"Graph Attention Network","title":"Step 4: Loss Functions and Accuracy","text":"","category":"section"},{"location":"tutorials/gat/","page":"Graph Attention Network","title":"Graph Attention Network","text":"Cross entropy loss is used as loss function and accuracy is used to evaluate the model.","category":"page"},{"location":"tutorials/gat/","page":"Graph Attention Network","title":"Graph Attention Network","text":"model_loss(model, X, y, idx) =\n    logitcrossentropy(model(X)[:,idx,:], y[:,idx,:])","category":"page"},{"location":"tutorials/gat/","page":"Graph Attention Network","title":"Graph Attention Network","text":"accuracy(model, X::AbstractArray, y::AbstractArray, idx) =\n    mean(onecold(softmax(cpu(model(X))[:,idx,:])) .== onecold(cpu(y)[:,idx,:])","category":"page"},{"location":"tutorials/gat/#Step-5:-Training-GAT-Model","page":"Graph Attention Network","title":"Step 5: Training GAT Model","text":"","category":"section"},{"location":"tutorials/gat/","page":"Graph Attention Network","title":"Graph Attention Network","text":"# ADAM optimizer\nopt = ADAM(args.η)\n\n# parameters\nps = Flux.params(model)\n\n# training\n@info \"Start Training, total $(args.epochs) epochs\"\nfor epoch = 1:args.epochs\n    @info \"Epoch $(epoch)\"\n\n    for (X, y) in train_loader\n        loss, back = Flux.pullback(ps) do\n            model_loss(model, X |> device, y |> device, train_idx |> device)\n        end\n        train_acc = accuracy(model, train_loader, device, train_idx)\n        test_acc = accuracy(model, test_loader, device, test_idx)\n        grad = back(1f0)\n        Flux.Optimise.update!(opt, ps, grad)\n    end\nend","category":"page"},{"location":"tutorials/gat/","page":"Graph Attention Network","title":"Graph Attention Network","text":"For a complete example, please check examples/gat.jl.","category":"page"}]
}
