var documenterSearchIndex = {"docs":
[{"location":"tutorials/semisupervised_gcn/#Semi-supervised-Learning-with-Graph-Convolution-Networks-(GCN)","page":"Semi-Supervised Learning with GCN","title":"Semi-supervised Learning with Graph Convolution Networks (GCN)","text":"","category":"section"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"Graph convolution networks (GCN) have been considered as the first step to graph neural networks (GNN). This example will go through how to train a vanilla GCN.","category":"page"},{"location":"tutorials/semisupervised_gcn/#Semi-supervised-Learning-in-Graph-Neural-Networks","page":"Semi-Supervised Learning with GCN","title":"Semi-supervised Learning in Graph Neural Networks","text":"","category":"section"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"The semi-supervised learning task defines a learning by given features and labels for only partial nodes in a graph. We train features and labels for partial nodes, and test the model for another partial nodes in graph.","category":"page"},{"location":"tutorials/semisupervised_gcn/#Node-Classification-task","page":"Semi-Supervised Learning with GCN","title":"Node Classification task","text":"","category":"section"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"In this task, we learn a node classification task which learns a model to predict labels for each node in a graph. In GCN network, node features are given and the model outputs node labels.","category":"page"},{"location":"tutorials/semisupervised_gcn/#Step-1:-Load-Dataset","page":"Semi-Supervised Learning with GCN","title":"Step 1: Load Dataset","text":"","category":"section"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"GeometricFlux provides planetoid dataset in GeometricFlux.Datasets, which is provided by GraphMLDatasets. Planetoid dataset has three sub-datasets: Cora, Citeseer, PubMed. We demonstrate Cora dataset in this example. traindata provides the functionality for loading training data from various kinds of datasets. Dataset can be specified by the first argument, and the second for sub-datasets.","category":"page"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"using GeometricFlux.Datasets\n\ntrain_X, train_y = traindata(Planetoid(), :cora)","category":"page"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"traindata returns a pre-defined training features and labels. These features are node features.","category":"page"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"train_X, train_y = map(x->Matrix(x), traindata(Planetoid(), :cora))","category":"page"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"We can load graph from graphdata, and the graph is preprocessed into SimpleGraph type, which is provided by Graphs.","category":"page"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"g = graphdata(Planetoid(), :cora)\ntrain_idx = train_indices(Planetoid(), :cora)","category":"page"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"We need node indices to index a subgraph from original graph. train_indices gives node indices for training.","category":"page"},{"location":"tutorials/semisupervised_gcn/#Step-2:-Wrapping-Graph-and-Features-into-FeaturedGraph","page":"Semi-Supervised Learning with GCN","title":"Step 2: Wrapping Graph and Features into FeaturedGraph","text":"","category":"section"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"FeaturedGraph is a container for holding a graph, node features, edge features and global features. It is provided by GraphSignals. To wrap graph and node features into FeaturedGraph, graph g should be placed as the first argument and nf is to specify node features.","category":"page"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"using GraphSignals\n\nFeaturedGraph(g, nf=train_X)","category":"page"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"If we want to get a subgraph from a FeaturedGraph object, we call subgraph and provide node indices train_idx as second argument.","category":"page"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"subgraph(FeaturedGraph(g, nf=train_X), train_idx)","category":"page"},{"location":"tutorials/semisupervised_gcn/#Step-3:-Build-a-GCN-model","page":"Semi-Supervised Learning with GCN","title":"Step 3: Build a GCN model","text":"","category":"section"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"A GCn model is composed of two layers of GCNConv and the activation function for first layer is relu. In the middle, a Dropout layer is placed. We need a GraphParallel to integrate with regular Flux layer, and it specifies node features go to node_layer=Dropout(0.5).","category":"page"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"model = Chain(\n    GCNConv(input_dim=>hidden_dim, relu),\n    GraphParallel(node_layer=Dropout(0.5)),\n    GCNConv(hidden_dim=>target_dim),\n    node_feature,\n)","category":"page"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"Since the model input is a FeaturedGraph object, the model output a FeaturedGraph object as well. In the end of model, we get node features out from a FeaturedGraph object using node_feature.","category":"page"},{"location":"tutorials/semisupervised_gcn/#Step-4:-Loss-Functions-and-Accuracy","page":"Semi-Supervised Learning with GCN","title":"Step 4: Loss Functions and Accuracy","text":"","category":"section"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"Then, since it is a node classification task, we define the model loss by logitcrossentropy, and a L2 regularization is used. In the vanilla GCN, only first layer is applied to L2 regularization and can be adjusted by hyperparameter λ.","category":"page"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"l2norm(x) = sum(abs2, x)\n\nfunction model_loss(model, λ, batch)\n    loss = 0.f0\n    for (x, y) in batch\n        loss += logitcrossentropy(model(x), y)\n        loss += λ*sum(l2norm, Flux.params(model[1]))\n    end\n    return loss\nend","category":"page"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"Accuracy for a batch and for data loader are provided.","category":"page"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"function accuracy(model, batch::AbstractVector)\n    return mean(mean(onecold(softmax(cpu(model(x)))) .== onecold(cpu(y))) for (x, y) in batch)\nend\n\naccuracy(model, loader::DataLoader, device) = mean(accuracy(model, batch |> device) for batch in loader)","category":"page"},{"location":"tutorials/semisupervised_gcn/#Step-5:-Training-GCN-Model","page":"Semi-Supervised Learning with GCN","title":"Step 5: Training GCN Model","text":"","category":"section"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"We train the model with the same process as training a Flux model.","category":"page"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"train_loader, test_loader = load_data(:cora, args.batch_size)\n\n# optimizer\nopt = ADAM(args.η)\n    \n# parameters\nps = Flux.params(model)\n\n# training\ntrain_steps = 0\n@info \"Start Training, total $(args.epochs) epochs\"\nfor epoch = 1:args.epochs\n    @info \"Epoch $(epoch)\"\n\n    for batch in train_loader\n        grad = gradient(() -> model_loss(model, args.λ, batch |> device), ps)\n        Flux.Optimise.update!(opt, ps, grad)\n        train_steps += 1\n    end\nend","category":"page"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"So far, we complete a basic tutorial for training a GCN model!","category":"page"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"For the complete example, please check the script examples/semisupervised_gcn.jl.","category":"page"},{"location":"tutorials/semisupervised_gcn/#Acceleration-by-Pre-computing-Normalized-Adjacency-Matrix","page":"Semi-Supervised Learning with GCN","title":"Acceleration by Pre-computing Normalized Adjacency Matrix","text":"","category":"section"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"The training process can be slow in this example. Since we place the graph and features together in FeaturedGraph object, GCNConv will need to compute a normalized adjacency matrix in the training process. This behavior will lead to long training time. We can accelerate training process by pre-compute normalized adjacency matrix for all FeaturedGraph objects. To do so, we can call the following function and it will compute normalized adjacency matrix for fg before training. This will reduce the training time.","category":"page"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"GraphSignals.normalized_adjacency_matrix!(fg)","category":"page"},{"location":"tutorials/semisupervised_gcn/","page":"Semi-Supervised Learning with GCN","title":"Semi-Supervised Learning with GCN","text":"Since the normalized adjacency matrix is used in GCNConv, we could pre-compute normalized adjacency matrix for it. If a layer doesn't require a normalized adjacency matrix, this step will lead to error.","category":"page"},{"location":"references/#References","page":"References","title":"References","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"","category":"page"},{"location":"basics/passgraph/#Graph-Passing-Strategy","page":"Graph Passing","title":"Graph Passing Strategy","text":"","category":"section"},{"location":"basics/passgraph/","page":"Graph Passing","title":"Graph Passing","text":"Graph is an input data structure for graph neural network. Passing a graph into a GNN layer can have different behaviors. If the graph remains fixed across samples, that is, all samples utilize the same graph structure, a static graph is used. Or, graphs can be carried within FeaturedGraph to provide variable graphs to GNN layer. Users have the flexibility to pick an adequate approach for their own needs.","category":"page"},{"location":"basics/passgraph/#Variable-Graph-Strategy","page":"Graph Passing","title":"Variable Graph Strategy","text":"","category":"section"},{"location":"basics/passgraph/","page":"Graph Passing","title":"Graph Passing","text":"Variable graphs are supported through FeaturedGraph, which contains both the graph information and the features. Each FeaturedGraph can contain a distinct graph structure and its features. Data of FeaturedGraph are fed directly to graph convolutional layer or graph neural network to let each feature be learned on different graph structures. A adjacency matrix adj_mat is given to construct a FeaturedGraph as follows:","category":"page"},{"location":"basics/passgraph/","page":"Graph Passing","title":"Graph Passing","text":"fg = FeaturedGraph(adj_mat, features)\nlayer = GCNConv(feat=>h1, relu)","category":"page"},{"location":"basics/passgraph/","page":"Graph Passing","title":"Graph Passing","text":"Simple(Di)Graph, SimpleWeighted(Di)Graph or Meta(Di)Graph provided by the packages Graphs, SimpleWeightedGraphs and MetaGraphs, respectively, are acceptable for constructing a FeaturedGraph. An adjacency list is also accepted, too.","category":"page"},{"location":"basics/passgraph/#FeaturedGraph-in,-FeaturedGraph-out","page":"Graph Passing","title":"FeaturedGraph in, FeaturedGraph out","text":"","category":"section"},{"location":"basics/passgraph/","page":"Graph Passing","title":"Graph Passing","text":"Since a variable graph is provided from data, a FeaturedGraph object or a set of FeaturedGraph objects should be fed in a GNN model. The FeaturedGraph object should contain a graph and sufficient features that a GNN model needed. After operations, a FeaturedGraph object is given as output.","category":"page"},{"location":"basics/passgraph/","page":"Graph Passing","title":"Graph Passing","text":"fg = FeaturedGraph(g, nf=X)\ngc = GCNConv(in_channel=>out_channel)\nnew_fg = gc(fg)","category":"page"},{"location":"basics/passgraph/#Static-Graph-Strategy","page":"Graph Passing","title":"Static Graph Strategy","text":"","category":"section"},{"location":"basics/passgraph/","page":"Graph Passing","title":"Graph Passing","text":"A static graph is used to reduce redundant computation during passing through layers. A static graph can be set in graph convolutional layers such that this graph is shared for computations across those layers. An adjacency matrix adj_mat is given to represent a graph and is provided to a graph convolutional layer as follows:","category":"page"},{"location":"basics/passgraph/","page":"Graph Passing","title":"Graph Passing","text":"fg = FeaturedGraph(adj_mat)\nlayer = WithGraph(fg, GCNConv(feat=>h1, relu))","category":"page"},{"location":"basics/passgraph/","page":"Graph Passing","title":"Graph Passing","text":"Simple(Di)Graph, SimpleWeighted(Di)Graph or Meta(Di)Graph provided by the packages Graphs, SimpleWeightedGraphs and MetaGraphs, respectively, are valid arguments for passing as a static graph to this layer. An adjacency list is also accepted in the type of Vector{Vector} is also accepted.","category":"page"},{"location":"basics/passgraph/#Cached-Graph-in-Layers","page":"Graph Passing","title":"Cached Graph in Layers","text":"","category":"section"},{"location":"basics/passgraph/","page":"Graph Passing","title":"Graph Passing","text":"While a variable graph is given by FeaturedGraph, a GNN layer doesn't need a static graph anymore. A cache mechanism is designed to cache static graph to reduce computation time. A cached graph is retrieved from WithGraph layer and operation is then performed. For each time, it will assign current computed graph back to layer.","category":"page"},{"location":"basics/passgraph/#Array-in,-Array-out","page":"Graph Passing","title":"Array in, Array out","text":"","category":"section"},{"location":"basics/passgraph/","page":"Graph Passing","title":"Graph Passing","text":"Since a static graph is provided from WithGraph layer, it doesn't accept a FeaturedGraph object anymore. Instead, it accepts a regular array as input, and outputs an array back.","category":"page"},{"location":"basics/passgraph/","page":"Graph Passing","title":"Graph Passing","text":"fg = FeaturedGraph(g)\nlayer = WithGraph(fg, GCNConv(in_channel=>out_channel))\nH = layer(X)","category":"page"},{"location":"basics/passgraph/#What-you-feed-is-what-you-get","page":"Graph Passing","title":"What you feed is what you get","text":"","category":"section"},{"location":"basics/passgraph/","page":"Graph Passing","title":"Graph Passing","text":"In GeometricFlux, there are are two APIs which allow different input/output types for GNN layers. For example, GCNConv layer provides the following two APIs:","category":"page"},{"location":"basics/passgraph/","page":"Graph Passing","title":"Graph Passing","text":"(g::WithGraph{<:GCNConv})(X::AbstractArray) -> AbstractArray\n(g::GCNConv)(fg::FeaturedGraph) -> FeaturedGraph","category":"page"},{"location":"basics/passgraph/","page":"Graph Passing","title":"Graph Passing","text":"If your feed a GCNConv layer with a Array, it will return you a Array. If you feed a GCNConv layer with a FeaturedGraph, it will return you a FeaturedGraph. These APIs ensure the consistency between input and output types.","category":"page"},{"location":"manual/models/#Models","page":"Models","title":"Models","text":"","category":"section"},{"location":"manual/models/#Autoencoders","page":"Models","title":"Autoencoders","text":"","category":"section"},{"location":"manual/models/#Graph-Autoencoder","page":"Models","title":"Graph Autoencoder","text":"","category":"section"},{"location":"manual/models/","page":"Models","title":"Models","text":"Z = enc(X A) \nhatA = sigma (ZZ^T)","category":"page"},{"location":"manual/models/","page":"Models","title":"Models","text":"where A denotes the adjacency matrix.","category":"page"},{"location":"manual/models/","page":"Models","title":"Models","text":"GeometricFlux.GAE","category":"page"},{"location":"manual/models/#GeometricFlux.GAE","page":"Models","title":"GeometricFlux.GAE","text":"GAE(enc, [σ=identity])\n\nGraph autoencoder.\n\nArguments\n\nenc: encoder. It can be any graph convolutional layer.\nσ: Activation function for decoder.\n\nEncoder is specified by user and decoder will be InnerProductDecoder layer.\n\n\n\n\n\n","category":"type"},{"location":"manual/models/","page":"Models","title":"Models","text":"Reference: Variational Graph Auto-Encoders","category":"page"},{"location":"manual/models/","page":"Models","title":"Models","text":"","category":"page"},{"location":"manual/models/#Variational-Graph-Autoencoder","page":"Models","title":"Variational Graph Autoencoder","text":"","category":"section"},{"location":"manual/models/","page":"Models","title":"Models","text":"H = enc(X A) \nZ_mu Z_logσ = GCN_mu(H A) GCN_sigma(H A) \nhatA = sigma (ZZ^T)","category":"page"},{"location":"manual/models/","page":"Models","title":"Models","text":"where A denotes the adjacency matrix, X denotes node features.","category":"page"},{"location":"manual/models/","page":"Models","title":"Models","text":"GeometricFlux.VGAE","category":"page"},{"location":"manual/models/#GeometricFlux.VGAE","page":"Models","title":"GeometricFlux.VGAE","text":"VGAE(enc[, σ])\n\nVariational graph autoencoder.\n\nArguments\n\nenc: encoder. It can be any graph convolutional layer.\n\nEncoder is specified by user and decoder will be InnerProductDecoder layer.\n\n\n\n\n\n","category":"type"},{"location":"manual/models/","page":"Models","title":"Models","text":"Reference: Variational Graph Auto-Encoders","category":"page"},{"location":"manual/models/","page":"Models","title":"Models","text":"","category":"page"},{"location":"manual/models/#Special-Layers","page":"Models","title":"Special Layers","text":"","category":"section"},{"location":"manual/models/#Inner-product-Decoder","page":"Models","title":"Inner-product Decoder","text":"","category":"section"},{"location":"manual/models/","page":"Models","title":"Models","text":"hatA = sigma (ZZ^T)","category":"page"},{"location":"manual/models/","page":"Models","title":"Models","text":"where Z denotes the input matrix from encoder.","category":"page"},{"location":"manual/models/","page":"Models","title":"Models","text":"GeometricFlux.InnerProductDecoder","category":"page"},{"location":"manual/models/#GeometricFlux.InnerProductDecoder","page":"Models","title":"GeometricFlux.InnerProductDecoder","text":"InnerProductDecoder(σ)\n\nInner-product decoder layer.\n\nArguments\n\nσ: activation function.\n\n\n\n\n\n","category":"type"},{"location":"manual/models/","page":"Models","title":"Models","text":"Reference: Variational Graph Auto-Encoders","category":"page"},{"location":"manual/models/","page":"Models","title":"Models","text":"","category":"page"},{"location":"manual/models/#Variational-Graph-Encoder","page":"Models","title":"Variational Graph Encoder","text":"","category":"section"},{"location":"manual/models/","page":"Models","title":"Models","text":"H = enc(X A) \nZ_mu Z_logσ = GCN_mu(H A) GCN_sigma(H A)","category":"page"},{"location":"manual/models/","page":"Models","title":"Models","text":"GeometricFlux.VariationalGraphEncoder","category":"page"},{"location":"manual/models/#GeometricFlux.VariationalGraphEncoder","page":"Models","title":"GeometricFlux.VariationalGraphEncoder","text":"VariationalGraphEncoder(nn, h_dim, z_dim)\n\nVariational graph encoder layer.\n\nArguments\n\nnn: neural network. It can be any graph convolutional layer.\nh_dim: dimension of hidden layer. This should fit the output dimension of nn.\nz_dim: dimension of latent variable layer. This will be parametrized into μ and logσ.\n\nEncoder can be any graph convolutional layer.\n\n\n\n\n\n","category":"type"},{"location":"manual/models/","page":"Models","title":"Models","text":"Reference: Variational Graph Auto-Encoders","category":"page"},{"location":"basics/batch/#Batch-Learning","page":"Batch Learning","title":"Batch Learning","text":"","category":"section"},{"location":"basics/batch/#Batch-Learning-for-Variable-Graph-Strategy","page":"Batch Learning","title":"Batch Learning for Variable Graph Strategy","text":"","category":"section"},{"location":"basics/batch/","page":"Batch Learning","title":"Batch Learning","text":"Batch learning for variable graph strategy can be prepared as follows:","category":"page"},{"location":"basics/batch/","page":"Batch Learning","title":"Batch Learning","text":"train_data = [(FeaturedGraph(g, nf=train_X), train_y) for _ in 1:N]\ntrain_batch = Flux.batch(train_data)","category":"page"},{"location":"basics/batch/","page":"Batch Learning","title":"Batch Learning","text":"It batches up FeaturedGraph objects into specified mini-batch. A batch is passed to a GNN model and trained/inferred one by one. It is hard for FeaturedGraph objects to train or infer in real batch for GPU.","category":"page"},{"location":"basics/batch/#Batch-Learning-for-Static-Graph-Strategy","page":"Batch Learning","title":"Batch Learning for Static Graph Strategy","text":"","category":"section"},{"location":"basics/batch/","page":"Batch Learning","title":"Batch Learning","text":"A efficient batch learning should use static graph strategy. Batch learning for static graph strategy can be prepared as follows:","category":"page"},{"location":"basics/batch/","page":"Batch Learning","title":"Batch Learning","text":"train_data = (repeat(train_X, outer=(1,1,N)), repeat(train_y, outer=(1,1,N)))\ntrain_loader = DataLoader(train_data, batchsize=batch_size, shuffle=true)","category":"page"},{"location":"basics/batch/","page":"Batch Learning","title":"Batch Learning","text":"An efficient batch learning should feed array to a GNN model. In the example, the mini-batch dimension is the third dimension for train_X array. The train_X array is split by DataLoader into mini-batches and feed a mini-batch to GNN model at a time. This strategy leverages the advantage of GPU training by accelerating training GNN model in a real batch learning.","category":"page"},{"location":"manual/embedding/#Embeddings","page":"Embeddings","title":"Embeddings","text":"","category":"section"},{"location":"manual/embedding/#Node2vec","page":"Embeddings","title":"Node2vec","text":"","category":"section"},{"location":"manual/embedding/","page":"Embeddings","title":"Embeddings","text":"GeometricFlux.node2vec","category":"page"},{"location":"manual/embedding/#GeometricFlux.node2vec","page":"Embeddings","title":"GeometricFlux.node2vec","text":"node2vec(g; walks_per_node, len, p, q, dims)\n\nReturns an embedding matrix with size of nv(g) x dims. It computes node embeddings on graph g accroding to node2vec Aditya Grover, Jure Leskovec (2016). It performs biased random walks on the graph, then computes word embeddings by treating those random walks as sentences.\n\nArguments\n\ng::FeaturedGraph: The graph to perform random walk on.\nwalks_per_node::Int: Number of walks starting on each node,\n\ntotal number of walks is nv(g) * walks_per_node\n\nlen::Int: Length of random walks\np::Real: Return parameter from Aditya Grover, Jure Leskovec (2016)\nq::Real: In-out parameter from Aditya Grover, Jure Leskovec (2016)\ndims::Int: Number of vector dimensions\n\n\n\n\n\n","category":"function"},{"location":"basics/conv/#Graph-Convolutions","page":"Graph Convolutions","title":"Graph Convolutions","text":"","category":"section"},{"location":"basics/conv/","page":"Graph Convolutions","title":"Graph Convolutions","text":"Graph convolution can be classified into spectral-based graph convolution and spatial-based graph convolution. Spectral-based graph convolution, such as GCNConv and ChebConv, performs operation on features of whole graph at one time. Spatial-based graph convolution, such as GraphConv and GATConv, performs operation on features of local subgraph instead. Message-passing scheme is an abstraction for spatial-based graph convolutional layers. Any spatial-based graph convolutional layer can be implemented under the framework of message-passing scheme.","category":"page"},{"location":"basics/subgraph/#Subgraph","page":"Subgraph","title":"Subgraph","text":"","category":"section"},{"location":"basics/subgraph/#Subgraph-of-FeaturedGraph","page":"Subgraph","title":"Subgraph of FeaturedGraph","text":"","category":"section"},{"location":"basics/subgraph/","page":"Subgraph","title":"Subgraph","text":"A FeaturedGraph object can derive a subgraph from a selected subset of the vertices of the graph.","category":"page"},{"location":"basics/subgraph/","page":"Subgraph","title":"Subgraph","text":"train_idx = train_indices(Planetoid(), :cora)\nfg = FeaturedGraph(g)\nfsg = subgraph(fg, train_idx)","category":"page"},{"location":"basics/subgraph/","page":"Subgraph","title":"Subgraph","text":"A FeaturedSubgraph object is returned from subgraph by selected vertices train_idx.","category":"page"},{"location":"manual/featuredgraph/#FeaturedGraph","page":"FeaturedGraph","title":"FeaturedGraph","text":"","category":"section"},{"location":"manual/featuredgraph/","page":"FeaturedGraph","title":"FeaturedGraph","text":"GraphSignals.FeaturedGraph\nGraphSignals.graph\nGraphSignals.node_feature\nGraphSignals.has_node_feature\nGraphSignals.edge_feature\nGraphSignals.has_edge_feature\nGraphSignals.global_feature\nGraphSignals.has_global_feature\nGraphSignals.subgraph\nGraphSignals.ConcreteFeaturedGraph","category":"page"},{"location":"manual/featuredgraph/#GraphSignals.FeaturedGraph","page":"FeaturedGraph","title":"GraphSignals.FeaturedGraph","text":"FeaturedGraph(g, [mt]; directed=:auto, nf, ef, gf, T, N, E)\n\nA type representing a graph structure and storing also arrays  that contain features associated to nodes, edges, and the whole graph. \n\nA FeaturedGraph can be constructed out of different objects g representing the connections inside the graph. When constructed from another featured graph fg, the internal graph representation is preserved and shared.\n\nArguments\n\ng: Data representing the graph topology. Possible type are \nAn adjacency matrix.\nAn adjacency list.\nA Graphs' graph, i.e. SimpleGraph, SimpleDiGraph from Graphs, or SimpleWeightedGraph,   SimpleWeightedDiGraph from SimpleWeightedGraphs.\nAn AbstractFeaturedGraph object.\nmt::Symbol: Matrix type for g in matrix form. if graph is in matrix form, mt is recorded as one of :adjm,   :normedadjm, :laplacian, :normalized or :scaled.\ndirected: It specify that direction of a graph. It can be :auto, :directed and :undirected.   Default value is :auto, which infers direction automatically.\nnf: Node features.\nef: Edge features.\ngf: Global features.\nT: It specifies the element type of graph. Default value is the element type of g.\nN: Number of nodes for g.\nE: Number of edges for g.\n\nUsage\n\nusing GraphSignals, CUDA\n\n# Construct from adjacency list representation\ng = [[2,3], [1,4,5], [1], [2,5], [2,4]]\nfg = FeaturedGraph(g)\n\n# Number of nodes and edges\nnv(fg)  # 5\nne(fg)  # 10\n\n# From a Graphs' graph\nfg = FeaturedGraph(erdos_renyi(100, 20))\n\n# Copy featured graph while also adding node features\nfg = FeaturedGraph(fg, nf=rand(100, 5))\n\n# Send to gpu\nfg = fg |> cu\n\nSee also graph, node_feature, edge_feature, and global_feature.\n\n\n\n\n\n","category":"type"},{"location":"manual/featuredgraph/#GraphSignals.graph","page":"FeaturedGraph","title":"GraphSignals.graph","text":"graph(fg)\n\nGet referenced graph in fg.\n\nArguments\n\nfg::AbstractFeaturedGraph: A concrete object of AbstractFeaturedGraph type.\n\n\n\n\n\n","category":"function"},{"location":"manual/featuredgraph/#GraphSignals.node_feature","page":"FeaturedGraph","title":"GraphSignals.node_feature","text":"node_feature(fg)\n\nGet node feature attached to fg.\n\nArguments\n\nfg::AbstractFeaturedGraph: A concrete object of AbstractFeaturedGraph type.\n\n\n\n\n\n","category":"function"},{"location":"manual/featuredgraph/#GraphSignals.has_node_feature","page":"FeaturedGraph","title":"GraphSignals.has_node_feature","text":"has_node_feature(::AbstractFeaturedGraph)\n\nCheck if node_feature is available or not for fg.\n\nArguments\n\nfg::AbstractFeaturedGraph: A concrete object of AbstractFeaturedGraph type.\n\n\n\n\n\n","category":"function"},{"location":"manual/featuredgraph/#GraphSignals.edge_feature","page":"FeaturedGraph","title":"GraphSignals.edge_feature","text":"edge_feature(fg)\n\nGet edge feature attached to fg.\n\nArguments\n\nfg::AbstractFeaturedGraph: A concrete object of AbstractFeaturedGraph type.\n\n\n\n\n\n","category":"function"},{"location":"manual/featuredgraph/#GraphSignals.has_edge_feature","page":"FeaturedGraph","title":"GraphSignals.has_edge_feature","text":"has_edge_feature(::AbstractFeaturedGraph)\n\nCheck if edge_feature is available or not for fg.\n\nArguments\n\nfg::AbstractFeaturedGraph: A concrete object of AbstractFeaturedGraph type.\n\n\n\n\n\n","category":"function"},{"location":"manual/featuredgraph/#GraphSignals.global_feature","page":"FeaturedGraph","title":"GraphSignals.global_feature","text":"global_feature(fg)\n\nGet global feature attached to fg.\n\nArguments\n\nfg::AbstractFeaturedGraph: A concrete object of AbstractFeaturedGraph type.\n\n\n\n\n\n","category":"function"},{"location":"manual/featuredgraph/#GraphSignals.has_global_feature","page":"FeaturedGraph","title":"GraphSignals.has_global_feature","text":"has_global_feature(::AbstractFeaturedGraph)\n\nCheck if global_feature is available or not for fg.\n\nArguments\n\nfg::AbstractFeaturedGraph: A concrete object of AbstractFeaturedGraph type.\n\n\n\n\n\n","category":"function"},{"location":"manual/featuredgraph/#GraphSignals.subgraph","page":"FeaturedGraph","title":"GraphSignals.subgraph","text":"subgraph(fg, nodes)\n\nReturns a subgraph of type FeaturedSubgraph from a given featured graph fg. It constructs a subgraph by reserving nodes in a graph.\n\nArguments\n\nfg::AbstractFeaturedGraph: A base featured graph to construct a subgraph.\nnodes::AbstractVector: It specifies nodes to be reserved from fg.\n\n\n\n\n\n","category":"function"},{"location":"manual/featuredgraph/#GraphSignals.ConcreteFeaturedGraph","page":"FeaturedGraph","title":"GraphSignals.ConcreteFeaturedGraph","text":"ConcreteFeaturedGraph(fg; kwargs...)\n\nThis is a syntax sugar for construction for FeaturedGraph and FeaturedSubgraph object. It is an idempotent operation, which gives the same type of object as inputs. It wraps input fg again but reconfigures with kwargs.\n\nArguments\n\nfg: FeaturedGraph and FeaturedSubgraph object.\n\nUsage\n\njulia> using GraphSignals\n\njulia> adjm = [0 1 1 1;\n               1 0 1 0;\n               1 1 0 1;\n               1 0 1 0];\n\njulia> nf = rand(10, 4);\n\njulia> fg = FeaturedGraph(adjm; nf=nf)\nFeaturedGraph:\n\tUndirected graph with (#V=4, #E=5) in adjacency matrix\n\tNode feature:\tℝ^10 <Matrix{Float64}>\n\njulia> ConcreteFeaturedGraph(fg, nf=rand(7, 4))\nFeaturedGraph:\n    Undirected graph with (#V=4, #E=5) in adjacency matrix\n    Node feature:\tℝ^7 <Matrix{Float64}>\n\n\n\n\n\n","category":"function"},{"location":"tutorials/gcn_fixed_graph/#GCN-with-Fixed-Graph","page":"GCN with Fixed Graph","title":"GCN with Fixed Graph","text":"","category":"section"},{"location":"tutorials/gcn_fixed_graph/","page":"GCN with Fixed Graph","title":"GCN with Fixed Graph","text":"In the tutorial for semi-supervised learning with GCN, variable graphs are provided to GNN from FeaturedGraph, which contains a graph and node features. Each FeaturedGraph object can contain different graph and different node features, and can be train on the same GNN model. However, variable graph doesn't have the proper form of graph structure with respect to GNN layers and this lead to inefficient training/inference process. Fixed graph strategy can be used to train a GNN model with the same graph structure in GeometricFlux.","category":"page"},{"location":"tutorials/gcn_fixed_graph/#Fixed-Graph","page":"GCN with Fixed Graph","title":"Fixed Graph","text":"","category":"section"},{"location":"tutorials/gcn_fixed_graph/","page":"GCN with Fixed Graph","title":"GCN with Fixed Graph","text":"A fixed graph is given to a layer by WithGraph syntax. WithGraph wrap a FeaturedGraph object and a GNN layer as first and second arguments, respectively.","category":"page"},{"location":"tutorials/gcn_fixed_graph/","page":"GCN with Fixed Graph","title":"GCN with Fixed Graph","text":"fg = FeaturedGraph(graph)\nWithGraph(fg, GCNConv(1024=>256, relu))","category":"page"},{"location":"tutorials/gcn_fixed_graph/","page":"GCN with Fixed Graph","title":"GCN with Fixed Graph","text":"This way, we can customize by binding different graph to certain layer and the layer will specialize graph to a required form. For example, a GCNConv layer requires graph in the form of normalized adjacency matrix. Once the graph is bound to a GCNConv layer, it transforms graph into normalized adjacency matrix and stores in WithGraph object. It accelerates training or inference by avoiding calculating transformations. The features in FeaturedGraph object in WithGraph are not used in any layer or model training or inference.","category":"page"},{"location":"tutorials/gcn_fixed_graph/#Array-in,-Array-out","page":"GCN with Fixed Graph","title":"Array in, Array out","text":"","category":"section"},{"location":"tutorials/gcn_fixed_graph/","page":"GCN with Fixed Graph","title":"GCN with Fixed Graph","text":"With this approach, a GNN layer accepts features in array. It takes an array as input and outputs array. Thus, a GNN layer wrapped with WithGraph should accept a feature array, just like regular deep learning model.","category":"page"},{"location":"tutorials/gcn_fixed_graph/#Batch-Learning","page":"GCN with Fixed Graph","title":"Batch Learning","text":"","category":"section"},{"location":"tutorials/gcn_fixed_graph/","page":"GCN with Fixed Graph","title":"GCN with Fixed Graph","text":"Since features are in the form of array, they can be batched up for batched learning. We will demonstrate how to achieve these goals.","category":"page"},{"location":"tutorials/gcn_fixed_graph/#Step-1:-Load-Dataset","page":"GCN with Fixed Graph","title":"Step 1: Load Dataset","text":"","category":"section"},{"location":"tutorials/gcn_fixed_graph/","page":"GCN with Fixed Graph","title":"GCN with Fixed Graph","text":"Different from loading datasets in semi-supervised learning example, we use alldata for supervised learning here and padding=true is added in order to padding features from partial nodes to pseudo-full nodes. A padded features contains zeros in the nodes that are not supposed to be train on.","category":"page"},{"location":"tutorials/gcn_fixed_graph/","page":"GCN with Fixed Graph","title":"GCN with Fixed Graph","text":"train_X, train_y = map(x -> Matrix(x), alldata(Planetoid(), dataset, padding=true))","category":"page"},{"location":"tutorials/gcn_fixed_graph/","page":"GCN with Fixed Graph","title":"GCN with Fixed Graph","text":"We need graph and node indices for training as well.","category":"page"},{"location":"tutorials/gcn_fixed_graph/","page":"GCN with Fixed Graph","title":"GCN with Fixed Graph","text":"g = graphdata(Planetoid(), dataset)\ntrain_idx = 1:size(train_X, 2)","category":"page"},{"location":"tutorials/gcn_fixed_graph/#Step-2:-Batch-up-Features-and-Labels","page":"GCN with Fixed Graph","title":"Step 2: Batch up Features and Labels","text":"","category":"section"},{"location":"tutorials/gcn_fixed_graph/","page":"GCN with Fixed Graph","title":"GCN with Fixed Graph","text":"In order to make batch learning available, we separate graph and node features. We don't subgraph here. Node features are batched up by repeating node features here for demonstration, since planetoid dataset doesn't have batched settings. Different repeat numbers can be specified by train_repeats and train_repeats.","category":"page"},{"location":"tutorials/gcn_fixed_graph/","page":"GCN with Fixed Graph","title":"GCN with Fixed Graph","text":"fg = FeaturedGraph(g)\ntrain_data = (repeat(train_X, outer=(1,1,train_repeats)), repeat(train_y, outer=(1,1,train_repeats)))","category":"page"},{"location":"tutorials/gcn_fixed_graph/#Step-3:-Build-a-GCN-model","page":"GCN with Fixed Graph","title":"Step 3: Build a GCN model","text":"","category":"section"},{"location":"tutorials/gcn_fixed_graph/","page":"GCN with Fixed Graph","title":"GCN with Fixed Graph","text":"Here comes to building a GCN model. We build a model as building a regular Flux model but just wrap GCNConv layer with WithGraph.","category":"page"},{"location":"tutorials/gcn_fixed_graph/","page":"GCN with Fixed Graph","title":"GCN with Fixed Graph","text":"model = Chain(\n    WithGraph(fg, GCNConv(args.input_dim=>args.hidden_dim, relu)),\n    Dropout(0.5),\n    WithGraph(fg, GCNConv(args.hidden_dim=>args.target_dim)),\n)","category":"page"},{"location":"tutorials/gcn_fixed_graph/#Step-4:-Loss-Functions-and-Accuracy","page":"GCN with Fixed Graph","title":"Step 4: Loss Functions and Accuracy","text":"","category":"section"},{"location":"tutorials/gcn_fixed_graph/","page":"GCN with Fixed Graph","title":"GCN with Fixed Graph","text":"Almost all codes are the same as in semi-supervised learning example, except that indices for subgraphing are needed to get partial features out for calculating loss.","category":"page"},{"location":"tutorials/gcn_fixed_graph/","page":"GCN with Fixed Graph","title":"GCN with Fixed Graph","text":"l2norm(x) = sum(abs2, x)\n\nfunction model_loss(model, λ, X, y, idx)\n    loss = logitcrossentropy(model(X)[:,idx,:], y[:,idx,:])\n    loss += λ*sum(l2norm, Flux.params(model[1]))\n    return loss\nend","category":"page"},{"location":"tutorials/gcn_fixed_graph/","page":"GCN with Fixed Graph","title":"GCN with Fixed Graph","text":"And the accuracy measurement also needs indices.","category":"page"},{"location":"tutorials/gcn_fixed_graph/","page":"GCN with Fixed Graph","title":"GCN with Fixed Graph","text":"function accuracy(model, X::AbstractArray, y::AbstractArray, idx)\n    return mean(onecold(softmax(cpu(model(X))[:,idx,:])) .== onecold(cpu(y)[:,idx,:]))\nend\n\naccuracy(model, loader::DataLoader, device, idx) = mean(accuracy(model, X |> device, y |> device, idx) for (X, y) in loader)","category":"page"},{"location":"tutorials/gcn_fixed_graph/#Step-5:-Training-GCN-Model","page":"GCN with Fixed Graph","title":"Step 5: Training GCN Model","text":"","category":"section"},{"location":"tutorials/gcn_fixed_graph/","page":"GCN with Fixed Graph","title":"GCN with Fixed Graph","text":"train_loader, test_loader, fg, train_idx, test_idx = load_data(:cora, args.batch_size)\n\n# optimizer\nopt = ADAM(args.η)\n\n# parameters\nps = Flux.params(model)\n\n# training\ntrain_steps = 0\n@info \"Start Training, total $(args.epochs) epochs\"\nfor epoch = 1:args.epochs\n    @info \"Epoch $(epoch)\"\n\n    for (X, y) in train_loader\n        grad = gradient(() -> model_loss(model, args.λ, X |> device, y |> device, train_idx |> device), ps)\n        Flux.Optimise.update!(opt, ps, grad)\n        train_steps += 1\n    end\nend","category":"page"},{"location":"tutorials/gcn_fixed_graph/","page":"GCN with Fixed Graph","title":"GCN with Fixed Graph","text":"Now we could just train the GCN model directly!","category":"page"},{"location":"abstractions/msgpass/#Message-passing-scheme","page":"Message passing scheme","title":"Message passing scheme","text":"","category":"section"},{"location":"abstractions/msgpass/","page":"Message passing scheme","title":"Message passing scheme","text":"Message passing scheme is a popular GNN scheme in many frameworks. It adapts the property of connectivity of neighbors and form a general approach for spatial graph convolutional neural network. It comprises two user-defined functions and one aggregate function. A message function is defined to process information from edge states and node states from neighbors and itself. Messages from each node are obtained and aggregated by aggregate function to provide node-level information for update function. Update function takes current node state and aggregated message and gives a new node state.","category":"page"},{"location":"abstractions/msgpass/","page":"Message passing scheme","title":"Message passing scheme","text":"Message passing scheme is realized into a abstract type MessagePassing. Any subtype of MessagePassing is a message passing layer which utilize default message and update functions:","category":"page"},{"location":"abstractions/msgpass/","page":"Message passing scheme","title":"Message passing scheme","text":"message(mp, x_i, x_j, e_ij) = x_j\nupdate(mp, m, x) = m","category":"page"},{"location":"abstractions/msgpass/","page":"Message passing scheme","title":"Message passing scheme","text":"mp denotes a message passing layer. message accepts node state x_i for node i and its neighbor state x_j for node j, as well as corresponding edge state e_ij for edge (i,j). The default message function gives all the neighbor state x_j for neighbor of node i. update takes aggregated message m and current node state x, and then outputs m.","category":"page"},{"location":"abstractions/msgpass/","page":"Message passing scheme","title":"Message passing scheme","text":"GeometricFlux.MessagePassing","category":"page"},{"location":"abstractions/msgpass/#GeometricFlux.MessagePassing","page":"Message passing scheme","title":"GeometricFlux.MessagePassing","text":"MessagePassing\n\nAn abstract type for message-passing scheme.\n\nSee also message and update.\n\n\n\n\n\n","category":"type"},{"location":"abstractions/msgpass/#Message-function","page":"Message passing scheme","title":"Message function","text":"","category":"section"},{"location":"abstractions/msgpass/","page":"Message passing scheme","title":"Message passing scheme","text":"A message function accepts feature vector representing node state x_i, feature vectors for neighbor state x_j and corresponding edge state e_ij. A vector is expected to output from message for message. User can override message for customized message passing layer to provide desired behavior.","category":"page"},{"location":"abstractions/msgpass/","page":"Message passing scheme","title":"Message passing scheme","text":"GeometricFlux.message","category":"page"},{"location":"abstractions/msgpass/#GeometricFlux.message","page":"Message passing scheme","title":"GeometricFlux.message","text":"message(mp::MessagePassing, x_i, x_j, e_ij)\n\nMessage function for the message-passing scheme, returning the message from node j to node i . In the message-passing scheme. the incoming messages  from the neighborhood of i will later be aggregated in order to update the features of node i.\n\nBy default, the function returns x_j. Layers subtyping MessagePassing should  specialize this method with custom behavior.\n\nArguments\n\nmp: message-passing layer.\nx_i: the features of node i.\nx_j: the features of the nighbor j of node i.\ne_ij: the features of edge (i, j).\n\nSee also update.\n\n\n\n\n\n","category":"function"},{"location":"abstractions/msgpass/#Aggregate-messages","page":"Message passing scheme","title":"Aggregate messages","text":"","category":"section"},{"location":"abstractions/msgpass/","page":"Message passing scheme","title":"Message passing scheme","text":"Messages from message function are aggregated by an aggregate function. An aggregated message is passed to update function for node-level computation. An aggregate function is given by the following:","category":"page"},{"location":"abstractions/msgpass/","page":"Message passing scheme","title":"Message passing scheme","text":"propagate(mp, fg::FeaturedGraph, aggr::Symbol=:add)","category":"page"},{"location":"abstractions/msgpass/","page":"Message passing scheme","title":"Message passing scheme","text":"propagate function calls the whole message passing layer. fg acts as an input for message passing layer and aggr represents assignment of aggregate function to propagate function. :add represents an aggregate function of addition of all messages.","category":"page"},{"location":"abstractions/msgpass/","page":"Message passing scheme","title":"Message passing scheme","text":"The following aggr are available aggregate functions:","category":"page"},{"location":"abstractions/msgpass/","page":"Message passing scheme","title":"Message passing scheme","text":":add: sum over all messages :sub: negative of sum over all messages :mul: multiplication over all messages :div: inverse of multiplication over all messages :max: the maximum of all messages :min: the minimum of all messages :mean: the average of all messages","category":"page"},{"location":"abstractions/msgpass/#Update-function","page":"Message passing scheme","title":"Update function","text":"","category":"section"},{"location":"abstractions/msgpass/","page":"Message passing scheme","title":"Message passing scheme","text":"An update function takes aggregated message m and current node state x as arguments. An output vector is expected to be the new node state for next layer. User can override update for customized message passing layer to provide desired behavior.","category":"page"},{"location":"abstractions/msgpass/","page":"Message passing scheme","title":"Message passing scheme","text":"GeometricFlux.update","category":"page"},{"location":"abstractions/msgpass/#GeometricFlux.update","page":"Message passing scheme","title":"GeometricFlux.update","text":"update(mp::MessagePassing, m, x)\n\nUpdate function for the message-passing scheme, returning a new set of node features x′ based on old  features x and the incoming message from the neighborhood aggregation m.\n\nBy default, the function returns m. Layers subtyping MessagePassing should  specialize this method with custom behavior.\n\nArguments\n\nmp: message-passing layer.\nm: the aggregated edge messages from the message function.\nx: the node features to be updated.\n\nSee also message.\n\n\n\n\n\n","category":"function"},{"location":"manual/pool/#Pooling-layers","page":"Pooling Layers","title":"Pooling layers","text":"","category":"section"},{"location":"manual/pool/","page":"Pooling Layers","title":"Pooling Layers","text":"GlobalPool","category":"page"},{"location":"manual/pool/#GeometricFlux.GlobalPool","page":"Pooling Layers","title":"GeometricFlux.GlobalPool","text":"GlobalPool(aggr, dim...)\n\nGlobal pooling layer.\n\nIt pools all features with aggr operation.\n\nArguments\n\naggr: An aggregate function applied to pool all features.\n\n\n\n\n\n","category":"type"},{"location":"manual/pool/","page":"Pooling Layers","title":"Pooling Layers","text":"LocalPool","category":"page"},{"location":"manual/pool/#GeometricFlux.LocalPool","page":"Pooling Layers","title":"GeometricFlux.LocalPool","text":"LocalPool(aggr, cluster)\n\nLocal pooling layer.\n\nIt pools features with aggr operation accroding to cluster. It is implemented with scatter operation.\n\nArguments\n\naggr: An aggregate function applied to pool all features.\ncluster: An index structure which indicates what features to aggregate with.\n\n\n\n\n\n","category":"type"},{"location":"manual/pool/","page":"Pooling Layers","title":"Pooling Layers","text":"TopKPool","category":"page"},{"location":"manual/pool/#GeometricFlux.TopKPool","page":"Pooling Layers","title":"GeometricFlux.TopKPool","text":"TopKPool(adj, k, in_channel)\n\nTop-k pooling layer.\n\nArguments\n\nadj: Adjacency matrix  of a graph.\nk: Top-k nodes are selected to pool together.\nin_channel: The dimension of input channel.\n\n\n\n\n\n","category":"type"},{"location":"manual/conv/#Convolution-Layers","page":"Convolutional Layers","title":"Convolution Layers","text":"","category":"section"},{"location":"manual/conv/#Graph-Convolutional-Layer","page":"Convolutional Layers","title":"Graph Convolutional Layer","text":"","category":"section"},{"location":"manual/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"X = sigma(hatD^-12 hatA hatD^-12 X Theta)","category":"page"},{"location":"manual/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"where hatA = A + I, A denotes the adjacency matrix, and hatD = hatd_ij = sum_j=0 hata_ij is degree matrix.","category":"page"},{"location":"manual/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"GCNConv","category":"page"},{"location":"manual/conv/#GeometricFlux.GCNConv","page":"Convolutional Layers","title":"GeometricFlux.GCNConv","text":"GCNConv(in => out, σ=identity; bias=true, init=glorot_uniform)\n\nGraph convolutional layer. The input to the layer is a node feature array X of size (num_features, num_nodes).\n\nArguments\n\nin: The dimension of input features.\nout: The dimension of output features.\nσ: Activation function.\nbias: Add learnable bias.\ninit: Weights' initializer.\n\nExample\n\njulia> using GeometricFlux, Flux\n\njulia> gc = GCNConv(1024=>256, relu)\nGCNConv(1024 => 256, relu)\n\nSee also WithGraph for training layer with static graph.\n\n\n\n\n\n","category":"type"},{"location":"manual/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"Reference: Semi-supervised Classification with Graph Convolutional Networks","category":"page"},{"location":"manual/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"","category":"page"},{"location":"manual/conv/#Chebyshev-Spectral-Graph-Convolutional-Layer","page":"Convolutional Layers","title":"Chebyshev Spectral Graph Convolutional Layer","text":"","category":"section"},{"location":"manual/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"X = sum^K-1_k=0 Z^(k) Theta^(k)","category":"page"},{"location":"manual/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"where Z^(k) is the k-th term of Chebyshev polynomials, and can be calculated by the following recursive form:","category":"page"},{"location":"manual/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"Z^(0) = X \nZ^(1) = hatL X \nZ^(k) = 2 hatL Z^(k-1) - Z^(k-2)","category":"page"},{"location":"manual/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"and hatL = frac2lambda_max L - I.","category":"page"},{"location":"manual/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"ChebConv","category":"page"},{"location":"manual/conv/#GeometricFlux.ChebConv","page":"Convolutional Layers","title":"GeometricFlux.ChebConv","text":"ChebConv([fg,] in=>out, k; bias=true, init=glorot_uniform)\n\nChebyshev spectral graph convolutional layer.\n\nArguments\n\nfg: Optionally pass a FeaturedGraph. \nin: The dimension of input features.\nout: The dimension of output features.\nk: The order of Chebyshev polynomial.\nbias: Add learnable bias.\ninit: Weights' initializer.\n\n\n\n\n\n","category":"type"},{"location":"manual/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"Reference: Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering","category":"page"},{"location":"manual/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"","category":"page"},{"location":"manual/conv/#Graph-Neural-Network-Layer","page":"Convolutional Layers","title":"Graph Neural Network Layer","text":"","category":"section"},{"location":"manual/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"textbfx_i = sigma (Theta_1 textbfx_i + sum_j in mathcalN(i) Theta_2 textbfx_j)","category":"page"},{"location":"manual/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"GraphConv","category":"page"},{"location":"manual/conv/#GeometricFlux.GraphConv","page":"Convolutional Layers","title":"GeometricFlux.GraphConv","text":"GraphConv([fg,] in => out, σ=identity, aggr=+; bias=true, init=glorot_uniform)\n\nGraph neural network layer.\n\nArguments\n\nfg: Optionally pass a FeaturedGraph. \nin: The dimension of input features.\nout: The dimension of output features.\nσ: Activation function.\naggr: An aggregate function applied to the result of message function. +, -,\n\n*, /, max, min and mean are available.\n\nbias: Add learnable bias.\ninit: Weights' initializer.\n\n\n\n\n\n","category":"type"},{"location":"manual/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"Reference: Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks","category":"page"},{"location":"manual/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"","category":"page"},{"location":"manual/conv/#Graph-Attentional-Layer","page":"Convolutional Layers","title":"Graph Attentional Layer","text":"","category":"section"},{"location":"manual/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"textbfx_i = alpha_ii Theta textbfx_i + sum_j in mathcalN(i) alpha_ij Theta textbfx_j","category":"page"},{"location":"manual/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"where the attention coefficient alpha_ij can be calculated from","category":"page"},{"location":"manual/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"alpha_ij = fracexp(LeakyReLU(textbfa^T Theta textbfx_i  Theta textbfx_j))sum_k in mathcalN(i) cup i exp(LeakyReLU(textbfa^T Theta textbfx_i  Theta textbfx_k))","category":"page"},{"location":"manual/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"GATConv","category":"page"},{"location":"manual/conv/#GeometricFlux.GATConv","page":"Convolutional Layers","title":"GeometricFlux.GATConv","text":"GATConv([fg,] in => out;\n        heads=1,\n        concat=true,\n        init=glorot_uniform    \n        bias=true, \n        negative_slope=0.2)\n\nGraph attentional layer.\n\nArguments\n\nfg: Optionally pass a FeaturedGraph. \nin: The dimension of input features.\nout: The dimension of output features.\nbias::Bool: Keyword argument, whether to learn the additive bias.\nheads: Number attention heads \nconcat: Concatenate layer output or not. If not, layer output is averaged.\nnegative_slope::Real: Keyword argument, the parameter of LeakyReLU.\n\n\n\n\n\n","category":"type"},{"location":"manual/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"Reference: Graph Attention Networks","category":"page"},{"location":"manual/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"","category":"page"},{"location":"manual/conv/#Gated-Graph-Convolution-Layer","page":"Convolutional Layers","title":"Gated Graph Convolution Layer","text":"","category":"section"},{"location":"manual/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"textbfh^(0)_i = textbfx_i  textbf0 \ntextbfh^(l)_i = GRU(textbfh^(l-1)_i sum_j in mathcalN(i) Theta textbfh^(l-1)_j)","category":"page"},{"location":"manual/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"where textbfh^(l)_i denotes the l-th hidden variables passing through GRU. The dimension of input textbfx_i needs to be less or equal to out.","category":"page"},{"location":"manual/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"GatedGraphConv","category":"page"},{"location":"manual/conv/#GeometricFlux.GatedGraphConv","page":"Convolutional Layers","title":"GeometricFlux.GatedGraphConv","text":"GatedGraphConv([fg,] out, num_layers; aggr=+, init=glorot_uniform)\n\nGated graph convolution layer.\n\nArguments\n\nfg: Optionally pass a FeaturedGraph. \nout: The dimension of output features.\nnum_layers: The number of gated recurrent unit.\naggr: An aggregate function applied to the result of message function. +, -,\n\n*, /, max, min and mean are available.\n\n\n\n\n\n","category":"type"},{"location":"manual/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"Reference: Gated Graph Sequence Neural Networks","category":"page"},{"location":"manual/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"","category":"page"},{"location":"manual/conv/#Edge-Convolutional-Layer","page":"Convolutional Layers","title":"Edge Convolutional Layer","text":"","category":"section"},{"location":"manual/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"textbfx_i = sum_j in mathcalN(i) f_Theta(textbfx_i  textbfx_j - textbfx_i)","category":"page"},{"location":"manual/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"where f_Theta denotes a neural network parametrized by Theta, i.e., a MLP.","category":"page"},{"location":"manual/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"EdgeConv","category":"page"},{"location":"manual/conv/#GeometricFlux.EdgeConv","page":"Convolutional Layers","title":"GeometricFlux.EdgeConv","text":"EdgeConv([fg,] nn; aggr=max)\n\nEdge convolutional layer.\n\nArguments\n\nfg: Optionally pass a FeaturedGraph. \nnn: A neural network (e.g. a Dense layer or a MLP). \naggr: An aggregate function applied to the result of message function. +, max and mean are available.\n\n\n\n\n\n","category":"type"},{"location":"manual/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"Reference: Dynamic Graph CNN for Learning on Point Clouds","category":"page"},{"location":"manual/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"","category":"page"},{"location":"manual/conv/#Graph-Isomorphism-Network","page":"Convolutional Layers","title":"Graph Isomorphism Network","text":"","category":"section"},{"location":"manual/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"textbfx_i = f_Thetaleft((1 + varepsilon) dot textbfx_i + sum_j in mathcalN(i) textbfx_j right)","category":"page"},{"location":"manual/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"where f_Theta denotes a neural network parametrized by Theta, i.e., a MLP.","category":"page"},{"location":"manual/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"GINConv","category":"page"},{"location":"manual/conv/#GeometricFlux.GINConv","page":"Convolutional Layers","title":"GeometricFlux.GINConv","text":"GINConv([fg,] nn, [eps=0])\n\nGraph Isomorphism Network.\n\nArguments\n\nfg: Optionally pass in a FeaturedGraph as input.\nnn: A neural network/layer.\neps: Weighting factor.\n\nThe definition of this is as defined in the original paper, Xu et. al. (2018) https://arxiv.org/abs/1810.00826.\n\n\n\n\n\n","category":"type"},{"location":"manual/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"Reference: How Powerful are Graph Neural Networks?","category":"page"},{"location":"manual/conv/#Crystal-Graph-Convolutional-Network","page":"Convolutional Layers","title":"Crystal Graph Convolutional Network","text":"","category":"section"},{"location":"manual/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"textbfx_i = textbfx_i + sum_j in mathcalN(i) sigmaleft( textbfz_ij textbfW_f + textbfb_f right) odot textsoftplusleft(textbfz_ij textbfW_s + textbfb_s right)","category":"page"},{"location":"manual/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"where textbfz_ij = textbfx_i textbfx_j textbfe_ij denotes the concatenation of node features, neighboring node features, and edge features. The operation odot represents elementwise multiplication, and sigma denotes the sigmoid function.","category":"page"},{"location":"manual/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"CGConv","category":"page"},{"location":"manual/conv/#GeometricFlux.CGConv","page":"Convolutional Layers","title":"GeometricFlux.CGConv","text":"CGConv([fg,] (node_dim, edge_dim), out, init, bias=true, as_edge=false)\n\nCrystal Graph Convolutional network. Uses both node and edge features.\n\nArguments\n\nfg: Optional [FeaturedGraph] argument(@ref)\nnode_dim: Dimensionality of the input node features. Also is necessarily the output dimensionality.\nedge_dim: Dimensionality of the input edge features.\nout: Dimensionality of the output features.\ninit: Initialization algorithm for each of the weight matrices\nbias: Whether or not to learn an additive bias parameter.\nas_edge: When call to layer CGConv(M), accept input feature as node features or edge features.\n\nUsage\n\nYou can call CGConv in several different ways:\n\nPass a FeaturedGraph: CGConv(fg), returns FeaturedGraph \nPass both node and edge features: CGConv(X, E) \nPass one matrix, which is determined as node features or edge features by as_edge keyword argument.\n\n\n\n\n\n","category":"type"},{"location":"manual/conv/","page":"Convolutional Layers","title":"Convolutional Layers","text":"Reference: Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable Prediction of Material Properties","category":"page"},{"location":"manual/linalg/#Linear-Algebra","page":"Linear Algebra","title":"Linear Algebra","text":"","category":"section"},{"location":"manual/linalg/","page":"Linear Algebra","title":"Linear Algebra","text":"GraphSignals.degrees\nGraphSignals.degree_matrix\nGraphSignals.normalized_adjacency_matrix\nGraphSignals.laplacian_matrix\nGraphSignals.normalized_laplacian\nGraphSignals.scaled_laplacian\nGraphSignals.random_walk_laplacian\nGraphSignals.signless_laplacian","category":"page"},{"location":"manual/linalg/#GraphSignals.degrees","page":"Linear Algebra","title":"GraphSignals.degrees","text":"degrees(g, [T]; dir=:out)\n\nDegree of each vertex. Return a vector which contains the degree of each vertex in graph g.\n\nArguments\n\ng: should be a adjacency matrix, SimpleGraph, SimpleDiGraph (from Graphs) or   SimpleWeightedGraph, SimpleWeightedDiGraph (from SimpleWeightedGraphs).\nT: result element type of degree vector; default is the element type of g (optional).\ndir: direction of degree; should be :in, :out, or :both (optional).\n\nExamples\n\njulia> using GraphSignals\n\njulia> m = [0 1 1; 1 0 0; 1 0 0];\n\njulia> GraphSignals.degrees(m)\n3-element Vector{Int64}:\n 2\n 1\n 1\n\n\n\n\n\n","category":"function"},{"location":"manual/linalg/#GraphSignals.degree_matrix","page":"Linear Algebra","title":"GraphSignals.degree_matrix","text":"degree_matrix(g, [T]; dir=:out)\n\nDegree matrix of graph g. Return a matrix which contains degrees of each vertex in its diagonal. The values other than diagonal are zeros.\n\nArguments\n\ng: should be a adjacency matrix, FeaturedGraph, SimpleGraph, SimpleDiGraph (from Graphs)   or SimpleWeightedGraph, SimpleWeightedDiGraph (from SimpleWeightedGraphs).\nT: result element type of degree vector; default is the element type of g (optional).\ndir: direction of degree; should be :in, :out, or :both (optional).\n\nExamples\n\njulia> using GraphSignals\n\njulia> m = [0 1 1; 1 0 0; 1 0 0];\n\njulia> GraphSignals.degree_matrix(m)\n3×3 LinearAlgebra.Diagonal{Int64, Vector{Int64}}:\n 2  ⋅  ⋅\n ⋅  1  ⋅\n ⋅  ⋅  1\n\n\n\n\n\n","category":"function"},{"location":"manual/linalg/#GraphSignals.normalized_adjacency_matrix","page":"Linear Algebra","title":"GraphSignals.normalized_adjacency_matrix","text":"normalized_adjacency_matrix(g, [T]; selfloop=false)\n\nNormalized adjacency matrix of graph g.\n\nArguments\n\ng: should be a adjacency matrix, FeaturedGraph, SimpleGraph, SimpleDiGraph (from Graphs)   or SimpleWeightedGraph, SimpleWeightedDiGraph (from SimpleWeightedGraphs).\nT: result element type of degree vector; default is the element type of g (optional).\nselfloop: adding self loop while calculating the matrix (optional).\n\n\n\n\n\n","category":"function"},{"location":"manual/linalg/#Graphs.LinAlg.laplacian_matrix","page":"Linear Algebra","title":"Graphs.LinAlg.laplacian_matrix","text":"laplacian_matrix(g, [T]; dir=:out)\n\nLaplacian matrix of graph g.\n\nArguments\n\ng: should be a adjacency matrix, FeaturedGraph, SimpleGraph, SimpleDiGraph (from Graphs)   or SimpleWeightedGraph, SimpleWeightedDiGraph (from SimpleWeightedGraphs).\nT: result element type of degree vector; default is the element type of g (optional).\ndir: direction of degree; should be :in, :out, or :both (optional).\n\n\n\n\n\n","category":"function"},{"location":"manual/linalg/#GraphSignals.normalized_laplacian","page":"Linear Algebra","title":"GraphSignals.normalized_laplacian","text":"normalized_laplacian(g, [T]; dir=:both, selfloop=false)\n\nNormalized Laplacian matrix of graph g.\n\nArguments\n\ng: should be a adjacency matrix, FeaturedGraph, SimpleGraph, SimpleDiGraph (from Graphs)   or SimpleWeightedGraph, SimpleWeightedDiGraph (from SimpleWeightedGraphs).\nT: result element type of degree vector; default is the element type of g (optional).\nselfloop: adding self loop while calculating the matrix (optional).\ndir: direction of graph; should be :in or :out (optional).\n\n\n\n\n\n","category":"function"},{"location":"manual/linalg/#GraphSignals.scaled_laplacian","page":"Linear Algebra","title":"GraphSignals.scaled_laplacian","text":"scaled_laplacian(g, [T])\n\nScaled Laplacien matrix of graph g, defined as hatL = frac2lambda_max L - I where L is the normalized Laplacian matrix.\n\nArguments\n\ng: should be a adjacency matrix, FeaturedGraph, SimpleGraph, SimpleDiGraph (from Graphs)   or SimpleWeightedGraph, SimpleWeightedDiGraph (from SimpleWeightedGraphs).\nT: result element type of degree vector; default is the element type of g (optional).\n\n\n\n\n\n","category":"function"},{"location":"manual/linalg/#GraphSignals.random_walk_laplacian","page":"Linear Algebra","title":"GraphSignals.random_walk_laplacian","text":"random_walk_laplacian(g, [T]; dir=:out)\n\nRandom walk normalized Laplacian matrix of graph g.\n\nArguments\n\ng: should be a adjacency matrix, FeaturedGraph, SimpleGraph, SimpleDiGraph (from Graphs)   or SimpleWeightedGraph, SimpleWeightedDiGraph (from SimpleWeightedGraphs).\nT: result element type of degree vector; default is the element type of g (optional).\ndir: direction of degree; should be :in, :out, or :both (optional).\n\n\n\n\n\n","category":"function"},{"location":"manual/linalg/#GraphSignals.signless_laplacian","page":"Linear Algebra","title":"GraphSignals.signless_laplacian","text":"signless_laplacian(g, [T]; dir=:out)\n\nSignless Laplacian matrix of graph g.\n\nArguments\n\ng: should be a adjacency matrix, FeaturedGraph, SimpleGraph, SimpleDiGraph (from Graphs)   or SimpleWeightedGraph, SimpleWeightedDiGraph (from SimpleWeightedGraphs).\nT: result element type of degree vector; default is the element type of g (optional).\ndir: direction of degree; should be :in, :out, or :both (optional).\n\n\n\n\n\n","category":"function"},{"location":"cooperate/#Cooperate-with-Flux-Layers","page":"Cooperate with Flux Layers","title":"Cooperate with Flux Layers","text":"","category":"section"},{"location":"cooperate/","page":"Cooperate with Flux Layers","title":"Cooperate with Flux Layers","text":"GeometricFlux is designed to be compatible with Flux layers. Flux layers usually have array input and array output. Since the mechanism of \"what you feed is what you get\", the API for array type is compatible directly with other Flux layers. However, the API for FeaturedGraph is not compatible directly.","category":"page"},{"location":"cooperate/#Fetching-Features-from-FeaturedGraph-and-Output-Compatible-Result-with-Flux-Layers","page":"Cooperate with Flux Layers","title":"Fetching Features from FeaturedGraph and Output Compatible Result with Flux Layers","text":"","category":"section"},{"location":"cooperate/","page":"Cooperate with Flux Layers","title":"Cooperate with Flux Layers","text":"With a layer outputs a FeaturedGraph, it is not compatible with Flux layers. Since Flux layers need single feature in array form as input, node features, edge features and global features can be selected by using FeaturedGraph APIs: node_feature, edge_feature or global_feature, respectively.","category":"page"},{"location":"cooperate/","page":"Cooperate with Flux Layers","title":"Cooperate with Flux Layers","text":"model = Chain(\n    GCNConv(1024=>256, relu),\n    node_feature,  # or edge_feature or global_feature\n    softmax\n)","category":"page"},{"location":"cooperate/","page":"Cooperate with Flux Layers","title":"Cooperate with Flux Layers","text":"In a multitask learning scenario, multiple outputs are required. A branching selection of features can be made as follows:","category":"page"},{"location":"cooperate/","page":"Cooperate with Flux Layers","title":"Cooperate with Flux Layers","text":"model = Chain(\n    GCNConv(1024=>256, relu),\n    x -> (node_feature(x), global_feature(x)),\n    (nf, gf) -> (softmax(nf), identity.(gf))\n)","category":"page"},{"location":"cooperate/#Branching-Different-Features-Through-Different-Layers","page":"Cooperate with Flux Layers","title":"Branching Different Features Through Different Layers","text":"","category":"section"},{"location":"cooperate/","page":"Cooperate with Flux Layers","title":"Cooperate with Flux Layers","text":"A GraphParallel construct is designed for passing each feature through different layers from a FeaturedGraph. An example is given as follow:","category":"page"},{"location":"cooperate/","page":"Cooperate with Flux Layers","title":"Cooperate with Flux Layers","text":"Flux.Chain(\n    ...\n    GraphParallel(\n        node_layer=Dropout(0.5),\n        edge_layer=Dense(1024, 256, relu),\n        global_layer=identity,\n    ),\n    ...\n)","category":"page"},{"location":"cooperate/","page":"Cooperate with Flux Layers","title":"Cooperate with Flux Layers","text":"GraphParallel will pass node feature to a Dropout layer and edge feature to a Dense layer. Meanwhile, a FeaturedGraph is decomposed and keep the graph in FeaturedGraph to the downstream layers. A new FeaturedGraph is constructed with processed node feature, edge feature and global feature. GraphParallel acts as a layer which accepts a FeaturedGraph and output a FeaturedGraph. Thus, it by pass the graph in a FeaturedGraph but pass different features to different layers.","category":"page"},{"location":"cooperate/","page":"Cooperate with Flux Layers","title":"Cooperate with Flux Layers","text":"GeometricFlux.GraphParallel","category":"page"},{"location":"cooperate/#GeometricFlux.GraphParallel","page":"Cooperate with Flux Layers","title":"GeometricFlux.GraphParallel","text":"GraphParallel(; node_layer=identity, edge_layer=identity, global_layer=identity)\n\nPassing features in FeaturedGraph in parallel. It takes FeaturedGraph as input and it can be specified by assigning layers for specific (node, edge and global) features.\n\nArguments\n\nnode_layer: A regular Flux layer for passing node features.\nedge_layer: A regular Flux layer for passing edge features.\nglobal_layer: A regular Flux layer for passing global features.\n\nExample\n\njulia> using Flux, GeometricFlux\n\njulia> l = GraphParallel(\n            node_layer=Dropout(0.5),\n            global_layer=Dense(10, 5)\n       )\nGraphParallel(node_layer=Dropout(0.5), edge_layer=identity, global_layer=Dense(10, 5))\n\n\n\n\n\n","category":"type"},{"location":"basics/layers/#Building-Graph-Neural-Networks","page":"Building Layers","title":"Building Graph Neural Networks","text":"","category":"section"},{"location":"basics/layers/","page":"Building Layers","title":"Building Layers","text":"Building GNN is as simple as building neural network in Flux. The syntax here is the same as Flux. Chain is used to stack layers into a GNN. A simple example is shown here:","category":"page"},{"location":"basics/layers/","page":"Building Layers","title":"Building Layers","text":"model = Chain(\n    GCNConv(feat=>h1),\n    GCNConv(h1=>h2, relu),\n)","category":"page"},{"location":"basics/layers/","page":"Building Layers","title":"Building Layers","text":"In the example above, the feature dimension in first layer is mapped from feat to h1. In second layer, h1 is then mapped to h2. Default activation function is given as identity if it is not specified by users.","category":"page"},{"location":"basics/layers/","page":"Building Layers","title":"Building Layers","text":"The initialization function GCNConv(...) constructs a GCNConv layer. For most of the layer types in GeometricFlux, a layer can be initialized in two ways:","category":"page"},{"location":"basics/layers/","page":"Building Layers","title":"Building Layers","text":"GNN layer without graph: initializing without a predefined graph topology. This allows the layer to accept different graph topology.\nGNN layer with static graph: initializing with a predefined graph topology, e.g. graph wrapped in FeaturedGraph. This strategy is suitable for datasets where each input requires the same graph structure and it has better performance than variable graph strategy.","category":"page"},{"location":"basics/layers/","page":"Building Layers","title":"Building Layers","text":"The example above demonstrate the variable graph strategy. The equivalent GNN architecture but with static graph strategy is shown as following:","category":"page"},{"location":"basics/layers/","page":"Building Layers","title":"Building Layers","text":"model = Chain(\n    WithGraph(fg, GCNConv(feat=>h1)),\n    WithGraph(fg, GCNConv(h1=>h2, relu)),\n)","category":"page"},{"location":"basics/layers/","page":"Building Layers","title":"Building Layers","text":"GeometricFlux.WithGraph","category":"page"},{"location":"basics/layers/#GeometricFlux.WithGraph","page":"Building Layers","title":"GeometricFlux.WithGraph","text":"WithGraph(layer, fg)\n\nTrain GNN layers with fixed graph.\n\nArguments\n\nlayer: A GNN layer.\nfg: A fixed FeaturedGraph to train with.\n\nExample\n\njulia> using GraphSignals, GeometricFlux\n\njulia> adj = [0 1 0 1;\n              1 0 1 0;\n              0 1 0 1;\n              1 0 1 0];\n\njulia> fg = FeaturedGraph(adj);\n\njulia> gc = WithGraph(GCNConv(1024=>256), fg)\nWithGraph(GCNConv(1024 => 256), FeaturedGraph(#V=4, #E=4))\n\n\n\n\n\n","category":"type"},{"location":"basics/layers/#Applying-Layers","page":"Building Layers","title":"Applying Layers","text":"","category":"section"},{"location":"basics/layers/","page":"Building Layers","title":"Building Layers","text":"When using GNN layers, the general guidelines are:","category":"page"},{"location":"basics/layers/","page":"Building Layers","title":"Building Layers","text":"With static graph strategy: you should pass in a d times n times batch matrix for node features, and the layer maps node features mathbbR^d rightarrow mathbbR^k then the output will be in matrix with dimensions k times n times batch. The same ostensibly goes for edge features but as of now no layer type supports outputting new edge features.\nWith variable graph strategy: you should pass in a FeaturedGraph, the output will be also be a FeaturedGraph with modified node (and/or edge) features. Add node_feature as the following entry in the Flux chain (or simply call node_feature() on the output) if you wish to subsequently convert them to matrix form.","category":"page"},{"location":"basics/layers/#Define-Your-Own-GNN-Layer","page":"Building Layers","title":"Define Your Own GNN Layer","text":"","category":"section"},{"location":"basics/layers/","page":"Building Layers","title":"Building Layers","text":"Customizing your own GNN layers are the same as defining a layer in Flux. You may want to check Flux documentation first.","category":"page"},{"location":"basics/layers/","page":"Building Layers","title":"Building Layers","text":"To define a customized GNN layer, for example, we take a simple GCNConv layer as example here.","category":"page"},{"location":"basics/layers/","page":"Building Layers","title":"Building Layers","text":"struct GCNConv <: AbstractGraphLayer\n    weight\n    bias\n    σ\nend\n\n@functor GCNConv","category":"page"},{"location":"basics/layers/","page":"Building Layers","title":"Building Layers","text":"We first should define a GCNConv type and let it be the subtype of AbstractGraphLayer. In this type, it holds parameters that a layer operate on. Don't forget to add @functor macro to GCNConv type.","category":"page"},{"location":"basics/layers/","page":"Building Layers","title":"Building Layers","text":"(l::GCNConv)(Ã::AbstractMatrix, x::AbstractMatrix) = l.σ.(l.weight * x * Ã .+ l.bias)","category":"page"},{"location":"basics/layers/","page":"Building Layers","title":"Building Layers","text":"Then, we can define the operation for GCNConv layer.","category":"page"},{"location":"basics/layers/","page":"Building Layers","title":"Building Layers","text":"function (l::GCNConv)(fg::AbstractFeaturedGraph)\n    nf = node_feature(fg)\n    Ã = Zygote.ignore() do\n        GraphSignals.normalized_adjacency_matrix(fg, eltype(nf); selfloop=true)\n    end\n    return ConcreteFeaturedGraph(fg, nf = l(Ã, nf))\nend","category":"page"},{"location":"basics/layers/","page":"Building Layers","title":"Building Layers","text":"Here comes to the GNN-specific behaviors. A GNN layer should accept object of subtype of AbstractFeaturedGraph to support variable graph strategy. A variable graph strategy should fetch node/edge/global features from fg and transform graph in fg into required form for layer operation, e.g. GCNConv layer needs a normalized adjacency matrix with self loop. Then, normalized adjacency matrix Ã and node features nf are pass through GCNConv layer l(Ã, nf) to give a new node feature. Finally, a ConcreteFeaturedGraph wrap graph in fg and new node features into a new object of subtype of AbstractFeaturedGraph.","category":"page"},{"location":"basics/layers/","page":"Building Layers","title":"Building Layers","text":"layer = GCNConv(10=>5, relu)\nnew_fg = layer(fg)\ngradient(() -> sum(node_feature(layer(fg))), Flux.params(layer))","category":"page"},{"location":"basics/layers/","page":"Building Layers","title":"Building Layers","text":"Now we complete a simple version of GCNConv layer. One can test the forward pass and gradient if they work properly.","category":"page"},{"location":"basics/layers/","page":"Building Layers","title":"Building Layers","text":"GeometricFlux.AbstractGraphLayer","category":"page"},{"location":"basics/layers/#GeometricFlux.AbstractGraphLayer","page":"Building Layers","title":"GeometricFlux.AbstractGraphLayer","text":"AbstractGraphLayer\n\nAn abstract type of graph neural network layer for GeometricFlux.\n\n\n\n\n\n","category":"type"},{"location":"introduction/#Introduction-to-Graph-Neural-Networks-(GNN)","page":"Introduction","title":"Introduction to Graph Neural Networks (GNN)","text":"","category":"section"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"Graph neural networks act as standalone network architecture other than convolutional neural networks (CNN), recurrent neural networks (RNN). As its name implies, GNN needs a graph as training data. The problem setting requires at least a graph to train on.","category":"page"},{"location":"introduction/#What-is-Graph-Neural-Networks?","page":"Introduction","title":"What is Graph Neural Networks?","text":"","category":"section"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"Graph convolutional layers is the building block for GNN, and it extends from classic convolutional layer. Convolutional layer performs convolution operation over regular grid geometry, e.g. pixels in images arrange regularly along vertical and horizontal directions, while graph convolutional performs convolutional operation over irregular graph topology, e.g. graph is composed of a set of nodes which connect to each other with edges. In signal processing field, images are viewed as a kind of signals. Precisely, image can be represented as a function which maps from coordinates to color for each pixel. A matrix satisfies the definition and it maps image indices to a RGB value representing each pixel. Analogically, a graph signal can be defined as a function which maps from node/edge in a graph to certain value or features. We call them node features or edge features if the features correspond to node or edge, respectively. Graph convolutional layer maps features on nodes or edges to their embeddings.","category":"page"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"<figure>\n    <img src=\"../assets/geometry.svg\" width=\"50%\" alt=\"geometry.svg\" /><br>\n    <figcaption><em>Geometry for images and graphs.</em></figcaption>\n</figure>","category":"page"},{"location":"introduction/#What-is-the-Difference-between-Deep-Learning-and-GNN?","page":"Introduction","title":"What is the Difference between Deep Learning and GNN?","text":"","category":"section"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"Practically, GNN requires graph to be input in a certain form and features will be mapped according to the input graph, while classic deep learning architecture doesn't require a graph or a geometric object as input. In the design of GeometricFlux, the input graph can be two kinds: static graph or variable graph. A static graph is carried within a GNN layer, while a variable graph can be carried with features. The concept of a static graph defines the graph topology in the GNN layer and view it as a built-in static topology for a layer. The concept of variable graph is, totally different from static graph, to consider graph as a part of input data, which is more nature to most of people.","category":"page"},{"location":"introduction/#Features-for-GNNs","page":"Introduction","title":"Features for GNNs","text":"","category":"section"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"Graph signals include node signals, edge signals and global (or graph) signals. According to the problem setting, signals are further classified as features or labels. Features that can be used in GNN includes node features, edge features and global (or graph) features. Global (or graph) features are features that corresponds the whole graph and represents the status of a graph.","category":"page"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"<figure>\n    <img src=\"../assets/graph signals.svg\" width=\"70%\" alt=\"graph signals.svg\" /><br>\n    <figcaption><em>Signals and graph signals.</em></figcaption>\n</figure>","category":"page"},{"location":"#GeometricFlux:-The-Geometric-Deep-Learning-Library-in-Julia","page":"Home","title":"GeometricFlux: The Geometric Deep Learning Library in Julia","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Welcome to GeometricFlux package! GeometricFlux is a framework for geometric deep learning/machine learning. It provides classic graph neural network layers and some utility constructs.","category":"page"},{"location":"","page":"Home","title":"Home","text":"It extends Flux machine learning library for geometric deep learning.\nIt supports of CUDA GPU with CUDA.jl\nIt integrates with JuliaGraphs ecosystems.\nIt supports generic graph neural network architectures (i.g. message passing scheme and graph network block)\nIt contains built-in GNN benchmark datasets (provided by GraphMLDatasets)","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"] add GeometricFlux","category":"page"},{"location":"#Quick-start","page":"Home","title":"Quick start","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The basic graph convolutional network (GCN) is constructed as follow.","category":"page"},{"location":"","page":"Home","title":"Home","text":"fg = FeaturedGraph(adj_mat)\nmodel = Chain(\n    WithGraph(fg, GCNConv(num_features=>hidden, relu)),\n    WithGraph(fg, GCNConv(hidden=>target_dim)),\n    softmax\n)","category":"page"},{"location":"#Load-dataset","page":"Home","title":"Load dataset","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Load cora dataset from GeometricFlux.","category":"page"},{"location":"","page":"Home","title":"Home","text":"using GeometricFlux.Datasets\n\ntrain_X, train_y = traindata(Planetoid(), :cora)\ntest_X, test_y = testdata(Planetoid(), :cora)\ng = graphdata(Planetoid(), :cora)\ntrain_idx = train_indices(Planetoid(), :cora)\ntest_idx = test_indices(Planetoid(), :cora)","category":"page"},{"location":"#Training/testing-data","page":"Home","title":"Training/testing data","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Data is stored in sparse array, thus, we have to convert it into normal array.","category":"page"},{"location":"","page":"Home","title":"Home","text":"train_X = train_X |> Matrix\ntrain_y = train_y |> Matrix","category":"page"},{"location":"#Loss-function","page":"Home","title":"Loss function","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"loss(x, y) = logitcrossentropy(model(x), y)\naccuracy(x, y) = mean(onecold(model(x)) .== onecold(y))","category":"page"},{"location":"#Training","page":"Home","title":"Training","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"ps = Flux.params(model)\ntrain_data = [(train_X, train_y)]\nopt = ADAM()\nevalcb() = @show(accuracy(train_X, train_y))\n\n@epochs epochs Flux.train!(loss, ps, train_data, opt, cb=throttle(evalcb, 10))","category":"page"},{"location":"#Logs","page":"Home","title":"Logs","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"[ Info: Epoch 1\naccuracy(train_X, train_y) = 0.11669128508124077\n[ Info: Epoch 2\naccuracy(train_X, train_y) = 0.19608567208271788\n[ Info: Epoch 3\naccuracy(train_X, train_y) = 0.3098227474150665\n[ Info: Epoch 4\naccuracy(train_X, train_y) = 0.387370753323486\n[ Info: Epoch 5\naccuracy(train_X, train_y) = 0.44645494830132937\n[ Info: Epoch 6\naccuracy(train_X, train_y) = 0.46824224519940916\n[ Info: Epoch 7\naccuracy(train_X, train_y) = 0.48892171344165436\n[ Info: Epoch 8\naccuracy(train_X, train_y) = 0.5025849335302807\n[ Info: Epoch 9\naccuracy(train_X, train_y) = 0.5151403249630724\n[ Info: Epoch 10\naccuracy(train_X, train_y) = 0.5291728212703102\n[ Info: Epoch 11\naccuracy(train_X, train_y) = 0.543205317577548\n[ Info: Epoch 12\naccuracy(train_X, train_y) = 0.5550221565731167\n[ Info: Epoch 13\naccuracy(train_X, train_y) = 0.5638847858197932\n[ Info: Epoch 14\naccuracy(train_X, train_y) = 0.5657311669128509\n[ Info: Epoch 15\naccuracy(train_X, train_y) = 0.5749630723781388\n[ Info: Epoch 16\naccuracy(train_X, train_y) = 0.5834564254062038\n[ Info: Epoch 17\naccuracy(train_X, train_y) = 0.5919497784342689\n[ Info: Epoch 18\naccuracy(train_X, train_y) = 0.5978581979320532\n[ Info: Epoch 19\naccuracy(train_X, train_y) = 0.6019202363367799\n[ Info: Epoch 20\naccuracy(train_X, train_y) = 0.6067208271787297","category":"page"},{"location":"","page":"Home","title":"Home","text":"Check examples/semisupervised_gcn.jl for details.","category":"page"},{"location":"abstractions/gn/#Graph-network-block","page":"Graph network block","title":"Graph network block","text":"","category":"section"},{"location":"abstractions/gn/","page":"Graph network block","title":"Graph network block","text":"Graph network (GN) is a more generic model for graph neural network. It describes an update order: edge, node and then global. There are three corresponding update functions for edge, node and then global, respectively. Three update functions return their default values as follow:","category":"page"},{"location":"abstractions/gn/","page":"Graph network block","title":"Graph network block","text":"update_edge(gn, e, vi, vj, u) = e\nupdate_vertex(gn, ē, vi, u) = vi\nupdate_global(gn, ē, v̄, u) = u","category":"page"},{"location":"abstractions/gn/","page":"Graph network block","title":"Graph network block","text":"Information propagation between different levels are achieved by aggregate functions. Three aggregate functions aggregate_neighbors, aggregate_edges and aggregate_vertices are defined to aggregate states.","category":"page"},{"location":"abstractions/gn/","page":"Graph network block","title":"Graph network block","text":"GN block is realized into a abstract type GraphNet. User can make a subtype of GraphNet to customize GN block. Thus, a GN block is defined as a layer in GNN. MessagePassing is a subtype of GraphNet.","category":"page"},{"location":"abstractions/gn/#Update-functions","page":"Graph network block","title":"Update functions","text":"","category":"section"},{"location":"abstractions/gn/","page":"Graph network block","title":"Graph network block","text":"update_edge acts as the first update function to apply to edge states. It takes edge state e, node i state vi, node j state vj and global state u. It is expected to return a feature vector for new edge state. update_vertex updates nodes state by taking aggregated edge state ē, node i state vi and global state u. It is expected to return a feature vector for new node state. update_global updates global state with aggregated information from edge and node. It takes aggregated edge state ē, aggregated node state v̄ and global state u. It is expected to return a feature vector for new global state. User can define their own behavior by overriding update functions.","category":"page"},{"location":"abstractions/gn/#Aggregate-functions","page":"Graph network block","title":"Aggregate functions","text":"","category":"section"},{"location":"abstractions/gn/","page":"Graph network block","title":"Graph network block","text":"An aggregate function aggregate_neighbors aggregates edge states for edges incident to some node i into node-level information. Aggregate function aggregate_edges aggregates all edge states into global-level information. The last aggregate function aggregate_vertices aggregates all vertex states into global-level information. It is available for assigning aggregate function by assigning aggregate operations to propagate function.","category":"page"},{"location":"abstractions/gn/","page":"Graph network block","title":"Graph network block","text":"propagate(gn, fg::FeaturedGraph, naggr=nothing, eaggr=nothing, vaggr=nothing)","category":"page"},{"location":"abstractions/gn/","page":"Graph network block","title":"Graph network block","text":"naggr, eaggr and vaggr are arguments for aggregate_neighbors, aggregate_edges and aggregate_vertices, respectively. Available aggregate functions are assigned by following symbols to them: :add, :sub, :mul, :div, :max, :min and :mean.","category":"page"}]
}
