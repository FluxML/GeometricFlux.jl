var documenterSearchIndex = {"docs":
[{"location":"manual/linalg/#Linear-Algebra-1","page":"Linear Algebra","title":"Linear Algebra","text":"","category":"section"},{"location":"manual/linalg/#","page":"Linear Algebra","title":"Linear Algebra","text":"degrees","category":"page"},{"location":"manual/models/#Models-1","page":"Models","title":"Models","text":"","category":"section"},{"location":"manual/pool/#Pooling-layers-1","page":"Pooling Layers","title":"Pooling layers","text":"","category":"section"},{"location":"manual/conv/#Convolution-Layers-1","page":"Convolutional Layers","title":"Convolution Layers","text":"","category":"section"},{"location":"manual/conv/#Graph-Convolutional-Layer-1","page":"Convolutional Layers","title":"Graph Convolutional Layer","text":"","category":"section"},{"location":"manual/conv/#","page":"Convolutional Layers","title":"Convolutional Layers","text":"X = sigma(hatD^-12 hatA hatD^-12 X Theta)","category":"page"},{"location":"manual/conv/#","page":"Convolutional Layers","title":"Convolutional Layers","text":"where hatA = A + I, A denotes the adjacency matrix, and hatD = hatd_ij = sum_j=0 hata_ij is degree matrix.","category":"page"},{"location":"manual/conv/#","page":"Convolutional Layers","title":"Convolutional Layers","text":"GCNConv","category":"page"},{"location":"manual/conv/#GeometricFlux.GCNConv","page":"Convolutional Layers","title":"GeometricFlux.GCNConv","text":"GCNConv([graph, ]in=>out)\nGCNConv([graph, ]in=>out, σ)\n\nGraph convolutional layer.\n\nArguments\n\ngraph: should be a adjacency matrix, SimpleGraph, SimpleDiGraph (from LightGraphs) or SimpleWeightedGraph, SimpleWeightedDiGraph (from SimpleWeightedGraphs).\nin: the dimension of input features.\nout: the dimension of output features.\nbias::Bool=true: keyword argument, whether to learn the additive bias.\n\nData should be stored in (# features, # nodes) order. For example, a 1000-node graph each node of which poses 100 feautres is constructed. The input data would be a 1000×100 array.\n\n\n\n\n\n","category":"type"},{"location":"manual/conv/#","page":"Convolutional Layers","title":"Convolutional Layers","text":"Reference: Semi-supervised Classification with Graph Convolutional Networks","category":"page"},{"location":"manual/conv/#","page":"Convolutional Layers","title":"Convolutional Layers","text":"","category":"page"},{"location":"manual/conv/#Chebyshev-Spectral-Graph-Convolutional-Layer-1","page":"Convolutional Layers","title":"Chebyshev Spectral Graph Convolutional Layer","text":"","category":"section"},{"location":"manual/conv/#","page":"Convolutional Layers","title":"Convolutional Layers","text":"X = sum^K-1_k=0 Z^(k) Theta^(k)","category":"page"},{"location":"manual/conv/#","page":"Convolutional Layers","title":"Convolutional Layers","text":"where Z^(k) is the k-th term of Chebyshev polynomials, and can be calculated by the following recursive form:","category":"page"},{"location":"manual/conv/#","page":"Convolutional Layers","title":"Convolutional Layers","text":"Z^(0) = X \nZ^(1) = hatL X \nZ^(k) = 2 hatL Z^(k-1) - Z^(k-2)","category":"page"},{"location":"manual/conv/#","page":"Convolutional Layers","title":"Convolutional Layers","text":"and hatL = frac2lambda_max L - I.","category":"page"},{"location":"manual/conv/#","page":"Convolutional Layers","title":"Convolutional Layers","text":"ChebConv","category":"page"},{"location":"manual/conv/#GeometricFlux.ChebConv","page":"Convolutional Layers","title":"GeometricFlux.ChebConv","text":"ChebConv(graph, in=>out, k)\n\nChebyshev spectral graph convolutional layer.\n\nArguments\n\ngraph: should be a adjacency matrix, SimpleGraph, SimpleDiGraph (from LightGraphs) or SimpleWeightedGraph, SimpleWeightedDiGraph (from SimpleWeightedGraphs).\nin: the dimension of input features.\nout: the dimension of output features.\nk: the order of Chebyshev polynomial.\nbias::Bool=true: keyword argument, whether to learn the additive bias.\n\n\n\n\n\n","category":"type"},{"location":"manual/conv/#","page":"Convolutional Layers","title":"Convolutional Layers","text":"Reference: Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering","category":"page"},{"location":"manual/conv/#","page":"Convolutional Layers","title":"Convolutional Layers","text":"","category":"page"},{"location":"manual/conv/#Graph-Neural-Network-Layer-1","page":"Convolutional Layers","title":"Graph Neural Network Layer","text":"","category":"section"},{"location":"manual/conv/#","page":"Convolutional Layers","title":"Convolutional Layers","text":"textbfx_i = Theta_1 textbfx_i + sum_j in mathcalN(i) Theta_2 textbfx_j","category":"page"},{"location":"manual/conv/#","page":"Convolutional Layers","title":"Convolutional Layers","text":"GraphConv","category":"page"},{"location":"manual/conv/#GeometricFlux.GraphConv","page":"Convolutional Layers","title":"GeometricFlux.GraphConv","text":"GraphConv(graph, in=>out)\nGraphConv(graph, in=>out, aggr)\n\nGraph neural network layer.\n\nArguments\n\ngraph: should be a adjacency matrix, SimpleGraph, SimpleDiGraph (from LightGraphs) or SimpleWeightedGraph, SimpleWeightedDiGraph (from SimpleWeightedGraphs).\nin: the dimension of input features.\nout: the dimension of output features.\nbias::Bool=true: keyword argument, whether to learn the additive bias.\naggr::Symbol=:add: an aggregate function applied to the result of message function. :add, :max and :mean are available.\n\n\n\n\n\n","category":"type"},{"location":"manual/conv/#","page":"Convolutional Layers","title":"Convolutional Layers","text":"Reference: Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks","category":"page"},{"location":"manual/conv/#","page":"Convolutional Layers","title":"Convolutional Layers","text":"","category":"page"},{"location":"manual/conv/#Graph-Attentional-Layer-1","page":"Convolutional Layers","title":"Graph Attentional Layer","text":"","category":"section"},{"location":"manual/conv/#","page":"Convolutional Layers","title":"Convolutional Layers","text":"textbfx_i = alpha_ii Theta textbfx_i + sum_j in mathcalN(i) alpha_ij Theta textbfx_j","category":"page"},{"location":"manual/conv/#","page":"Convolutional Layers","title":"Convolutional Layers","text":"where the attention coefficient alpha_ij can be calculated from","category":"page"},{"location":"manual/conv/#","page":"Convolutional Layers","title":"Convolutional Layers","text":"alpha_ij = fracexp(LeakyReLU(textbfa^T Theta textbfx_i  Theta textbfx_j))sum_k in mathcalN(i) cup i exp(LeakyReLU(textbfa^T Theta textbfx_i  Theta textbfx_k))","category":"page"},{"location":"manual/conv/#","page":"Convolutional Layers","title":"Convolutional Layers","text":"GATConv","category":"page"},{"location":"manual/conv/#GeometricFlux.GATConv","page":"Convolutional Layers","title":"GeometricFlux.GATConv","text":"GATConv(graph, in=>out)\n\nGraph attentional layer.\n\nArguments\n\ngraph: should be a adjacency matrix, SimpleGraph, SimpleDiGraph (from LightGraphs) or SimpleWeightedGraph, SimpleWeightedDiGraph (from SimpleWeightedGraphs).\nin: the dimension of input features.\nout: the dimension of output features.\nbias::Bool=true: keyword argument, whether to learn the additive bias.\nnegative_slope::Real=0.2: keyword argument, the parameter of LeakyReLU.\n\n\n\n\n\n","category":"type"},{"location":"manual/conv/#","page":"Convolutional Layers","title":"Convolutional Layers","text":"Reference: Graph Attention Networks","category":"page"},{"location":"manual/conv/#","page":"Convolutional Layers","title":"Convolutional Layers","text":"","category":"page"},{"location":"manual/conv/#Gated-Graph-Convolution-Layer-1","page":"Convolutional Layers","title":"Gated Graph Convolution Layer","text":"","category":"section"},{"location":"manual/conv/#","page":"Convolutional Layers","title":"Convolutional Layers","text":"textbfh^(0)_i = textbfx_i  textbf0 \ntextbfh^(l)_i = GRU(textbfh^(l-1)_i sum_j in mathcalN(i) Theta textbfh^(l-1)_j)","category":"page"},{"location":"manual/conv/#","page":"Convolutional Layers","title":"Convolutional Layers","text":"where textbfh^(l)_i denotes the l-th hidden variables passing through GRU. The dimension of input textbfx_i needs to be less or equal to out.","category":"page"},{"location":"manual/conv/#","page":"Convolutional Layers","title":"Convolutional Layers","text":"GatedGraphConv","category":"page"},{"location":"manual/conv/#GeometricFlux.GatedGraphConv","page":"Convolutional Layers","title":"GeometricFlux.GatedGraphConv","text":"GatedGraphConv(graph, out, num_layers)\n\nGated graph convolution layer.\n\nArguments\n\ngraph: should be a adjacency matrix, SimpleGraph, SimpleDiGraph (from LightGraphs) or SimpleWeightedGraph, SimpleWeightedDiGraph (from SimpleWeightedGraphs).\nout: the dimension of output features.\nnum_layers specifies the number of gated recurrent unit.\naggr::Symbol=:add: an aggregate function applied to the result of message function. :add, :max and :mean are available.\n\n\n\n\n\n","category":"type"},{"location":"manual/conv/#","page":"Convolutional Layers","title":"Convolutional Layers","text":"Reference: Gated Graph Sequence Neural Networks","category":"page"},{"location":"manual/conv/#","page":"Convolutional Layers","title":"Convolutional Layers","text":"","category":"page"},{"location":"manual/conv/#Edge-Convolutional-Layer-1","page":"Convolutional Layers","title":"Edge Convolutional Layer","text":"","category":"section"},{"location":"manual/conv/#","page":"Convolutional Layers","title":"Convolutional Layers","text":"textbfx_i = sum_j in mathcalN(i) f_Theta(textbfx_i  textbfx_j - textbfx_i)","category":"page"},{"location":"manual/conv/#","page":"Convolutional Layers","title":"Convolutional Layers","text":"where f_Theta denotes a neural network parametrized by Theta, i.e., a MLP.","category":"page"},{"location":"manual/conv/#","page":"Convolutional Layers","title":"Convolutional Layers","text":"EdgeConv","category":"page"},{"location":"manual/conv/#GeometricFlux.EdgeConv","page":"Convolutional Layers","title":"GeometricFlux.EdgeConv","text":"EdgeConv(graph, nn)\nEdgeConv(graph, nn, aggr)\n\nEdge convolutional layer.\n\nArguments\n\ngraph: should be a adjacency matrix, SimpleGraph, SimpleDiGraph (from LightGraphs) or SimpleWeightedGraph, SimpleWeightedDiGraph (from SimpleWeightedGraphs).\nnn: a neural network\naggr::Symbol=:max: an aggregate function applied to the result of message function. :add, :max and :mean are available.\n\n\n\n\n\n","category":"type"},{"location":"manual/conv/#","page":"Convolutional Layers","title":"Convolutional Layers","text":"Reference: Dynamic Graph CNN for Learning on Point Clouds","category":"page"},{"location":"#GeometricFlux:-The-Geometric-Deep-Learning-Library-in-Julia-1","page":"Home","title":"GeometricFlux: The Geometric Deep Learning Library in Julia","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"Documentation for GeometricFlux.jl","category":"page"},{"location":"#Installation-1","page":"Home","title":"Installation","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"] add GeometricFlux","category":"page"},{"location":"#Quick-start-1","page":"Home","title":"Quick start","text":"","category":"section"},{"location":"manual/utils/#Utilities-1","page":"Utilities","title":"Utilities","text":"","category":"section"},{"location":"manual/utils/#","page":"Utilities","title":"Utilities","text":"save_div","category":"page"}]
}
