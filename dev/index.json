[{"id":3,"pagetitle":"Home","title":"GeometricFlux: The Geometric Deep Learning Library in Julia","ref":"/geometricflux/stable/#GeometricFlux:-The-Geometric-Deep-Learning-Library-in-Julia","content":" GeometricFlux: The Geometric Deep Learning Library in Julia Welcome to GeometricFlux package! GeometricFlux is a framework for geometric deep learning/machine learning. It provides classic graph neural network layers and some utility constructs. It extends Flux machine learning library for geometric deep learning. It supports of CUDA GPU with CUDA.jl It integrates with JuliaGraphs ecosystems. It supports generic graph neural network architectures (i.g. message passing scheme and graph network block) It contains built-in GNN benchmark datasets (provided by GraphMLDatasets)"},{"id":4,"pagetitle":"Home","title":"Installation","ref":"/geometricflux/stable/#Installation","content":" Installation ] add GeometricFlux"},{"id":5,"pagetitle":"Home","title":"Quick start","ref":"/geometricflux/stable/#Quick-start","content":" Quick start The basic graph convolutional network (GCN) is constructed as follow. fg = FeaturedGraph(adj_mat)\nmodel = Chain(\n    WithGraph(fg, GCNConv(num_features=>hidden, relu)),\n    WithGraph(fg, GCNConv(hidden=>target_dim)),\n    softmax\n)"},{"id":6,"pagetitle":"Home","title":"Load dataset","ref":"/geometricflux/stable/#Load-dataset","content":" Load dataset Load cora dataset from GeometricFlux. using GeometricFlux.Datasets\n\ntrain_X, train_y = traindata(Planetoid(), :cora)\ntest_X, test_y = testdata(Planetoid(), :cora)\ng = graphdata(Planetoid(), :cora)\ntrain_idx = train_indices(Planetoid(), :cora)\ntest_idx = test_indices(Planetoid(), :cora)"},{"id":7,"pagetitle":"Home","title":"Training/testing data","ref":"/geometricflux/stable/#Training/testing-data","content":" Training/testing data Data is stored in sparse array, thus, we have to convert it into normal array. train_X = train_X |> Matrix\ntrain_y = train_y |> Matrix"},{"id":8,"pagetitle":"Home","title":"Loss function","ref":"/geometricflux/stable/#Loss-function","content":" Loss function loss(x, y) = logitcrossentropy(model(x), y)\naccuracy(x, y) = mean(onecold(model(x)) .== onecold(y))"},{"id":9,"pagetitle":"Home","title":"Training","ref":"/geometricflux/stable/#Training","content":" Training ps = Flux.params(model)\ntrain_data = [(train_X, train_y)]\nopt = ADAM()\nevalcb() = @show(accuracy(train_X, train_y))\n\n@epochs epochs Flux.train!(loss, ps, train_data, opt, cb=throttle(evalcb, 10))"},{"id":10,"pagetitle":"Home","title":"Logs","ref":"/geometricflux/stable/#Logs","content":" Logs [ Info: Epoch 1\naccuracy(train_X, train_y) = 0.11669128508124077\n[ Info: Epoch 2\naccuracy(train_X, train_y) = 0.19608567208271788\n[ Info: Epoch 3\naccuracy(train_X, train_y) = 0.3098227474150665\n[ Info: Epoch 4\naccuracy(train_X, train_y) = 0.387370753323486\n[ Info: Epoch 5\naccuracy(train_X, train_y) = 0.44645494830132937\n[ Info: Epoch 6\naccuracy(train_X, train_y) = 0.46824224519940916\n[ Info: Epoch 7\naccuracy(train_X, train_y) = 0.48892171344165436\n[ Info: Epoch 8\naccuracy(train_X, train_y) = 0.5025849335302807\n[ Info: Epoch 9\naccuracy(train_X, train_y) = 0.5151403249630724\n[ Info: Epoch 10\naccuracy(train_X, train_y) = 0.5291728212703102\n[ Info: Epoch 11\naccuracy(train_X, train_y) = 0.543205317577548\n[ Info: Epoch 12\naccuracy(train_X, train_y) = 0.5550221565731167\n[ Info: Epoch 13\naccuracy(train_X, train_y) = 0.5638847858197932\n[ Info: Epoch 14\naccuracy(train_X, train_y) = 0.5657311669128509\n[ Info: Epoch 15\naccuracy(train_X, train_y) = 0.5749630723781388\n[ Info: Epoch 16\naccuracy(train_X, train_y) = 0.5834564254062038\n[ Info: Epoch 17\naccuracy(train_X, train_y) = 0.5919497784342689\n[ Info: Epoch 18\naccuracy(train_X, train_y) = 0.5978581979320532\n[ Info: Epoch 19\naccuracy(train_X, train_y) = 0.6019202363367799\n[ Info: Epoch 20\naccuracy(train_X, train_y) = 0.6067208271787297 Check  examples/semisupervised_gcn.jl  for details."},{"id":13,"pagetitle":"Graph network block","title":"Graph network block","ref":"/geometricflux/stable/abstractions/gn/#Graph-network-block","content":" Graph network block Graph network (GN) is a more generic model for graph neural network. For details, a graph network block is defined as follow: \\[\\begin{aligned}\n    e_{ij}^{\\prime} &= \\phi^{e}(e_{ij}, v_i, v_j, u) \\\\\n    v_{i}^{\\prime} &= \\phi^{v}(\\bar{e}_{i}^{\\prime}, v_i, u) \\\\\n    u^{\\prime} &= \\phi^{u}(\\bar{e}^{\\prime}, \\bar{v}^{\\prime}, u)\n\\end{aligned}\n\\ \\ \\ \\ \n\\begin{aligned}\n    \\bar{e}_{i}^{\\prime} &= \\rho^{e \\rightarrow v}(E_i^{\\prime}) \\\\\n    \\bar{e}^{\\prime} &= \\rho^{e \\rightarrow u}(E^{\\prime}) \\\\\n    \\bar{v}^{\\prime} &= \\rho^{v \\rightarrow u}(V^{\\prime})\n\\end{aligned}\\] where  $\\phi$  and  $\\rho$  denote update functions and aggregate functions, respectively.  $v_i$  and  $v_j$  are node features from node  $i$  and its neighbor node  $j$ ,  $e_{ij}$  is edge feature for edge  $(i, j)$ , and  $u$  is global feature for whole graph.  $e_{ij}^{\\prime}$ ,  $v_{i}^{\\prime}$  and  $u^{\\prime}$  are new features for edge, node and global.  $\\bar{e}_{i}^{\\prime}$ ,  $\\bar{e}^{\\prime}$  and  $\\bar{v}^{\\prime}$  are aggregated new features for edge, node and global. Reference:  Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, Caglar Gulcehre, Francis Song, Andrew Ballard, Justin Gilmer, George Dahl, Ashish Vaswani, Kelsey Allen, Charles Nash, Victoria Langston, Chris Dyer, Nicolas Heess, Daan Wierstra, Pushmeet Kohli, Matt Botvinick, Oriol Vinyals, Yujia Li, Razvan Pascanu (2018) Ingraph network, it describes an update order: edge, node and then global. There are three corresponding update functions for edge, node and then global, respectively. Three update functions return their default values as follow: update_edge(gn, e, vi, vj, u) = e\nupdate_vertex(gn, ē, vi, u) = vi\nupdate_global(gn, ē, v̄, u) = u Information propagation between different levels are achieved by aggregate functions. Three aggregate functions  aggregate_neighbors ,  aggregate_edges  and  aggregate_vertices  are defined to aggregate states. GN block is realized into a abstract type  GraphNet . User can make a subtype of  GraphNet  to customize GN block. Thus, a GN block is defined as a layer in GNN.  MessagePassing  is a subtype of  GraphNet ."},{"id":14,"pagetitle":"Graph network block","title":"Update functions","ref":"/geometricflux/stable/abstractions/gn/#Update-functions","content":" Update functions update_edge  acts as the first update function to apply to edge states. It takes edge state  e , node  i  state  vi , node  j  state  vj  and global state  u . It is expected to return a feature vector for new edge state.  update_vertex  updates nodes state by taking aggregated edge state  ē , node  i  state  vi  and global state  u . It is expected to return a feature vector for new node state.  update_global  updates global state with aggregated information from edge and node. It takes aggregated edge state  ē , aggregated node state  v̄  and global state  u . It is expected to return a feature vector for new global state. User can define their own behavior by overriding update functions."},{"id":15,"pagetitle":"Graph network block","title":"GeometricFlux.update_edge","ref":"/geometricflux/stable/abstractions/gn/#GeometricFlux.update_edge","content":" GeometricFlux.update_edge  —  Function update_edge(gn, e, vi, vj, u) Update function for edge feature in graph network. Arguments gn::GraphNet : A graph network layer. e : Edge feature. vi : Node feature for node  i . vj : Node feature for neighbors of node  i . u : Global feature. See also  update_vertex ,  update_global ,  update_batch_edge ,  update_batch_vertex ,  aggregate_neighbors ,  aggregate_edges ,  aggregate_vertices . source"},{"id":16,"pagetitle":"Graph network block","title":"GeometricFlux.update_vertex","ref":"/geometricflux/stable/abstractions/gn/#GeometricFlux.update_vertex","content":" GeometricFlux.update_vertex  —  Function update_vertex(gn, ē, vi, u) Update function for node feature in graph network. Arguments gn::GraphNet : A graph network layer. ē : Aggregated edge feature. vi : Node feature for node  i . u : Global feature. See also  update_edge ,  update_global ,  update_batch_edge ,  update_batch_vertex ,  aggregate_neighbors ,  aggregate_edges ,  aggregate_vertices . source"},{"id":17,"pagetitle":"Graph network block","title":"GeometricFlux.update_global","ref":"/geometricflux/stable/abstractions/gn/#GeometricFlux.update_global","content":" GeometricFlux.update_global  —  Function update_global(gn, ē, v̄, u) Update function for global feature in graph network. Arguments gn::GraphNet : A graph network layer. ē : Aggregated edge feature. v̄ : Aggregated node feature for node  i . u : Global feature. See also  update_edge ,  update_vertex ,  update_batch_edge ,  update_batch_vertex ,  aggregate_neighbors ,  aggregate_edges ,  aggregate_vertices . source"},{"id":18,"pagetitle":"Graph network block","title":"GeometricFlux.update_batch_edge","ref":"/geometricflux/stable/abstractions/gn/#GeometricFlux.update_batch_edge","content":" GeometricFlux.update_batch_edge  —  Function update_batch_edge(gn, el, E, V, u) Returns new edge features of size  (E_out_dim, #E, [batch_size]) . Arguments gn::GraphNet : A graph network layer. el::NamedTuple : Collection of graph information. E : All edge features. Its size should be  (E_in_dim, #E, [batch_size]) . V : All node features. u : Global features. See also  update_edge ,  update_vertex ,  update_global ,  update_batch_vertex ,  aggregate_neighbors ,  aggregate_edges ,  aggregate_vertices . source"},{"id":19,"pagetitle":"Graph network block","title":"GeometricFlux.update_batch_vertex","ref":"/geometricflux/stable/abstractions/gn/#GeometricFlux.update_batch_vertex","content":" GeometricFlux.update_batch_vertex  —  Function update_batch_vertex(gn, el, Ē, V, u) Returns new node features of size  (V_out_dim, #V, [batch_size]) . Arguments gn::GraphNet : A graph network layer. el::NamedTuple : Collection of graph information. Ē : All edge features. Its size should be  (E_in_dim, #V, [batch_size]) . V : All node features. Its size should be  (V_in_dim, #V, [batch_size]) . u : Global features. See also  update_edge ,  update_vertex ,  update_global ,  update_batch_edge ,  aggregate_neighbors ,  aggregate_edges ,  aggregate_vertices . source"},{"id":20,"pagetitle":"Graph network block","title":"Aggregate functions","ref":"/geometricflux/stable/abstractions/gn/#Aggregate-functions","content":" Aggregate functions An aggregate function  aggregate_neighbors  aggregates edge states for edges incident to some node  i  into node-level information. Aggregate function  aggregate_edges  aggregates all edge states into global-level information. The last aggregate function  aggregate_vertices  aggregates all vertex states into global-level information."},{"id":21,"pagetitle":"Graph network block","title":"GeometricFlux.aggregate_neighbors","ref":"/geometricflux/stable/abstractions/gn/#GeometricFlux.aggregate_neighbors","content":" GeometricFlux.aggregate_neighbors  —  Function aggregate_neighbors(gn, el, aggr, E) Returns aggregated neighbor features of size  (E_out_dim, #V, [batch_size]) . Arguments gn::GraphNet : A graph network layer. el::NamedTuple : Collection of graph information. aggr : Aggregate function to apply on neighbor features. E : All edge features from neighbors. Its size should be  (E_out_dim, #E, [batch_size]) . See also  update_edge ,  update_vertex ,  update_global ,  update_batch_edge ,  update_batch_vertex ,  aggregate_edges ,  aggregate_vertices . source"},{"id":22,"pagetitle":"Graph network block","title":"GeometricFlux.aggregate_edges","ref":"/geometricflux/stable/abstractions/gn/#GeometricFlux.aggregate_edges","content":" GeometricFlux.aggregate_edges  —  Function aggregate_edges(gn, aggr, E) Returns aggregated edge features of size  (E_out_dim, 1, [batch_size])  for updating global feature. Arguments gn::GraphNet : A graph network layer. aggr : Aggregate function to apply on edge features. E : All edge features. Its size should be  (E_out_dim, #E, [batch_size]) . See also  update_edge ,  update_vertex ,  update_global ,  update_batch_edge ,  update_batch_vertex ,  aggregate_neighbors ,  aggregate_vertices . source"},{"id":23,"pagetitle":"Graph network block","title":"GeometricFlux.aggregate_vertices","ref":"/geometricflux/stable/abstractions/gn/#GeometricFlux.aggregate_vertices","content":" GeometricFlux.aggregate_vertices  —  Function aggregate_vertices(gn, aggr, V) Returns aggregated node features of size  (V_out_dim, 1, [batch_size])  for updating global feature. Arguments gn::GraphNet : A graph network layer. aggr : Aggregate function to apply on node features. V : All node features. Its size should be  (V_out_dim, #V, [batch_size]) . See also  update_edge ,  update_vertex ,  update_global ,  update_batch_edge ,  update_batch_vertex ,  aggregate_neighbors ,  aggregate_edges . source It is available for assigning aggregate function by assigning aggregate operations to  propagate  function. propagate(gn, fg::FeaturedGraph, naggr=nothing, eaggr=nothing, vaggr=nothing) naggr ,  eaggr  and  vaggr  are arguments for  aggregate_neighbors ,  aggregate_edges  and  aggregate_vertices , respectively. Available aggregate functions are assigned by following symbols to them:  :add ,  :sub ,  :mul ,  :div ,  :max ,  :min  and  :mean ."},{"id":26,"pagetitle":"Message passing scheme","title":"Message passing scheme","ref":"/geometricflux/stable/abstractions/msgpass/#Message-passing-scheme","content":" Message passing scheme Message passing scheme is a popular GNN scheme in many frameworks. It adapts the property of connectivity of neighbors and form a general approach for spatial graph convolutional neural network. It comprises two user-defined functions and one aggregate function. A message function is defined to process information from edge states and node states from neighbors and itself. Messages from each node are obtained and aggregated by aggregate function to provide node-level information for update function. Update function takes current node state and aggregated message and gives a new node state. \\[\\begin{aligned}\n    m_{ij}^{(l+1)} &= message(h_i^{(l)}, h_j^{(l)}, e_{ij}^{(l)}) \\\\\n    m_{i}^{(l+1)} &= \\Box_{j \\in \\mathcal{N}(i)} m_{ij}^{(l+1)} \\\\\n    h_i^{(l+1)} &= update(h_i^{(l)}, m_{i}^{(l+1)})\n\\end{aligned}\\] where  $h_i$  and  $h_j$  are node features from node  $i$  and its neighbor node  $j$ ,  $e_{ij}$  is edge feature for edge  $(i, j)$ , and  $u$  is global feature for whole graph.  $m_{ij}^{(l+1)}$  denotes messages for  $(i, j)$  in  $l$ -th layer.  $message$  and  $update$  are message functions and update function, respectively. Aggregate function  $\\Box$  can be any supported aggregate functions, e.g.  max ,  sum  or  mean . Reference:  Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, George E. Dahl (2017) Message passing scheme is realized into a abstract type  MessagePassing . Any subtype of  MessagePassing  is a message passing layer which utilize default message and update functions: message(mp, x_i, x_j, e_ij) = x_j\nupdate(mp, m, x) = m mp  denotes a message passing layer.  message  accepts node state  x_i  for node  i  and its neighbor state  x_j  for node  j , as well as corresponding edge state  e_ij  for edge  (i,j) . The default message function gives all the neighbor state  x_j  for neighbor of node  i .  update  takes aggregated message  m  and current node state  x , and then outputs  m ."},{"id":27,"pagetitle":"Message passing scheme","title":"GeometricFlux.MessagePassing","ref":"/geometricflux/stable/abstractions/msgpass/#GeometricFlux.MessagePassing","content":" GeometricFlux.MessagePassing  —  Type MessagePassing An abstract type for message-passing scheme. See also  message  and  update . source"},{"id":28,"pagetitle":"Message passing scheme","title":"Message function","ref":"/geometricflux/stable/abstractions/msgpass/#Message-function","content":" Message function A message function accepts feature vector representing node state  x_i , feature vectors for neighbor state  x_j  and corresponding edge state  e_ij . A vector is expected to output from  message  for message. User can override  message  for customized message passing layer to provide desired behavior."},{"id":29,"pagetitle":"Message passing scheme","title":"GeometricFlux.message","ref":"/geometricflux/stable/abstractions/msgpass/#GeometricFlux.message","content":" GeometricFlux.message  —  Function message(mp::MessagePassing, x_i, x_j, e_ij) Message function for the message-passing scheme, returning the message from node  j  to node  i  . In the message-passing scheme. the incoming messages  from the neighborhood of  i  will later be aggregated in order to  update  the features of node  i . By default, the function returns  x_j . Layers subtyping  MessagePassing  should  specialize this method with custom behavior. Arguments mp : message-passing layer. x_i : the features of node  i . x_j : the features of the nighbor  j  of node  i . e_ij : the features of edge ( i ,  j ). See also  update . source"},{"id":30,"pagetitle":"Message passing scheme","title":"Aggregate messages","ref":"/geometricflux/stable/abstractions/msgpass/#Aggregate-messages","content":" Aggregate messages Messages from message function are aggregated by an aggregate function. An aggregated message is passed to update function for node-level computation. An aggregate function is given by the following: propagate(mp, fg::FeaturedGraph, aggr::Symbol=:add) propagate  function calls the whole message passing layer.  fg  acts as an input for message passing layer and  aggr  represents assignment of aggregate function to  propagate  function.  :add  represents an aggregate function of addition of all messages. The following  aggr  are available aggregate functions: :add : sum over all messages  :sub : negative of sum over all messages  :mul : multiplication over all messages  :div : inverse of multiplication over all messages  :max : the maximum of all messages  :min : the minimum of all messages  :mean : the average of all messages"},{"id":31,"pagetitle":"Message passing scheme","title":"Update function","ref":"/geometricflux/stable/abstractions/msgpass/#Update-function","content":" Update function An update function takes aggregated message  m  and current node state  x  as arguments. An output vector is expected to be the new node state for next layer. User can override  update  for customized message passing layer to provide desired behavior."},{"id":32,"pagetitle":"Message passing scheme","title":"GeometricFlux.update","ref":"/geometricflux/stable/abstractions/msgpass/#GeometricFlux.update","content":" GeometricFlux.update  —  Function update(mp::MessagePassing, m, x) Update function for the message-passing scheme, returning a new set of node features  x′  based on old  features  x  and the incoming message from the neighborhood aggregation  m . By default, the function returns  m . Layers subtyping  MessagePassing  should  specialize this method with custom behavior. Arguments mp : message-passing layer. m : the aggregated edge messages from the  message  function. x : the node features to be updated. See also  message . source"},{"id":35,"pagetitle":"Batch Learning","title":"Batch Learning","ref":"/geometricflux/stable/basics/batch/#Batch-Learning","content":" Batch Learning"},{"id":36,"pagetitle":"Batch Learning","title":"Mini-batch Learning for  FeaturedGraph","ref":"/geometricflux/stable/basics/batch/#Mini-batch-Learning-for-[FeaturedGraph](@ref)","content":" Mini-batch Learning for  FeaturedGraph FeaturedGraph supports DataLoader. Batch learning for  FeaturedGraph  can be prepared as follows: train_data = (FeaturedGraph(g, nf=train_X), train_y)\ntrain_batch = DataLoader(train_data, batchsize=batch_size, shuffle=true) FeaturedGraph  now supports  DataLoader  and one can specify mini-batch to it. A mini-batch is passed to a GNN model and trained/inferred in one  FeaturedGraph ."},{"id":37,"pagetitle":"Batch Learning","title":"Mini-batch Learning for array","ref":"/geometricflux/stable/basics/batch/#Mini-batch-Learning-for-array","content":" Mini-batch Learning for array Mini-batch learning on CUDA. Mini-batch learning for array can be prepared as follows: train_loader = DataLoader((train_X, train_y), batchsize=batch_size, shuffle=true) An array could be fed to a GNN model. In the example, the mini-batch dimension is the last dimension for  train_X  array. The  train_X  array is split by  DataLoader  into mini-batches and feed a mini-batch to GNN model at a time. This strategy leverages the advantage of GPU training by accelerating training GNN model in a real batch learning."},{"id":40,"pagetitle":"Graph Convolutions","title":"Graph Convolutions","ref":"/geometricflux/stable/basics/conv/#Graph-Convolutions","content":" Graph Convolutions Graph convolution can be classified into  spectral-based graph convolution  and  spatial-based graph convolution . Spectral-based graph convolution, such as  GCNConv  and  ChebConv , performs operation on features of  whole  graph at one time. Spatial-based graph convolution, such as  GraphConv  and  GATConv , performs operation on features of  local  subgraph instead. Message-passing scheme is an abstraction for spatial-based graph convolutional layers. Any spatial-based graph convolutional layer can be implemented under the framework of message-passing scheme."},{"id":43,"pagetitle":"Building Layers","title":"Building Graph Neural Networks","ref":"/geometricflux/stable/basics/layers/#Building-Graph-Neural-Networks","content":" Building Graph Neural Networks Building GNN is as simple as building neural network in Flux. The syntax here is the same as Flux.  Chain  is used to stack layers into a GNN. A simple example is shown here: model = Chain(\n    GCNConv(feat=>h1),\n    GCNConv(h1=>h2, relu),\n) In the example above, the feature dimension in first layer is mapped from  feat  to  h1 . In second layer,  h1  is then mapped to  h2 . Default activation function is given as  identity  if it is not specified by users. The initialization function  GCNConv(...)  constructs a  GCNConv  layer. For most of the layer types in GeometricFlux, a layer can be initialized in two ways: GNN layer without graph: initializing  without  a predefined graph topology. This allows the layer to accept different graph topology. GNN layer with static graph: initializing  with  a predefined graph topology, e.g. graph wrapped in  FeaturedGraph . This strategy is suitable for datasets where each input requires the same graph structure and it has better performance than variable graph strategy. The example above demonstrate the variable graph strategy. The equivalent GNN architecture but with static graph strategy is shown as following: model = Chain(\n    WithGraph(fg, GCNConv(feat=>h1)),\n    WithGraph(fg, GCNConv(h1=>h2, relu)),\n)"},{"id":44,"pagetitle":"Building Layers","title":"GeometricFlux.WithGraph","ref":"/geometricflux/stable/basics/layers/#GeometricFlux.WithGraph","content":" GeometricFlux.WithGraph  —  Type WithGraph([g], layer; dynamic=nothing) Train GNN layers with static graph. Arguments g : If a  FeaturedGraph  is given, a fixed graph is used to train with. layer : A GNN layer. dynamic : If a function is given, it enables dynamic graph update by constructing dynamic graph through given function within layers. Example julia> using GraphSignals, GeometricFlux\n\njulia> adj = [0 1 0 1;\n              1 0 1 0;\n              0 1 0 1;\n              1 0 1 0];\n\njulia> fg = FeaturedGraph(adj);\n\njulia> gc = WithGraph(fg, GCNConv(1024=>256))  # graph preprocessed by adding self loops\nWithGraph(Graph(#V=4, #E=8), GCNConv(1024 => 256))\n\njulia> WithGraph(fg, Dense(10, 5))\nDense(10 => 5)      # 55 parameters\n\njulia> model = Chain(\n           GCNConv(32=>32),\n           gc,\n       );\n\njulia> WithGraph(fg, model)\nChain(\n  WithGraph(\n    GCNConv(32 => 32),                  # 1_056 parameters\n  ),\n  WithGraph(\n    GCNConv(1024 => 256),               # 262_400 parameters\n  ),\n)         # Total: 4 trainable arrays, 263_456 parameters,\n          # plus 2 non-trainable, 32 parameters, summarysize 1.006 MiB. source"},{"id":45,"pagetitle":"Building Layers","title":"Applying Layers","ref":"/geometricflux/stable/basics/layers/#Applying-Layers","content":" Applying Layers When using GNN layers, the general guidelines are: With static graph strategy: you should pass in a  $d \\times n \\times batch$  matrix for node features, and the layer maps node features  $\\mathbb{R}^d \\rightarrow \\mathbb{R}^k$  then the output will be in matrix with dimensions  $k \\times n \\times batch$ . The same ostensibly goes for edge features but as of now no layer type supports outputting new edge features. With variable graph strategy: you should pass in a  FeaturedGraph , the output will be also be a  FeaturedGraph  with modified node (and/or edge) features. Add  node_feature  as the following entry in the Flux chain (or simply call  node_feature()  on the output) if you wish to subsequently convert them to matrix form."},{"id":46,"pagetitle":"Building Layers","title":"Define Your Own GNN Layer","ref":"/geometricflux/stable/basics/layers/#Define-Your-Own-GNN-Layer","content":" Define Your Own GNN Layer Customizing your own GNN layers are the same as defining a layer in Flux. You may want to check  Flux documentation  first. To define a customized GNN layer, for example, we take a simple  GCNConv  layer as example here. struct GCNConv <: AbstractGraphLayer\n    weight\n    bias\n    σ\nend\n\n@functor GCNConv We first should define a  GCNConv  type and let it be the subtype of  AbstractGraphLayer . In this type, it holds parameters that a layer operate on. Don't forget to add  @functor  macro to  GCNConv  type. (l::GCNConv)(Ã::AbstractMatrix, x::AbstractMatrix) = l.σ.(l.weight * x * Ã .+ l.bias) Then, we can define the operation for  GCNConv  layer. function (l::GCNConv)(fg::AbstractFeaturedGraph)\n    nf = node_feature(fg)\n    Ã = Zygote.ignore() do\n        GraphSignals.normalized_adjacency_matrix(fg, eltype(nf); selfloop=true)\n    end\n    return ConcreteFeaturedGraph(fg, nf = l(Ã, nf))\nend Here comes to the GNN-specific behaviors. A GNN layer should accept object of subtype of  AbstractFeaturedGraph  to support variable graph strategy. A variable graph strategy should fetch node/edge/global features from  fg  and transform graph in  fg  into required form for layer operation, e.g.  GCNConv  layer needs a normalized adjacency matrix with self loop. Then, normalized adjacency matrix  Ã  and node features  nf  are pass through  GCNConv  layer  l(Ã, nf)  to give a new node feature. Finally, a  ConcreteFeaturedGraph  wrap graph in  fg  and new node features into a new object of subtype of  AbstractFeaturedGraph . layer = GCNConv(10=>5, relu)\nnew_fg = layer(fg)\ngradient(() -> sum(node_feature(layer(fg))), Flux.params(layer)) Now we complete a simple version of  GCNConv  layer. One can test the forward pass and gradient if they work properly."},{"id":47,"pagetitle":"Building Layers","title":"GeometricFlux.AbstractGraphLayer","ref":"/geometricflux/stable/basics/layers/#GeometricFlux.AbstractGraphLayer","content":" GeometricFlux.AbstractGraphLayer  —  Type AbstractGraphLayer An abstract type of graph neural network layer for GeometricFlux. source"},{"id":50,"pagetitle":"Neighborhood graphs","title":"Neighborhood Graphs","ref":"/geometricflux/stable/basics/neighborhood_graph/#Neighborhood-Graphs","content":" Neighborhood Graphs In machine learning, it is often that using a neighborhood graph to approach manifold in high dimensional space. The construction of neighborhood graph is the essential step for machine learning algorithms on graph/manifold, especially manifold learning. The k-nearest neighbor (kNN) method is the most frequent use to construct a neighborhood graph. We provide  kneighbors_graph  to generate a kNN graph from a set of nodes/points. We prepare 1,024 10-dimensional data points. X = rand(Float32, 10, 1024) Then, we can generate a kNN graph with  k=7 , which means a data point should be linked to their top-7 nearest neighbor points. fg = kneighbors_graph(nf, 7) The default distance metric would be  Euclidean  distance from Distance.jl package. If one wants to customize  kneighbors_graph  by using different distance metric, you can just use the distance objects from Distance.jl package directly, and pass it to  kneighbors_graph . using Distances\n\nfg = kneighbors_graph(nf, 7, Cityblock())"},{"id":53,"pagetitle":"Graph Passing","title":"Graph Passing Strategy","ref":"/geometricflux/stable/basics/passgraph/#Graph-Passing-Strategy","content":" Graph Passing Strategy Graph is an input data structure for graph neural network. Passing a graph into a GNN layer can have different behaviors. If the graph remains fixed across samples, that is, all samples utilize the same graph structure, a static graph is used. Or, graphs can be carried within  FeaturedGraph  to provide variable graphs to GNN layer. Users have the flexibility to pick an adequate approach for their own needs."},{"id":54,"pagetitle":"Graph Passing","title":"Variable Graph Strategy","ref":"/geometricflux/stable/basics/passgraph/#Variable-Graph-Strategy","content":" Variable Graph Strategy Variable graphs are supported through  FeaturedGraph , which contains both the graph information and the features. Each  FeaturedGraph  can contain a distinct graph structure and its features. Data of  FeaturedGraph  are fed directly to graph convolutional layer or graph neural network to let each feature be learned on different graph structures. A adjacency matrix  adj_mat  is given to construct a  FeaturedGraph  as follows: fg = FeaturedGraph(adj_mat, features)\nlayer = GCNConv(feat=>h1, relu) Simple(Di)Graph ,  SimpleWeighted(Di)Graph  or  Meta(Di)Graph  provided by the packages Graphs, SimpleWeightedGraphs and MetaGraphs, respectively, are acceptable for constructing a  FeaturedGraph . An adjacency list is also accepted, too."},{"id":55,"pagetitle":"Graph Passing","title":"FeaturedGraph  in,  FeaturedGraph  out","ref":"/geometricflux/stable/basics/passgraph/#[FeaturedGraph](@ref)-in,-[FeaturedGraph](@ref)-out","content":" FeaturedGraph  in,  FeaturedGraph  out Since a variable graph is provided from data, a  FeaturedGraph  object or a set of  FeaturedGraph  objects should be fed in a GNN model. The  FeaturedGraph  object should contain a graph and sufficient features that a GNN model needed. After operations, a  FeaturedGraph  object is given as output. fg = FeaturedGraph(g, nf=X)\ngc = GCNConv(in_channel=>out_channel)\nnew_fg = gc(fg)"},{"id":56,"pagetitle":"Graph Passing","title":"Static Graph Strategy","ref":"/geometricflux/stable/basics/passgraph/#Static-Graph-Strategy","content":" Static Graph Strategy A static graph is used to reduce redundant computation during passing through layers. A static graph can be set in graph convolutional layers such that this graph is shared for computations across those layers. An adjacency matrix  adj_mat  is given to represent a graph and is provided to a graph convolutional layer as follows: fg = FeaturedGraph(adj_mat)\nlayer = WithGraph(fg, GCNConv(feat=>h1, relu)) Simple(Di)Graph ,  SimpleWeighted(Di)Graph  or  Meta(Di)Graph  provided by the packages Graphs, SimpleWeightedGraphs and MetaGraphs, respectively, are valid arguments for passing as a static graph to this layer. An adjacency list is also accepted in the type of  Vector{Vector}  is also accepted."},{"id":57,"pagetitle":"Graph Passing","title":"Cached Graph in Layers","ref":"/geometricflux/stable/basics/passgraph/#Cached-Graph-in-Layers","content":" Cached Graph in Layers While a variable graph is given by  FeaturedGraph , a GNN layer doesn't need a static graph anymore. A cache mechanism is designed to cache static graph to reduce computation time. A cached graph is retrieved from  WithGraph  layer and operation is then performed. For each time, it will assign current computed graph back to layer."},{"id":58,"pagetitle":"Graph Passing","title":"Array in, Array out","ref":"/geometricflux/stable/basics/passgraph/#Array-in,-Array-out","content":" Array in, Array out Since a static graph is provided from  WithGraph  layer, it doesn't accept a  FeaturedGraph  object anymore. Instead, it accepts a regular array as input, and outputs an array back. fg = FeaturedGraph(g)\nlayer = WithGraph(fg, GCNConv(in_channel=>out_channel))\nH = layer(X)"},{"id":59,"pagetitle":"Graph Passing","title":"What you feed is what you get","ref":"/geometricflux/stable/basics/passgraph/#What-you-feed-is-what-you-get","content":" What you feed is what you get In GeometricFlux, there are are two APIs which allow different input/output types for GNN layers. For example,  GCNConv  layer provides the following two APIs: (g::WithGraph{<:GCNConv})(X::AbstractArray) -> AbstractArray\n(g::GCNConv)(fg::FeaturedGraph) -> FeaturedGraph If your feed a  GCNConv  layer with a  Array , it will return you a  Array . If you feed a  GCNConv  layer with a  FeaturedGraph , it will return you a  FeaturedGraph .  These APIs ensure the consistency between input and output types."},{"id":62,"pagetitle":"Random graphs","title":"Random Graphs","ref":"/geometricflux/stable/basics/random_graph/#Random-Graphs","content":" Random Graphs"},{"id":63,"pagetitle":"Random graphs","title":"Random Graph Generation","ref":"/geometricflux/stable/basics/random_graph/#Random-Graph-Generation","content":" Random Graph Generation A graph is needed as input for a GNN model. Random graph can be generated, which is provided by Graphs.jl package. A random graph can be generated by  erdos_renyi  model. julia> using Graphs\n\njulia> g = erdos_renyi(10, 30)\n{10, 30} undirected simple Int64 graph To construct a  FeaturedGraph  object, just put the graph object and its corresponding features into it. julia> X = rand(Float32, 5, 10);\n\njulia> fg = FeaturedGraph(g, nf=X)\nFeaturedGraph:\n\tUndirected graph with (#V=10, #E=30) in adjacency matrix\n\tNode feature:\tℝ^5 <Matrix{Float32}> Various random graph with different generating model can be used here. julia> barabasi_albert(10, 3)\n{10, 21} undirected simple Int64 graph\n\njulia> watts_strogatz(10, 4, 0.3)\n{10, 20} undirected simple Int64 graph barabasi_albert  generates graphs from scale-free network model, while  watts_strogatz  generates graphs from small-world model."},{"id":64,"pagetitle":"Random graphs","title":"Common Graphs","ref":"/geometricflux/stable/basics/random_graph/#Common-Graphs","content":" Common Graphs There are commonly used graphs listed here. clique_graph(k, n)\ncomplete_graph(n)\ngrid(dims; periodic=false)\npath_digraph(n)\npath_graph(n)"},{"id":67,"pagetitle":"Subgraph","title":"Subgraph","ref":"/geometricflux/stable/basics/subgraph/#Subgraph","content":" Subgraph"},{"id":68,"pagetitle":"Subgraph","title":"Subgraph of  FeaturedGraph","ref":"/geometricflux/stable/basics/subgraph/#Subgraph-of-[FeaturedGraph](@ref)","content":" Subgraph of  FeaturedGraph A  FeaturedGraph  object can derive a subgraph from a selected subset of the vertices of the graph. train_idx = train_indices(Planetoid(), :cora)\nfg = FeaturedGraph(g)\nfsg = subgraph(fg, train_idx) A  FeaturedSubgraph  object is returned from  subgraph  by selected vertices  train_idx ."},{"id":71,"pagetitle":"Cooperate with Flux Layers","title":"Cooperate with Flux Layers","ref":"/geometricflux/stable/cooperate/#Cooperate-with-Flux-Layers","content":" Cooperate with Flux Layers GeometricFlux is designed to be compatible with Flux layers. Flux layers usually have array input and array output. Since the mechanism of \"what you feed is what you get\", the API for array type is compatible directly with other Flux layers. However, the API for  FeaturedGraph  is not compatible directly."},{"id":72,"pagetitle":"Cooperate with Flux Layers","title":"Fetching Features from  FeaturedGraph  and Output Compatible Result with Flux Layers","ref":"/geometricflux/stable/cooperate/#Fetching-Features-from-[FeaturedGraph](@ref)-and-Output-Compatible-Result-with-Flux-Layers","content":" Fetching Features from  FeaturedGraph  and Output Compatible Result with Flux Layers With a layer outputs a  FeaturedGraph , it is not compatible with Flux layers. Since Flux layers need single feature in array form as input, node features, edge features and global features can be selected by using  FeaturedGraph  APIs:  node_feature ,  edge_feature  or  global_feature , respectively. model = Chain(\n    GCNConv(1024=>256, relu),\n    node_feature,  # or edge_feature or global_feature\n    softmax\n) In a multitask learning scenario, multiple outputs are required. A branching selection of features can be made as follows: model = Chain(\n    GCNConv(1024=>256, relu),\n    x -> (node_feature(x), global_feature(x)),\n    (nf, gf) -> (softmax(nf), identity.(gf))\n)"},{"id":73,"pagetitle":"Cooperate with Flux Layers","title":"Branching Different Features Through Different Layers","ref":"/geometricflux/stable/cooperate/#Branching-Different-Features-Through-Different-Layers","content":" Branching Different Features Through Different Layers GraphParallel wraps regular Flux layers for different kinds of features for integration to GNN layers. A  GraphParallel  construct is designed for passing each feature through different layers from a  FeaturedGraph . An example is given as follow: Flux.Chain(\n    ...\n    GraphParallel(\n        node_layer=Dropout(0.5),\n        edge_layer=Dense(1024, 256, relu),\n        global_layer=identity,\n    ),\n    ...\n) GraphParallel  will pass node feature to a  Dropout  layer and edge feature to a  Dense  layer. Meanwhile, a  FeaturedGraph  is decomposed and keep the graph in  FeaturedGraph  to the downstream layers. A new  FeaturedGraph  is constructed with processed node feature, edge feature and global feature.  GraphParallel  acts as a layer which accepts a  FeaturedGraph  and output a  FeaturedGraph . Thus, it by pass the graph in a  FeaturedGraph  but pass different features to different layers."},{"id":74,"pagetitle":"Cooperate with Flux Layers","title":"GeometricFlux.GraphParallel","ref":"/geometricflux/stable/cooperate/#GeometricFlux.GraphParallel","content":" GeometricFlux.GraphParallel  —  Type GraphParallel(; node_layer=identity, edge_layer=identity, global_layer=identity,\n                positional_layer=identity) Passing features in  FeaturedGraph  in parallel. It takes  FeaturedGraph  as input and it can be specified by assigning layers for specific (node, edge and global) features. Arguments node_layer : A regular Flux layer for passing node features. edge_layer : A regular Flux layer for passing edge features. global_layer : A regular Flux layer for passing global features. positional_layer : A regular Flux layer for passing positional features. Example julia> using Flux, GeometricFlux\n\njulia> l = GraphParallel(\n            node_layer=Dropout(0.5),\n            global_layer=Dense(10, 5)\n       )\nGraphParallel(node_layer=Dropout(0.5), edge_layer=identity, global_layer=Dense(10 => 5), positional_layer=identity) source"},{"id":77,"pagetitle":"Dynamic Graph Update","title":"Dynamic Graph Update","ref":"/geometricflux/stable/dynamicgraph/#Dynamic-Graph-Update","content":" Dynamic Graph Update Dynamic graph update is a technique to generate a new graph within a graph convolutional layer proposed by  Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein, Justin M. Solomon (2019) . Most of manifold learning approaches aims to learn capture manifold structures in high dimensional space. They construct a graph to approximate the manifold and learn to reduce dimensions of space. The separation of capturing manifold and learning dimensional reduction limits the power of manifold learning. Thus,  latent graph learning  is proposed to learn the manifold and dimensional reduction simultaneously. The latent graph learning is also named as manifold learning 2.0 which leverages the power of graph neural network and learns latent graph structure within layers of a graph neural network. Latent graph learning learns the latent graph through training over point cloud, or a set of features. A fixed graph structure is not provided to a GNN model. Latent graph is dynamically constructed by constructing a neighborhood graph using features in graph convolutional layers. After construction of neighborhood graph, the neighborhood graph is fed as input with features into a graph convolutional layer. Currently, we support k-nearest neighbor method to construct a neighborhood graph. To use dynamic graph update, just replace the static graph strategy WithGraph(fg, EdgeConv(Dense(2*in_channel, out_channel))) as graph construction method. WithGraph(\n    EdgeConv(Dense(2*in_channel, out_channel)),\n    dynamic=X -> GraphSignals.kneighbors_graph(X, 3)\n)"},{"id":80,"pagetitle":"Introduction","title":"Introduction to Graph Neural Networks (GNN)","ref":"/geometricflux/stable/introduction/#Introduction-to-Graph-Neural-Networks-(GNN)","content":" Introduction to Graph Neural Networks (GNN) Graph neural networks act as standalone network architecture other than convolutional neural networks (CNN), recurrent neural networks (RNN). As its name implies, GNN needs a graph as training data. The problem setting requires at least a graph to train on."},{"id":81,"pagetitle":"Introduction","title":"What is Graph Neural Networks?","ref":"/geometricflux/stable/introduction/#What-is-Graph-Neural-Networks?","content":" What is Graph Neural Networks? Graph convolutional layers is the building block for GNN, and it extends from classic convolutional layer. Convolutional layer performs convolution operation over regular grid geometry, e.g. pixels in images arrange regularly along vertical and horizontal directions, while graph convolutional performs convolutional operation over irregular graph topology, e.g. graph is composed of a set of  nodes  which connect to each other with  edges . In signal processing field, images are viewed as a kind of signals. Precisely, image can be represented as a function which maps from coordinates to color for each pixel. A matrix satisfies the definition and it maps image indices to a RGB value representing each pixel. Analogically, a graph signal can be defined as a function which maps from node/edge in a graph to certain value or features. We call them node features or edge features if the features correspond to node or edge, respectively. Graph convolutional layer maps features on nodes or edges to their embeddings. Geometry for images and graphs."},{"id":82,"pagetitle":"Introduction","title":"What is the Difference between Deep Learning and GNN?","ref":"/geometricflux/stable/introduction/#What-is-the-Difference-between-Deep-Learning-and-GNN?","content":" What is the Difference between Deep Learning and GNN? Practically, GNN requires graph to be input in a certain form and features will be mapped according to the input graph, while classic deep learning architecture doesn't require a graph or a geometric object as input. In the design of GeometricFlux, the input graph can be two kinds: static graph or variable graph. A static graph is carried within a GNN layer, while a variable graph can be carried with features. The concept of a static graph defines the graph topology in the GNN layer and view it as a built-in static topology for a layer. The concept of variable graph is, totally different from static graph, to consider graph as a part of input data, which is more nature to most of people."},{"id":83,"pagetitle":"Introduction","title":"Features for GNNs","ref":"/geometricflux/stable/introduction/#Features-for-GNNs","content":" Features for GNNs Graph signals include node signals, edge signals and global (or graph) signals. According to the problem setting, signals are further classified as features or labels. Features that can be used in GNN includes node features, edge features and global (or graph) features. Global (or graph) features are features that corresponds the whole graph and represents the status of a graph. Signals and graph signals."},{"id":84,"pagetitle":"Introduction","title":"Variable graph:  FeaturedGraph  as Container for Graph and Features","ref":"/geometricflux/stable/introduction/#Variable-graph:-[FeaturedGraph](@ref)-as-Container-for-Graph-and-Features","content":" Variable graph:  FeaturedGraph  as Container for Graph and Features A GNN model accepts a graph and features as input. To this end,  FeaturedGraph  object is designed as a container for graph and various kinds of features. It can be passed to a GNN model directly. T = Float32\nfg = FeaturedGraph(g, nf=rand(10, 5), ef=rand(7, 11), gf=) It is worth noting that it is better to convert element type of graph to  Float32  explicitly. It can avoid some issues when training or inferring a GNN model. train_data = [(FeaturedGraph(g, nf=train_X), train_y) for _ in 1:N] A set of  FeaturedGraph  can include different graph structures  g  and different features  train_X  and then pass into the same GNN model in order to train/infer on variable graphs."},{"id":85,"pagetitle":"Introduction","title":"Build GNN Model","ref":"/geometricflux/stable/introduction/#Build-GNN-Model","content":" Build GNN Model model = Chain(\n    GCNConv(input_dim=>hidden_dim, relu),\n    GraphParallel(node_layer=Dropout(0.5)),\n    GCNConv(hidden_dim=>target_dim),\n    node_feature,\n) A GNN model can be built by stacking GNN layers with or without regular Flux layers. Regular Flux layers should be wrapped in  GraphParallel  and specified as  node_layer  which is applied to node features."},{"id":88,"pagetitle":"Embeddings","title":"Embeddings","ref":"/geometricflux/stable/manual/embedding/#Embeddings","content":" Embeddings"},{"id":89,"pagetitle":"Embeddings","title":"Node2vec","ref":"/geometricflux/stable/manual/embedding/#Node2vec","content":" Node2vec"},{"id":90,"pagetitle":"Embeddings","title":"GeometricFlux.node2vec","ref":"/geometricflux/stable/manual/embedding/#GeometricFlux.node2vec","content":" GeometricFlux.node2vec  —  Function node2vec(g; walks_per_node, len, p, q, dims) Returns an embedding matrix with size of ( nv(g) ,  dims ). It computes node embeddings on graph  g . It performs biased random walks on the graph, then computes word embeddings by treating those random walks as sentences. Arguments g::FeaturedGraph : The graph to perform random walk on. walks_per_node::Int : Number of walks starting on each node, total number of walks is  nv(g) * walks_per_node len::Int : Length of random walks p::Real : Return parameter. It controls the likelihood of immediately revisiting a node in the walk q::Real : In-out parameter. It allows the search to differentiate between inward and outward nodes. dims::Int : Number of vector dimensions source Reference:  Aditya Grover, Jure Leskovec (2016)"},{"id":93,"pagetitle":"FeaturedGraph","title":"FeaturedGraph","ref":"/geometricflux/stable/manual/featuredgraph/#FeaturedGraph","content":" FeaturedGraph"},{"id":94,"pagetitle":"FeaturedGraph","title":"GraphSignals.FeaturedGraph","ref":"/geometricflux/stable/manual/featuredgraph/#GraphSignals.FeaturedGraph","content":" GraphSignals.FeaturedGraph  —  Type FeaturedGraph(g, [mt]; directed=:auto, nf, ef, gf, pf=nothing,\n    T, N, E, with_batch=false) A type representing a graph structure and storing also arrays  that contain features associated to nodes, edges, and the whole graph.  A  FeaturedGraph  can be constructed out of different objects  g  representing the connections inside the graph. When constructed from another featured graph  fg , the internal graph representation is preserved and shared. Arguments g : Data representing the graph topology. Possible type are  An adjacency matrix. An adjacency list. A Graphs' graph, i.e.  SimpleGraph ,  SimpleDiGraph  from Graphs, or  SimpleWeightedGraph ,    SimpleWeightedDiGraph  from SimpleWeightedGraphs. An  AbstractFeaturedGraph  object. mt::Symbol : Matrix type for  g  in matrix form. if  graph  is in matrix form,  mt  is   recorded as one of  :adjm ,  :normedadjm ,  :laplacian ,  :normalized  or  :scaled . directed : It specify that direction of a graph. It can be  :auto ,  :directed  and    :undirected . Default value is  :auto , which infers direction automatically. nf : Node features. ef : Edge features. gf : Global features. pf : Positional features. If  nothing  is given, positional encoding is turned off. If an   array is given, positional encoding is assigned as given array. If  :auto  is given,   positional encoding is generated automatically for node features and  with_batch  is considered. T : It specifies the element type of graph. Default value is the element type of  g . N : Number of nodes for  g . E : Number of edges for  g . with_batch::Bool : Consider last dimension of all features as batch dimension. Usage using GraphSignals, CUDA\n\n# Construct from adjacency list representation\ng = [[2,3], [1,4,5], [1], [2,5], [2,4]]\nfg = FeaturedGraph(g)\n\n# Number of nodes and edges\nnv(fg)  # 5\nne(fg)  # 10\n\n# From a Graphs' graph\nfg = FeaturedGraph(erdos_renyi(100, 20))\n\n# Copy featured graph while also adding node features\nfg = FeaturedGraph(fg, nf=rand(100, 5))\n\n# Send to gpu\nfg = fg |> cu See also  graph ,  node_feature ,  edge_feature , and  global_feature ."},{"id":95,"pagetitle":"FeaturedGraph","title":"GraphSignals.graph","ref":"/geometricflux/stable/manual/featuredgraph/#GraphSignals.graph","content":" GraphSignals.graph  —  Function graph(fg) Get referenced graph in  fg . Arguments fg::AbstractFeaturedGraph : A concrete object of  AbstractFeaturedGraph  type."},{"id":96,"pagetitle":"FeaturedGraph","title":"GraphSignals.node_feature","ref":"/geometricflux/stable/manual/featuredgraph/#GraphSignals.node_feature","content":" GraphSignals.node_feature  —  Function node_feature(fg) Get node feature attached to  fg . Arguments fg::AbstractFeaturedGraph : A concrete object of  AbstractFeaturedGraph  type."},{"id":97,"pagetitle":"FeaturedGraph","title":"GraphSignals.has_node_feature","ref":"/geometricflux/stable/manual/featuredgraph/#GraphSignals.has_node_feature","content":" GraphSignals.has_node_feature  —  Function has_node_feature(::AbstractFeaturedGraph) Check if  node_feature  is available or not for  fg . Arguments fg::AbstractFeaturedGraph : A concrete object of  AbstractFeaturedGraph  type."},{"id":98,"pagetitle":"FeaturedGraph","title":"GraphSignals.edge_feature","ref":"/geometricflux/stable/manual/featuredgraph/#GraphSignals.edge_feature","content":" GraphSignals.edge_feature  —  Function edge_feature(fg) Get edge feature attached to  fg . Arguments fg::AbstractFeaturedGraph : A concrete object of  AbstractFeaturedGraph  type."},{"id":99,"pagetitle":"FeaturedGraph","title":"GraphSignals.has_edge_feature","ref":"/geometricflux/stable/manual/featuredgraph/#GraphSignals.has_edge_feature","content":" GraphSignals.has_edge_feature  —  Function has_edge_feature(::AbstractFeaturedGraph) Check if  edge_feature  is available or not for  fg . Arguments fg::AbstractFeaturedGraph : A concrete object of  AbstractFeaturedGraph  type."},{"id":100,"pagetitle":"FeaturedGraph","title":"GraphSignals.global_feature","ref":"/geometricflux/stable/manual/featuredgraph/#GraphSignals.global_feature","content":" GraphSignals.global_feature  —  Function global_feature(fg) Get global feature attached to  fg . Arguments fg::AbstractFeaturedGraph : A concrete object of  AbstractFeaturedGraph  type."},{"id":101,"pagetitle":"FeaturedGraph","title":"GraphSignals.has_global_feature","ref":"/geometricflux/stable/manual/featuredgraph/#GraphSignals.has_global_feature","content":" GraphSignals.has_global_feature  —  Function has_global_feature(::AbstractFeaturedGraph) Check if  global_feature  is available or not for  fg . Arguments fg::AbstractFeaturedGraph : A concrete object of  AbstractFeaturedGraph  type."},{"id":102,"pagetitle":"FeaturedGraph","title":"GraphSignals.positional_feature","ref":"/geometricflux/stable/manual/featuredgraph/#GraphSignals.positional_feature","content":" GraphSignals.positional_feature  —  Function positional_feature(fg) Get positional feature attached to  fg . Arguments fg::AbstractFeaturedGraph : A concrete object of  AbstractFeaturedGraph  type."},{"id":103,"pagetitle":"FeaturedGraph","title":"GraphSignals.has_positional_feature","ref":"/geometricflux/stable/manual/featuredgraph/#GraphSignals.has_positional_feature","content":" GraphSignals.has_positional_feature  —  Function has_positional_feature(::AbstractFeaturedGraph) Check if  positional_feature  is available or not for  fg . Arguments fg::AbstractFeaturedGraph : A concrete object of  AbstractFeaturedGraph  type."},{"id":104,"pagetitle":"FeaturedGraph","title":"GraphSignals.subgraph","ref":"/geometricflux/stable/manual/featuredgraph/#GraphSignals.subgraph","content":" GraphSignals.subgraph  —  Function subgraph(fg, nodes) Returns a subgraph of type  FeaturedSubgraph  from a given featured graph  fg . It constructs a subgraph by reserving  nodes  in a graph. Arguments fg::AbstractFeaturedGraph : A base featured graph to construct a subgraph. nodes::AbstractVector : It specifies nodes to be reserved from  fg ."},{"id":105,"pagetitle":"FeaturedGraph","title":"GraphSignals.ConcreteFeaturedGraph","ref":"/geometricflux/stable/manual/featuredgraph/#GraphSignals.ConcreteFeaturedGraph","content":" GraphSignals.ConcreteFeaturedGraph  —  Function ConcreteFeaturedGraph(fg; nf=node_feature(fg), ef=edge_feature(fg),\n                      gf=global_feature(fg), pf=positional_feature(fg)) This is a syntax sugar for construction for  FeaturedGraph  and  FeaturedSubgraph  object. It is an idempotent operation, which gives the same type of object as inputs. It wraps input  fg  again but reconfigures with  kwargs . Arguments fg :  FeaturedGraph  and  FeaturedSubgraph  object. nf : Node features. ef : Edge features. gf : Global features. pf : Positional features. Usage julia> using GraphSignals\n\njulia> adjm = [0 1 1 1;\n               1 0 1 0;\n               1 1 0 1;\n               1 0 1 0];\n\njulia> nf = rand(10, 4);\n\njulia> fg = FeaturedGraph(adjm; nf=nf)\nFeaturedGraph:\n\tUndirected graph with (#V=4, #E=5) in adjacency matrix\n\tNode feature:\tℝ^10 <Matrix{Float64}>\n\njulia> ConcreteFeaturedGraph(fg, nf=rand(7, 4))\nFeaturedGraph:\n    Undirected graph with (#V=4, #E=5) in adjacency matrix\n    Node feature:\tℝ^7 <Matrix{Float64}>    "},{"id":108,"pagetitle":"Graph Convolutional Layers","title":"Graph Convolutional Layers","ref":"/geometricflux/stable/manual/graph_conv/#Graph-Convolutional-Layers","content":" Graph Convolutional Layers"},{"id":109,"pagetitle":"Graph Convolutional Layers","title":"Graph Convolutional Layer","ref":"/geometricflux/stable/manual/graph_conv/#Graph-Convolutional-Layer","content":" Graph Convolutional Layer \\[X' = \\sigma(\\hat{D}^{-1/2} \\hat{A} \\hat{D}^{-1/2} X \\Theta)\\] where  $\\hat{A} = A + I$ ,  $A$  denotes the adjacency matrix, and  $\\hat{D} = [\\hat{d}_{ij}] = \\sum_{j=0} [\\hat{a}_{ij}]$  is degree matrix."},{"id":110,"pagetitle":"Graph Convolutional Layers","title":"GeometricFlux.GCNConv","ref":"/geometricflux/stable/manual/graph_conv/#GeometricFlux.GCNConv","content":" GeometricFlux.GCNConv  —  Type GCNConv(in => out, σ=identity; bias=true, init=glorot_uniform) Graph convolutional layer. The input to the layer is a node feature array  X  of size  (num_features, num_nodes) . Arguments in : The dimension of input features. out : The dimension of output features. σ : Activation function. bias : Add learnable bias. init : Weights' initializer. Examples julia> gc = GCNConv(1024=>256, relu)\nGCNConv(1024 => 256, relu) See also  WithGraph  for training layer with static graph. source Reference:  Thomas N. Kipf, Max Welling (2017)"},{"id":111,"pagetitle":"Graph Convolutional Layers","title":"Chebyshev Spectral Graph Convolutional Layer","ref":"/geometricflux/stable/manual/graph_conv/#Chebyshev-Spectral-Graph-Convolutional-Layer","content":" Chebyshev Spectral Graph Convolutional Layer \\[X' = \\sum^{K-1}_{k=0} Z^{(k)} \\Theta^{(k)}\\] where  $Z^{(k)}$  is the  $k$ -th term of Chebyshev polynomials, and can be calculated by the following recursive form: \\[Z^{(0)} = X \\\\\nZ^{(1)} = \\hat{L} X \\\\\nZ^{(k)} = 2 \\hat{L} Z^{(k-1)} - Z^{(k-2)}\\] and  $\\hat{L} = \\frac{2}{\\lambda_{max}} L - I$ ."},{"id":112,"pagetitle":"Graph Convolutional Layers","title":"GeometricFlux.ChebConv","ref":"/geometricflux/stable/manual/graph_conv/#GeometricFlux.ChebConv","content":" GeometricFlux.ChebConv  —  Type ChebConv(in=>out, k; bias=true, init=glorot_uniform) Chebyshev spectral graph convolutional layer. Arguments in : The dimension of input features. out : The dimension of output features. k : The order of Chebyshev polynomial. bias : Add learnable bias. init : Weights' initializer. Examples julia> cc = ChebConv(1024=>256, 5, relu)\nChebConv(1024 => 256, k=5, relu) See also  WithGraph  for training layer with static graph. source Reference:  Michaël Defferrard, Xavier Bresson, Pierre Vandergheynst (2016)"},{"id":113,"pagetitle":"Graph Convolutional Layers","title":"Graph Neural Network Layer","ref":"/geometricflux/stable/manual/graph_conv/#Graph-Neural-Network-Layer","content":" Graph Neural Network Layer \\[\\textbf{x}_i' = \\sigma (\\Theta_1 \\textbf{x}_i + \\sum_{j \\in \\mathcal{N}(i)} \\Theta_2 \\textbf{x}_j)\\]"},{"id":114,"pagetitle":"Graph Convolutional Layers","title":"GeometricFlux.GraphConv","ref":"/geometricflux/stable/manual/graph_conv/#GeometricFlux.GraphConv","content":" GeometricFlux.GraphConv  —  Type GraphConv(in => out, σ=identity, aggr=+; bias=true, init=glorot_uniform) Graph neural network layer. Arguments in : The dimension of input features. out : The dimension of output features. σ : Activation function. aggr : An aggregate function applied to the result of message function.  + ,  - , * ,  / ,  max ,  min  and  mean  are available. bias : Add learnable bias. init : Weights' initializer. Examples julia> GraphConv(1024=>256, relu)\nGraphConv(1024 => 256, relu, aggr=+)\n\njulia> GraphConv(1024=>256, relu, *)\nGraphConv(1024 => 256, relu, aggr=*) See also  WithGraph  for training layer with static graph. source Reference:  Christopher Morris, Martin Ritzert, Matthias Fey, William L. Hamilton, Jan Eric Lenssen, Gaurav Rattan, Martin Grohe (2019)"},{"id":115,"pagetitle":"Graph Convolutional Layers","title":"SAmple and aggreGatE (GraphSAGE) Network","ref":"/geometricflux/stable/manual/graph_conv/#SAmple-and-aggreGatE-(GraphSAGE)-Network","content":" SAmple and aggreGatE (GraphSAGE) Network \\[\\hat{\\textbf{x}}_j = sample(\\textbf{x}_j), \\forall j \\in \\mathcal{N}(i) \\\\\n\\textbf{m}_i = aggregate(\\hat{\\textbf{x}}_j) \\\\\n\\textbf{x}_i' = \\sigma (\\Theta_1 \\textbf{x}_i + \\Theta_2 \\textbf{m}_i)\\]"},{"id":116,"pagetitle":"Graph Convolutional Layers","title":"GeometricFlux.SAGEConv","ref":"/geometricflux/stable/manual/graph_conv/#GeometricFlux.SAGEConv","content":" GeometricFlux.SAGEConv  —  Type SAGEConv(in => out, σ=identity, aggr=mean; normalize=true, project=false,\n         bias=true, num_sample=10, init=glorot_uniform) SAmple and aggreGatE convolutional layer for GraphSAGE network. Arguments in : The dimension of input features. out : The dimension of output features. σ : Activation function. aggr : An aggregate function applied to the result of message function.  mean ,  max , LSTM  and  GCNConv  are available. normalize::Bool : Whether to normalize features across all nodes or not. project::Bool : Whether to project, i.e.  Dense(in, in) , before aggregation. bias : Add learnable bias. num_sample::Int : Number of samples for each node from their neighbors. init : Weights' initializer. Examples julia> SAGEConv(1024=>256, relu)\nSAGEConv(1024 => 256, relu, aggr=mean, normalize=true, #sample=10)\n\njulia> SAGEConv(1024=>256, relu, num_sample=5)\nSAGEConv(1024 => 256, relu, aggr=mean, normalize=true, #sample=5)\n\njulia> MeanAggregator(1024=>256, relu, normalize=false)\nSAGEConv(1024 => 256, relu, aggr=mean, normalize=false, #sample=10)\n\njulia> MeanPoolAggregator(1024=>256, relu)\nSAGEConv(1024 => 256, relu, project=Dense(1024 => 1024), aggr=mean, normalize=true, #sample=10)\n\njulia> MaxPoolAggregator(1024=>256, relu)\nSAGEConv(1024 => 256, relu, project=Dense(1024 => 1024), aggr=max, normalize=true, #sample=10)\n\njulia> LSTMAggregator(1024=>256, relu)\nSAGEConv(1024 => 256, relu, aggr=LSTMCell(1024 => 1024), normalize=true, #sample=10) See also  WithGraph  for training layer with static graph and  MeanAggregator ,  MeanPoolAggregator ,  MaxPoolAggregator  and  LSTMAggregator . source"},{"id":117,"pagetitle":"Graph Convolutional Layers","title":"GeometricFlux.MeanAggregator","ref":"/geometricflux/stable/manual/graph_conv/#GeometricFlux.MeanAggregator","content":" GeometricFlux.MeanAggregator  —  Function MeanAggregator(in => out, σ=identity; normalize=true, project=false,\n               bias=true, num_sample=10, init=glorot_uniform) SAGEConv with mean aggregator. See also  SAGEConv . source"},{"id":118,"pagetitle":"Graph Convolutional Layers","title":"GeometricFlux.MeanPoolAggregator","ref":"/geometricflux/stable/manual/graph_conv/#GeometricFlux.MeanPoolAggregator","content":" GeometricFlux.MeanPoolAggregator  —  Function MeanAggregator(in => out, σ=identity; normalize=true,\n               bias=true, num_sample=10, init=glorot_uniform) SAGEConv with meanpool aggregator. See also  SAGEConv . source"},{"id":119,"pagetitle":"Graph Convolutional Layers","title":"GeometricFlux.MaxPoolAggregator","ref":"/geometricflux/stable/manual/graph_conv/#GeometricFlux.MaxPoolAggregator","content":" GeometricFlux.MaxPoolAggregator  —  Function MeanAggregator(in => out, σ=identity; normalize=true,\n               bias=true, num_sample=10, init=glorot_uniform) SAGEConv with maxpool aggregator. See also  SAGEConv . source"},{"id":120,"pagetitle":"Graph Convolutional Layers","title":"GeometricFlux.LSTMAggregator","ref":"/geometricflux/stable/manual/graph_conv/#GeometricFlux.LSTMAggregator","content":" GeometricFlux.LSTMAggregator  —  Function LSTMAggregator(in => out, σ=identity; normalize=true, project=false,\n               bias=true, num_sample=10, init=glorot_uniform) SAGEConv with LSTM aggregator. See also  SAGEConv . source Reference:  William L Hamilton, Rex Ying, Jure Leskovec (2017)  and  GraphSAGE website"},{"id":121,"pagetitle":"Graph Convolutional Layers","title":"Graph Attentional Layer","ref":"/geometricflux/stable/manual/graph_conv/#Graph-Attentional-Layer","content":" Graph Attentional Layer \\[\\textbf{x}_i' = \\alpha_{i,i} \\Theta \\textbf{x}_i + \\sum_{j \\in \\mathcal{N}(i)} \\alpha_{i,j} \\Theta \\textbf{x}_j\\] where the attention coefficient  $\\alpha_{i,j}$  can be calculated from \\[\\alpha_{i,j} = \\frac{exp(LeakyReLU(\\textbf{a}^T [\\Theta \\textbf{x}_i || \\Theta \\textbf{x}_j]))}{\\sum_{k \\in \\mathcal{N}(i) \\cup \\{i\\}} exp(LeakyReLU(\\textbf{a}^T [\\Theta \\textbf{x}_i || \\Theta \\textbf{x}_k]))}\\]"},{"id":122,"pagetitle":"Graph Convolutional Layers","title":"GeometricFlux.GATConv","ref":"/geometricflux/stable/manual/graph_conv/#GeometricFlux.GATConv","content":" GeometricFlux.GATConv  —  Type GATConv(in => out, σ=identity; heads=1, concat=true,\n        init=glorot_uniform, bias=true, negative_slope=0.2) Graph attentional layer. Arguments in : The dimension of input features. out : The dimension of output features. bias::Bool : Keyword argument, whether to learn the additive bias. σ : Activation function. heads : Number attention heads concat : Concatenate layer output or not. If not, layer output is averaged. negative_slope::Real : Keyword argument, the parameter of LeakyReLU. Examples julia> GATConv(1024=>256, relu)\nGATConv(1024=>256, relu, heads=1, concat=true, LeakyReLU(λ=0.2))\n\njulia> GATConv(1024=>256, relu, heads=4)\nGATConv(1024=>1024, relu, heads=4, concat=true, LeakyReLU(λ=0.2))\n\njulia> GATConv(1024=>256, relu, heads=4, concat=false)\nGATConv(1024=>1024, relu, heads=4, concat=false, LeakyReLU(λ=0.2))\n\njulia> GATConv(1024=>256, relu, negative_slope=0.1f0)\nGATConv(1024=>256, relu, heads=1, concat=true, LeakyReLU(λ=0.1)) See also  WithGraph  for training layer with static graph. source Reference:  Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, Yoshua Bengio (2018)"},{"id":123,"pagetitle":"Graph Convolutional Layers","title":"Graph Attentional Layer v2","ref":"/geometricflux/stable/manual/graph_conv/#Graph-Attentional-Layer-v2","content":" Graph Attentional Layer v2 \\[\\textbf{x}_i' = \\alpha_{i,i} \\Theta \\textbf{x}_i + \\sum_{j \\in \\mathcal{N}(i)} \\alpha_{i,j} \\Theta \\textbf{x}_j\\] where the attention coefficient  $\\alpha_{i,j}$  can be calculated from \\[\\alpha_{i,j} = \\frac{exp(\\textbf{a}^T LeakyReLU(\\Theta [\\textbf{x}_i || \\textbf{x}_j]))}{\\sum_{k \\in \\mathcal{N}(i) \\cup \\{i\\}} exp(\\textbf{a}^T LeakyReLU(\\Theta [\\textbf{x}_i || \\textbf{x}_k]))}\\]"},{"id":124,"pagetitle":"Graph Convolutional Layers","title":"GeometricFlux.GATv2Conv","ref":"/geometricflux/stable/manual/graph_conv/#GeometricFlux.GATv2Conv","content":" GeometricFlux.GATv2Conv  —  Type GATv2Conv(in => out, σ=identity; heads=1, concat=true,\n          init=glorot_uniform, negative_slope=0.2) Graph attentional layer v2. Arguments in : The dimension of input features. out : The dimension of output features. σ : Activation function. heads : Number attention heads concat : Concatenate layer output or not. If not, layer output is averaged. negative_slope::Real : Keyword argument, the parameter of LeakyReLU. Examples julia> GATv2Conv(1024=>256, relu)\nGATv2Conv(1024=>256, relu, heads=1, concat=true, LeakyReLU(λ=0.2))\n\njulia> GATv2Conv(1024=>256, relu, heads=4)\nGATv2Conv(1024=>1024, relu, heads=4, concat=true, LeakyReLU(λ=0.2))\n\njulia> GATv2Conv(1024=>256, relu, heads=4, concat=false)\nGATv2Conv(1024=>1024, relu, heads=4, concat=false, LeakyReLU(λ=0.2))\n\njulia> GATv2Conv(1024=>256, relu, negative_slope=0.1f0)\nGATv2Conv(1024=>256, relu, heads=1, concat=true, LeakyReLU(λ=0.1)) See also  WithGraph  for training layer with static graph. source Reference:  Shaked Brody, Uri Alon, Eran Yahav (2022)"},{"id":125,"pagetitle":"Graph Convolutional Layers","title":"Gated Graph Convolution Layer","ref":"/geometricflux/stable/manual/graph_conv/#Gated-Graph-Convolution-Layer","content":" Gated Graph Convolution Layer \\[\\textbf{h}^{(0)}_i = \\textbf{x}_i || \\textbf{0} \\\\\n\\textbf{h}^{(l)}_i = GRU(\\textbf{h}^{(l-1)}_i, \\sum_{j \\in \\mathcal{N}(i)} \\Theta \\textbf{h}^{(l-1)}_j)\\] where  $\\textbf{h}^{(l)}_i$  denotes the  $l$ -th hidden variables passing through GRU. The dimension of input  $\\textbf{x}_i$  needs to be less or equal to  out ."},{"id":126,"pagetitle":"Graph Convolutional Layers","title":"GeometricFlux.GatedGraphConv","ref":"/geometricflux/stable/manual/graph_conv/#GeometricFlux.GatedGraphConv","content":" GeometricFlux.GatedGraphConv  —  Type GatedGraphConv([fg,] out, num_layers; aggr=+, init=glorot_uniform) Gated graph convolution layer. Arguments out : The dimension of output features. num_layers : The number of gated recurrent unit. aggr : An aggregate function applied to the result of message function.  + ,  - , * ,  / ,  max ,  min  and  mean  are available. Examples julia> GatedGraphConv(256, 4)\nGatedGraphConv((256 => 256)^4, aggr=+)\n\njulia> GatedGraphConv(256, 4, aggr=*)\nGatedGraphConv((256 => 256)^4, aggr=*) See also  WithGraph  for training layer with static graph. source Reference:  Yujia Li, Daniel Tarlow, Marc Brockschmidt, Richard Zemel (2016)"},{"id":127,"pagetitle":"Graph Convolutional Layers","title":"Edge Convolutional Layer","ref":"/geometricflux/stable/manual/graph_conv/#Edge-Convolutional-Layer","content":" Edge Convolutional Layer \\[\\textbf{x}_i' = \\sum_{j \\in \\mathcal{N}(i)} f_{\\Theta}(\\textbf{x}_i || \\textbf{x}_j - \\textbf{x}_i)\\] where  $f_{\\Theta}$  denotes a neural network parametrized by  $\\Theta$ ,  i.e. , a MLP."},{"id":128,"pagetitle":"Graph Convolutional Layers","title":"GeometricFlux.EdgeConv","ref":"/geometricflux/stable/manual/graph_conv/#GeometricFlux.EdgeConv","content":" GeometricFlux.EdgeConv  —  Type EdgeConv(nn; aggr=max) Edge convolutional layer. Arguments nn : A neural network (e.g. a Dense layer or a MLP). aggr : An aggregate function applied to the result of message function. + ,  max  and  mean  are available. Examples julia> EdgeConv(Dense(1024, 256, relu))\nEdgeConv(Dense(1024 => 256, relu), aggr=max)\n\njulia> EdgeConv(Dense(1024, 256, relu), aggr=+)\nEdgeConv(Dense(1024 => 256, relu), aggr=+) See also  WithGraph  for training layer with static graph. source Reference:  Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein, Justin M. Solomon (2019)"},{"id":129,"pagetitle":"Graph Convolutional Layers","title":"Graph Isomorphism Network","ref":"/geometricflux/stable/manual/graph_conv/#Graph-Isomorphism-Network","content":" Graph Isomorphism Network \\[\\textbf{x}_i' = f_{\\Theta}\\left((1 + \\varepsilon) \\cdot \\textbf{x}_i + \\sum_{j \\in \\mathcal{N}(i)} \\textbf{x}_j \\right)\\] where  $f_{\\Theta}$  denotes a neural network parametrized by  $\\Theta$ ,  i.e. , a MLP."},{"id":130,"pagetitle":"Graph Convolutional Layers","title":"GeometricFlux.GINConv","ref":"/geometricflux/stable/manual/graph_conv/#GeometricFlux.GINConv","content":" GeometricFlux.GINConv  —  Type GINConv(nn, [eps=0])\n\nGraph Isomorphism Network. Arguments nn : A neural network/layer. eps : Weighting factor. Examples julia> GINConv(Dense(1024, 256, relu))\nGINConv(Dense(1024 => 256, relu), ϵ=0.0)\n\njulia> GINConv(Dense(1024, 256, relu), 1.f-6)\nGINConv(Dense(1024 => 256, relu), ϵ=1.0e-6) See also  WithGraph  for training layer with static graph. source Reference:  Keyulu Xu, Weihua Hu, Jure Leskovec, Stefanie Jegelka (2019)"},{"id":131,"pagetitle":"Graph Convolutional Layers","title":"Crystal Graph Convolutional Network","ref":"/geometricflux/stable/manual/graph_conv/#Crystal-Graph-Convolutional-Network","content":" Crystal Graph Convolutional Network \\[\\textbf{x}_i' = \\textbf{x}_i + \\sum_{j \\in \\mathcal{N}(i)} \\sigma\\left( \\textbf{z}_{i,j} \\textbf{W}_f + \\textbf{b}_f \\right) \\odot \\text{softplus}\\left(\\textbf{z}_{i,j} \\textbf{W}_s + \\textbf{b}_s \\right)\\] where  $\\textbf{z}_{i,j} = [\\textbf{x}_i, \\textbf{x}_j}, \\textbf{e}_{i,j}]$  denotes the concatenation of node features, neighboring node features, and edge features. The operation  $\\odot$  represents elementwise multiplication, and  $\\sigma$  denotes the sigmoid function."},{"id":132,"pagetitle":"Graph Convolutional Layers","title":"GeometricFlux.CGConv","ref":"/geometricflux/stable/manual/graph_conv/#GeometricFlux.CGConv","content":" GeometricFlux.CGConv  —  Type CGConv((node_dim, edge_dim), init, bias=true) Crystal Graph Convolutional network. Uses both node and edge features. Arguments node_dim : Dimensionality of the input node features. Also is necessarily the output dimensionality. edge_dim : Dimensionality of the input edge features. init : Initialization algorithm for each of the weight matrices bias : Whether or not to learn an additive bias parameter. Examples julia> CGConv((128, 32))\nCGConv(node dim=128, edge dim=32) See also  WithGraph  for training layer with static graph. source Reference:  Tian Xie, Jeffrey C. Grossman (2018)"},{"id":135,"pagetitle":"Group Convolutional Layers","title":"Group Convolutional Layers","ref":"/geometricflux/stable/manual/group_conv/#Group-Convolutional-Layers","content":" Group Convolutional Layers"},{"id":136,"pagetitle":"Group Convolutional Layers","title":"$E(n)$ -equivariant Convolutional Layer","ref":"/geometricflux/stable/manual/group_conv/#E(n)-equivariant-Convolutional-Layer","content":" $E(n)$ -equivariant Convolutional Layer It employs message-passing scheme and can be defined by following functions: message function (Eq. 3 from paper):  $m_{ij} = \\phi_e(h_i^l, h_j^l, ||x_i^l - x_j^l||^2, a_{ij})$ aggregate (Eq. 5 from paper):  $m_i = \\sum_j m_{ij}$ update function (Eq. 6 from paper):  $h_i^{l+1} = \\phi_h(h_i^l, m_i)$ where  $h_i^l$  and  $h_j^l$  denotes the node feature for node  $i$  and  $j$ , respectively, in  $l$ -th layer, as well as  $x_i^l$  and  $x_j^l$  denote the positional feature for node  $i$  and  $j$ , respectively, in  $l$ -th layer.  $a_{ij}$  is the edge feature for edge  $(i,j)$ .  $\\phi_e$  and  $\\phi_h$  are neural network for edges and nodes."},{"id":137,"pagetitle":"Group Convolutional Layers","title":"GeometricFlux.EEquivGraphConv","ref":"/geometricflux/stable/manual/group_conv/#GeometricFlux.EEquivGraphConv","content":" GeometricFlux.EEquivGraphConv  —  Type EEquivGraphConv(in_dim=>out_dim, pos_dim, edge_dim; init=glorot_uniform) E(n)-equivariant graph neural network layer. Arguments in_dim::Int : node feature dimension. Data is assumed to be of the form [feature; coordinate], so  in_dim  must strictly be less than the dimension of the input vectors. out_dim : the output of the layer will have dimension  out_dim  + (dimension of input vector -  in_dim ). hidden_dim::Int : dimension of positional encoding. edge_dim::Int : dimension of edge feature. init : neural network initialization function. Examples julia> in_dim, out_dim, pos_dim = 3, 5, 2\n(3, 5, 2)\n\njulia> egnn = EEquivGraphConv(in_dim=>out_dim, pos_dim, in_dim)\nEEquivGraphConv(ϕ_edge=Chain(Dense(10 => 2), Dense(2 => 2)), ϕ_x=Chain(Dense(2 => 2), Dense(2 => 1; bias=false)), ϕ_h=Chain(Dense(5 => 2), Dense(2 => 5))) See also  WithGraph  for training layer with static graph and  EEquivGraphPE  for positional encoding. source Reference:  Victor Garcia Satorras, Emiel Hoogeboom, Max Welling (2021)"},{"id":140,"pagetitle":"Linear Algebra","title":"Linear Algebra","ref":"/geometricflux/stable/manual/linalg/#Linear-Algebra","content":" Linear Algebra"},{"id":141,"pagetitle":"Linear Algebra","title":"GraphSignals.degrees","ref":"/geometricflux/stable/manual/linalg/#GraphSignals.degrees","content":" GraphSignals.degrees  —  Function degrees(g, [T=eltype(g)]; dir=:out) Degree of each vertex. Return a vector which contains the degree of each vertex in graph  g . Arguments g : should be a adjacency matrix,  SimpleGraph ,  SimpleDiGraph  (from Graphs) or    SimpleWeightedGraph ,  SimpleWeightedDiGraph  (from SimpleWeightedGraphs). T : result element type of degree vector; default is the element type of  g  (optional). dir : direction of degree; should be  :in ,  :out , or  :both  (optional). Examples julia> using GraphSignals\n\njulia> m = [0 1 1; 1 0 0; 1 0 0];\n\njulia> GraphSignals.degrees(m)\n3-element Vector{Int64}:\n 2\n 1\n 1"},{"id":142,"pagetitle":"Linear Algebra","title":"GraphSignals.degree_matrix","ref":"/geometricflux/stable/manual/linalg/#GraphSignals.degree_matrix","content":" GraphSignals.degree_matrix  —  Function degree_matrix(g, [T=eltype(g)]; dir=:out, squared=false, inverse=false) Degree matrix of graph  g . Return a matrix which contains degrees of each vertex in its diagonal. The values other than diagonal are zeros. Arguments g : Should be a adjacency matrix,  FeaturedGraph ,  SimpleGraph ,  SimpleDiGraph  (from Graphs)   or  SimpleWeightedGraph ,  SimpleWeightedDiGraph  (from SimpleWeightedGraphs). T : The element type of result degree vector. The default type is the element type of  g . dir::Symbol : The way to calculate degree of a graph  g  regards its directions.   Should be  :in ,  :out , or  :both . squared::Bool : To return a squared degree vector or not. inverse::Bool : To return a inverse degree vector or not. Examples julia> using GraphSignals\n\njulia> m = [0 1 1; 1 0 0; 1 0 0];\n\njulia> GraphSignals.degree_matrix(m)\n3×3 LinearAlgebra.Diagonal{Int64, Vector{Int64}}:\n 2  ⋅  ⋅\n ⋅  1  ⋅\n ⋅  ⋅  1"},{"id":143,"pagetitle":"Linear Algebra","title":"GraphSignals.normalized_adjacency_matrix","ref":"/geometricflux/stable/manual/linalg/#GraphSignals.normalized_adjacency_matrix","content":" GraphSignals.normalized_adjacency_matrix  —  Function normalized_adjacency_matrix(g, [T=float(eltype(g))]; selfloop=false) Normalized adjacency matrix of graph  g , defined as \\[D^{-\\frac{1}{2}} \\tilde{A} D^{-\\frac{1}{2}}\\] where  $D$  is degree matrix and  $\\tilde{A}$  is adjacency matrix w/o self loop from  g . Arguments g : Should be a adjacency matrix,  FeaturedGraph ,  SimpleGraph ,  SimpleDiGraph  (from Graphs)   or  SimpleWeightedGraph ,  SimpleWeightedDiGraph  (from SimpleWeightedGraphs). T : The element type of result degree vector. The default type is the element type of  g . selfloop : Adding self loop to  $\\tilde{A}$  or not."},{"id":144,"pagetitle":"Linear Algebra","title":"Graphs.LinAlg.laplacian_matrix","ref":"/geometricflux/stable/manual/linalg/#Graphs.LinAlg.laplacian_matrix","content":" Graphs.LinAlg.laplacian_matrix  —  Function laplacian_matrix(g, [T=eltype(g)]; dir=:out) Laplacian matrix of graph  g , defined as \\[D - A\\] where  $D$  is degree matrix and  $A$  is adjacency matrix from  g . Arguments g : Should be a adjacency matrix,  FeaturedGraph ,  SimpleGraph ,  SimpleDiGraph  (from Graphs)   or  SimpleWeightedGraph ,  SimpleWeightedDiGraph  (from SimpleWeightedGraphs). T : The element type of result degree vector. The default type is the element type of  g . dir::Symbol : The way to calculate degree of a graph  g  regards its directions.   Should be  :in ,  :out , or  :both ."},{"id":145,"pagetitle":"Linear Algebra","title":"GraphSignals.normalized_laplacian","ref":"/geometricflux/stable/manual/linalg/#GraphSignals.normalized_laplacian","content":" GraphSignals.normalized_laplacian  —  Function normalized_laplacian(g, [T=float(eltype(g))]; dir=:both, selfloop=false) Normalized Laplacian matrix of graph  g , defined as \\[I - D^{-\\frac{1}{2}} \\tilde{A} D^{-\\frac{1}{2}}\\] where  $D$  is degree matrix and  $\\tilde{A}$  is adjacency matrix w/o self loop from  g . Arguments g : Should be a adjacency matrix,  FeaturedGraph ,  SimpleGraph ,  SimpleDiGraph  (from Graphs)   or  SimpleWeightedGraph ,  SimpleWeightedDiGraph  (from SimpleWeightedGraphs). T : The element type of result degree vector. The default type is the element type of  g . dir::Symbol : The way to calculate degree of a graph  g  regards its directions.   Should be  :in ,  :out , or  :both . selfloop::Bool : Adding self loop to  $\\tilde{A}$  or not."},{"id":146,"pagetitle":"Linear Algebra","title":"GraphSignals.scaled_laplacian","ref":"/geometricflux/stable/manual/linalg/#GraphSignals.scaled_laplacian","content":" GraphSignals.scaled_laplacian  —  Function scaled_laplacian(g, [T=float(eltype(g))]) Scaled Laplacien matrix of graph  g , defined as \\[\\hat{L} = \\frac{2}{\\lambda_{max}} \\tilde{L} - I\\] where  $\\tilde{L}$  is the normalized Laplacian matrix. Arguments g : Should be a adjacency matrix,  FeaturedGraph ,  SimpleGraph ,  SimpleDiGraph  (from Graphs)   or  SimpleWeightedGraph ,  SimpleWeightedDiGraph  (from SimpleWeightedGraphs). T : The element type of result degree vector. The default type is the element type of  g ."},{"id":147,"pagetitle":"Linear Algebra","title":"GraphSignals.random_walk_laplacian","ref":"/geometricflux/stable/manual/linalg/#GraphSignals.random_walk_laplacian","content":" GraphSignals.random_walk_laplacian  —  Function random_walk_laplacian(g, [T=float(eltype(g))]; dir=:out) Random walk normalized Laplacian matrix of graph  g , defined as \\[I - D^{-1} A\\] where  $D$  is degree matrix and  $A$  is adjacency matrix from  g . Arguments g : Should be a adjacency matrix,  FeaturedGraph ,  SimpleGraph ,  SimpleDiGraph  (from Graphs)   or  SimpleWeightedGraph ,  SimpleWeightedDiGraph  (from SimpleWeightedGraphs). T : The element type of result degree vector. The default type is the element type of  g . dir::Symbol : The way to calculate degree of a graph  g  regards its directions.   Should be  :in ,  :out , or  :both ."},{"id":148,"pagetitle":"Linear Algebra","title":"GraphSignals.signless_laplacian","ref":"/geometricflux/stable/manual/linalg/#GraphSignals.signless_laplacian","content":" GraphSignals.signless_laplacian  —  Function signless_laplacian(g, [T=eltype(g)]; dir=:out) Signless Laplacian matrix of graph  g , defined as \\[D + A\\] where  $D$  is degree matrix and  $A$  is adjacency matrix from  g . Arguments g : Should be a adjacency matrix,  FeaturedGraph ,  SimpleGraph ,  SimpleDiGraph  (from Graphs)   or  SimpleWeightedGraph ,  SimpleWeightedDiGraph  (from SimpleWeightedGraphs). T : The element type of result degree vector. The default type is the element type of  g . dir::Symbol : The way to calculate degree of a graph  g  regards its directions.   Should be  :in ,  :out , or  :both ."},{"id":151,"pagetitle":"Models","title":"Models","ref":"/geometricflux/stable/manual/models/#Models","content":" Models"},{"id":152,"pagetitle":"Models","title":"Autoencoders","ref":"/geometricflux/stable/manual/models/#Autoencoders","content":" Autoencoders"},{"id":153,"pagetitle":"Models","title":"Graph Autoencoder","ref":"/geometricflux/stable/manual/models/#Graph-Autoencoder","content":" Graph Autoencoder \\[Z = enc(X, A) \\\\\n\\hat{A} = \\sigma (ZZ^T)\\] where  $A$  denotes the adjacency matrix."},{"id":154,"pagetitle":"Models","title":"GeometricFlux.GAE","ref":"/geometricflux/stable/manual/models/#GeometricFlux.GAE","content":" GeometricFlux.GAE  —  Type GAE(enc, [σ=identity]) Graph autoencoder. Arguments enc : encoder. It can be any graph convolutional layer. σ : Activation function for decoder. Encoder is specified by user and decoder will be  InnerProductDecoder  layer. source Reference:  Thomas N. Kipf, Max Welling (2016)"},{"id":155,"pagetitle":"Models","title":"Variational Graph Autoencoder","ref":"/geometricflux/stable/manual/models/#Variational-Graph-Autoencoder","content":" Variational Graph Autoencoder \\[H = enc(X, A) \\\\\nZ_{\\mu}, Z_{logσ} = GCN_{\\mu}(H, A), GCN_{\\sigma}(H, A) \\\\\n\\hat{A} = \\sigma (ZZ^T)\\] where  $A$  denotes the adjacency matrix,  $X$  denotes node features."},{"id":156,"pagetitle":"Models","title":"GeometricFlux.VGAE","ref":"/geometricflux/stable/manual/models/#GeometricFlux.VGAE","content":" GeometricFlux.VGAE  —  Type VGAE(enc[, σ]) Variational graph autoencoder. Arguments enc : encoder. It can be any graph convolutional layer. Encoder is specified by user and decoder will be  InnerProductDecoder  layer. source Reference:  Thomas N. Kipf, Max Welling (2016)"},{"id":157,"pagetitle":"Models","title":"DeepSet","ref":"/geometricflux/stable/manual/models/#DeepSet","content":" DeepSet \\[Z = \\rho ( \\sum_{x_i \\in \\mathcal{V}} \\phi (x_i) )\\] where  $\\phi$  and  $\\rho$  denote two neural networks and  $x_i$  is the node feature for node  $i$ ."},{"id":158,"pagetitle":"Models","title":"GeometricFlux.DeepSet","ref":"/geometricflux/stable/manual/models/#GeometricFlux.DeepSet","content":" GeometricFlux.DeepSet  —  Type DeepSet(ϕ, ρ, aggr=+) Deep set model. Arguments ϕ : Neural network layer for each input before aggregation. ρ : Neural network layer after aggregation. aggr : An aggregate function applied to the result of message function.  + ,  - , * ,  / ,  max ,  min  and  mean  are available. Examples julia> ϕ = Dense(64, 16)\nDense(64 => 16)     # 1_040 parameters\n\njulia> ρ = Dense(16, 4)\nDense(16 => 4)      # 68 parameters\n\njulia> DeepSet(ϕ, ρ)\nDeepSet(Dense(64 => 16), Dense(16 => 4), aggr=+)\n\njulia> DeepSet(ϕ, ρ, aggr=max)\nDeepSet(Dense(64 => 16), Dense(16 => 4), aggr=max) See also  WithGraph  for training layer with static graph. source Reference:  Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, Alexander J Smola (2017)"},{"id":159,"pagetitle":"Models","title":"Special Layers","ref":"/geometricflux/stable/manual/models/#Special-Layers","content":" Special Layers"},{"id":160,"pagetitle":"Models","title":"Inner-product Decoder","ref":"/geometricflux/stable/manual/models/#Inner-product-Decoder","content":" Inner-product Decoder \\[\\hat{A} = \\sigma (ZZ^T)\\] where  $Z$  denotes the input matrix from encoder."},{"id":161,"pagetitle":"Models","title":"GeometricFlux.InnerProductDecoder","ref":"/geometricflux/stable/manual/models/#GeometricFlux.InnerProductDecoder","content":" GeometricFlux.InnerProductDecoder  —  Type InnerProductDecoder(σ) Inner-product decoder layer. Arguments σ : activation function. source Reference:  Thomas N. Kipf, Max Welling (2016)"},{"id":162,"pagetitle":"Models","title":"Variational Graph Encoder","ref":"/geometricflux/stable/manual/models/#Variational-Graph-Encoder","content":" Variational Graph Encoder \\[H = enc(X, A) \\\\\nZ_{\\mu}, Z_{logσ} = GCN_{\\mu}(H, A), GCN_{\\sigma}(H, A)\\]"},{"id":163,"pagetitle":"Models","title":"GeometricFlux.VariationalGraphEncoder","ref":"/geometricflux/stable/manual/models/#GeometricFlux.VariationalGraphEncoder","content":" GeometricFlux.VariationalGraphEncoder  —  Type VariationalGraphEncoder(nn, h_dim, z_dim) Variational graph encoder layer. Arguments nn : neural network. It can be any graph convolutional layer. h_dim : dimension of hidden layer. This should fit the output dimension of  nn . z_dim : dimension of latent variable layer. This will be parametrized into  μ  and  logσ . Encoder can be any graph convolutional layer. source Reference:  Thomas N. Kipf, Max Welling (2016)"},{"id":166,"pagetitle":"Neighborhood graphs","title":"Neighborhood Graphs","ref":"/geometricflux/stable/manual/neighborhood_graph/#Neighborhood-Graphs","content":" Neighborhood Graphs"},{"id":167,"pagetitle":"Neighborhood graphs","title":"GraphSignals.kneighbors_graph","ref":"/geometricflux/stable/manual/neighborhood_graph/#GraphSignals.kneighbors_graph","content":" GraphSignals.kneighbors_graph  —  Function kneighbors_graph(X, k, metric; include_self=false, weighted=false)\nkneighbors_graph(X, k; include_self=false, weighted=false) Generate  k -nearest neighborhood (kNN) graph from their node features. It returns a  FeaturedGraph  object, which contains a kNN graph and node features  X . Arguments X::AbstractMatrix : The feature matrix for each node with size  (feat_dim, num_nodes) . k : Number of nearest neighbor for each node in kNN graph. metric::Metric : Distance metric to measure distance between any two nodes. It aceepts distance objects from Distances. include_self::Bool : Whether distance from node to itself is included in nearest neighbor. weighted::Bool : Whether distance could be the edge weight in kNN graph. Usage julia> using GraphSignals, Distances\n\njulia> nf = rand(Float32, 10, 1024);\n\njulia> fg = kneighbors_graph(nf, 5)\nFeaturedGraph:\n\tDirected graph with (#V=1024, #E=5120) in adjacency matrix\n\tNode feature:\tℝ^10 <Matrix{Float32}>\n\njulia> fg = kneighbors_graph(nf, 5, Cityblock())\nFeaturedGraph:\n    Directed graph with (#V=1024, #E=5120) in adjacency matrix\n    Node feature:\tℝ^10 <Matrix{Float32}>\n\njulia> nf = rand(Float32[0, 1], 10, 1024);\n\njulia> fg = kneighbors_graph(nf, 5, Jaccard(); include_self=true)\nFeaturedGraph:\n    Directed graph with (#V=1024, #E=5120) in adjacency matrix\n    Node feature:\tℝ^10 <Matrix{Float32}>"},{"id":170,"pagetitle":"Graph Pooling Layers","title":"Graph Pooling Layers","ref":"/geometricflux/stable/manual/pool/#Graph-Pooling-Layers","content":" Graph Pooling Layers"},{"id":171,"pagetitle":"Graph Pooling Layers","title":"Global Pooling Layer","ref":"/geometricflux/stable/manual/pool/#Global-Pooling-Layer","content":" Global Pooling Layer"},{"id":172,"pagetitle":"Graph Pooling Layers","title":"GeometricFlux.GlobalPool","ref":"/geometricflux/stable/manual/pool/#GeometricFlux.GlobalPool","content":" GeometricFlux.GlobalPool  —  Type GlobalPool(aggr, dim...) Global pooling layer. It pools all features with  aggr  operation. Arguments aggr : An aggregate function applied to pool all features. source"},{"id":173,"pagetitle":"Graph Pooling Layers","title":"Local Pooling Layer","ref":"/geometricflux/stable/manual/pool/#Local-Pooling-Layer","content":" Local Pooling Layer"},{"id":174,"pagetitle":"Graph Pooling Layers","title":"GeometricFlux.LocalPool","ref":"/geometricflux/stable/manual/pool/#GeometricFlux.LocalPool","content":" GeometricFlux.LocalPool  —  Type LocalPool(aggr, cluster) Local pooling layer. It pools features with  aggr  operation accroding to  cluster . It is implemented with  scatter  operation. Arguments aggr : An aggregate function applied to pool all features. cluster : An index structure which indicates what features to aggregate with. source"},{"id":175,"pagetitle":"Graph Pooling Layers","title":"Top-k Pooling Layer","ref":"/geometricflux/stable/manual/pool/#Top-k-Pooling-Layer","content":" Top-k Pooling Layer"},{"id":176,"pagetitle":"Graph Pooling Layers","title":"GeometricFlux.TopKPool","ref":"/geometricflux/stable/manual/pool/#GeometricFlux.TopKPool","content":" GeometricFlux.TopKPool  —  Type TopKPool(adj, k, in_channel) Top-k pooling layer. Arguments adj : Adjacency matrix  of a graph. k : Top-k nodes are selected to pool together. in_channel : The dimension of input channel. source Reference:  Hongyang Gao, Shuiwang Ji (2019)"},{"id":179,"pagetitle":"Positional Encoding Layers","title":"Positional Encoding","ref":"/geometricflux/stable/manual/positional/#Positional-Encoding","content":" Positional Encoding"},{"id":180,"pagetitle":"Positional Encoding Layers","title":"Positional Encoding Methods","ref":"/geometricflux/stable/manual/positional/#Positional-Encoding-Methods","content":" Positional Encoding Methods"},{"id":181,"pagetitle":"Positional Encoding Layers","title":"GeometricFlux.AbstractPositionalEncoding","ref":"/geometricflux/stable/manual/positional/#GeometricFlux.AbstractPositionalEncoding","content":" GeometricFlux.AbstractPositionalEncoding  —  Type AbstractPositionalEncoding Abstract type of positional encoding for GNN. source"},{"id":182,"pagetitle":"Positional Encoding Layers","title":"GeometricFlux.RandomWalkPE","ref":"/geometricflux/stable/manual/positional/#GeometricFlux.RandomWalkPE","content":" GeometricFlux.RandomWalkPE  —  Type RandomWalkPE{K} Concrete type of positional encoding from random walk method. See also  positional_encode  for generating positional encoding. source"},{"id":183,"pagetitle":"Positional Encoding Layers","title":"GeometricFlux.LaplacianPE","ref":"/geometricflux/stable/manual/positional/#GeometricFlux.LaplacianPE","content":" GeometricFlux.LaplacianPE  —  Type LaplacianPE{K} Concrete type of positional encoding from graph Laplacian method. See also  positional_encode  for generating positional encoding. source"},{"id":184,"pagetitle":"Positional Encoding Layers","title":"GeometricFlux.positional_encode","ref":"/geometricflux/stable/manual/positional/#GeometricFlux.positional_encode","content":" GeometricFlux.positional_encode  —  Function positional_encode(RandomWalkPE{K}, A) Returns positional encoding (PE) of size  (K, N)  where N is node number. PE is generated by  K -step random walk over given graph. Arguments K::Int : First dimension of PE. A : Adjacency matrix of a graph. See also  RandomWalkPE  for random walk method. source positional_encode(LaplacianPE{K}, A) Returns positional encoding (PE) of size  (K, N)  where  N  is node number. PE is generated from eigenvectors of a graph Laplacian truncated by  K . Arguments K::Int : First dimension of PE. A : Adjacency matrix of a graph. See also  LaplacianPE  for graph Laplacian method. source"},{"id":185,"pagetitle":"Positional Encoding Layers","title":"Positional Encoding Layers","ref":"/geometricflux/stable/manual/positional/#Positional-Encoding-Layers","content":" Positional Encoding Layers"},{"id":186,"pagetitle":"Positional Encoding Layers","title":"$E(n)$ -equivariant Positional Encoding Layer","ref":"/geometricflux/stable/manual/positional/#E(n)-equivariant-Positional-Encoding-Layer","content":" $E(n)$ -equivariant Positional Encoding Layer It employs message-passing scheme and can be defined by following functions: message function:  $y_{ij}^l = (x_i^l - x_j^l)\\phi_x(m_{ij})$ aggregate:  $y_i^l = \\frac{1}{M} \\sum_{j \\in \\mathcal{N}(i)} y_{ij}^l$ update function:  $x_i^{l+1} = x_i^l + y_i^l$ where  $x_i^l$  and  $x_j^l$  denote the positional feature for node  $i$  and  $j$ , respectively, in  $l$ -th layer,  $\\phi_x$  is the neural network for positional encoding and  $m_{ij}$  is the edge feature for edge  $(i,j)$ .  $y_{ij}^l$  and  $y_i^l$  represent the encoded positional feature and aggregated positional feature, respectively, and  $M$  denotes number of neighbors of node  $i$ ."},{"id":187,"pagetitle":"Positional Encoding Layers","title":"GeometricFlux.EEquivGraphPE","ref":"/geometricflux/stable/manual/positional/#GeometricFlux.EEquivGraphPE","content":" GeometricFlux.EEquivGraphPE  —  Type EEquivGraphPE(in_dim=>out_dim; init=glorot_uniform, bias=true) E(n)-equivariant positional encoding layer. Arguments in_dim::Int : dimension of input positional feature. out_dim::Int :  dimension of output positional feature. init : neural network initialization function. bias::Bool : dimension of edge feature. Examples julia> in_dim_edge, out_dim = 2, 5\n(2, 5)\n\njulia> l = EEquivGraphPE(in_dim_edge=>out_dim)\nEEquivGraphPE(2 => 5) See also  EEquivGraphConv . source Reference:  Victor Garcia Satorras, Emiel Hoogeboom, Max Welling (2021)"},{"id":188,"pagetitle":"Positional Encoding Layers","title":"Learnable Structural Positional Encoding layer","ref":"/geometricflux/stable/manual/positional/#Learnable-Structural-Positional-Encoding-layer","content":" Learnable Structural Positional Encoding layer (WIP)"},{"id":189,"pagetitle":"Positional Encoding Layers","title":"GeometricFlux.LSPE","ref":"/geometricflux/stable/manual/positional/#GeometricFlux.LSPE","content":" GeometricFlux.LSPE  —  Type LSPE(fg, f_h, f_e, f_p, k; pe_method=RandomWalkPE) Learnable structural positional encoding layer.  LSPE  layer can be seen as a GNN layer warpped in  WithGraph . Arguments fg::FeaturedGraph : A given graoh for positional encoding. f_h::MessagePassing : Neural network layer for node update. f_e : Neural network layer for edge update. f_p : Neural network layer for positional encoding. k::Int : Dimension of positional encoding. pe_method : Initializer for positional encoding. source Reference:  Vijay Prakash Dwivedi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio, Xavier Bresson (2021)"},{"id":192,"pagetitle":"References","title":"References","ref":"/geometricflux/stable/references/#References","content":" References google_word2vec ,  Google code archive - long-term storage for google code project hosting. ,  Battaglia2018 Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, Caglar Gulcehre, Francis Song, Andrew Ballard, Justin Gilmer, George Dahl, Ashish Vaswani, Kelsey Allen, Charles Nash, Victoria Langston, Chris Dyer, Nicolas Heess, Daan Wierstra, Pushmeet Kohli, Matt Botvinick, Oriol Vinyals, Yujia Li, Razvan Pascanu,  Relational inductive biases, deep learning, and graph networks , ArXiv, 2018. Brody2022 Shaked Brody, Uri Alon, Eran Yahav,  HOW ATTENTIVE ARE GRAPH ATTENTION NETWORKS? ,  In International Conference on Learning Representations, 2022. Defferrard2016 Michaël Defferrard, Xavier Bresson, Pierre Vandergheynst, Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering,  In Proceedings of the 30th International Conference on Neural Information Processing Systems, 3844-3852, 2016. Curran Associates Inc.. Dwivedi2021 Vijay Prakash Dwivedi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio, Xavier Bresson,  Graph Neural Networks with Learnable Structural and Positional Representations , ArXiv, 2021. Gao2019 Hongyang Gao, Shuiwang Ji, Graph U-Nets,  In International Conference on Machine Learning, 2083-2092, 2019. Gilmer2017 Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, George E. Dahl,  Neural Message Passing for Quantum Chemistry ,  In ICML 2017, 2017. Grover2016 Aditya Grover, Jure Leskovec,  Node2vec: Scalable Feature Learning for Networks ,  In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 855-864, 2016. ACM. Hamilton2017 William L Hamilton, Rex Ying, Jure Leskovec, Inductive Representation Learning on Large Graphs,  In Proceedings of the 31st International Conference on Neural Information Processing Systems, 1025-1035, 2017. Curran Associates Inc.. Kipf2016 Thomas N. Kipf, Max Welling,  Variational Graph Auto-Encoders ,  In Neural Information Processing Systems, 2016. Kipf2017 Thomas N. Kipf, Max Welling,  Semi-Supervised Classification with Graph Convolutional Networks ,  In International Conference on Learning Representations, 2017. Li2016 Yujia Li, Daniel Tarlow, Marc Brockschmidt, Richard Zemel,  Gated Graph Sequence Neural Networks ,  In International Conference on Learning Representations, 2016. Morris2019 Christopher Morris, Martin Ritzert, Matthias Fey, William L. Hamilton, Jan Eric Lenssen, Gaurav Rattan, Martin Grohe,  Weisfeiler and Leman Go Neural: Higher-Order Graph Neural Networks ,  In Proceedings of the AAAI Conference on Artificial Intelligence, 4602-4609, 2019. Satorras2021 Victor Garcia Satorras, Emiel Hoogeboom, Max Welling,  E(n) Equivariant Graph Neural Networks ,  In Proceedings of the 38th International Conference on Machine Learning, 9323-9332, 2021. PMLR. GAT2018 Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, Yoshua Bengio,  Graph Attention Networks ,  In The 6th International Conference on Learning Representations, 2018. Wang2019 Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein, Justin M. Solomon,  Dynamic Graph CNN for Learning on Point Clouds , ACM Transactions on Graphics, 38, 1-12, 2019. Xie2018 Tian Xie, Jeffrey C. Grossman,  Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable Prediction of Material Properties , Physical Review Letters, 120, 145301, 2018. Xu2019 Keyulu Xu, Weihua Hu, Jure Leskovec, Stefanie Jegelka,  HOW POWERFUL ARE GRAPH NEURAL NETWORKS? ,  In International Conference on Learning Representations, 2019. Zaheer2017 Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, Alexander J Smola,  Deep Sets ,  In Advances in Neural Information Processing Systems, 2017. Curran Associates, Inc.."},{"id":197,"pagetitle":"Tutorials","title":"Tutorials","ref":"/geometricflux/stable/tutorials/#tutorials","content":" Tutorials"},{"id":198,"pagetitle":"Tutorials","title":"Examples","ref":"/geometricflux/stable/tutorials/#Examples","content":" Examples To begin with GeometricFlux, it is recommended to learn with following examples. Graph convolution networks (GCN) have been considered as the first step to graph neural networks (GNN). This example will go through how to train a vanilla GCN. Semi-supervised Learning with Graph Convolution Networks (GCN) In the tutorial for semi-supervised learning with GCN, variable graphs are provided to GNN from  FeaturedGraph , which contains a graph and node features. Each  FeaturedGraph  object can contain different graph and different node features, and can be train on the same GNN model. However, variable graph doesn&#39;t have the proper form of graph structure with respect to GNN layers and this lead to inefficient training&#x2F;inference process. Static graph strategy can be used to train a GNN model with the ... GCN with Static Graph Graph attention network (GAT) belongs to the message-passing network family, and it queries node feature over its neighbor features and generates result as layer output. Graph Attention Network Digits sum is a task of summing up digits in images or text. This example demonstrates summing up digits in arbitrary number of MNIST images. To accomplish such task, DeepSet model is suitable for this task. DeepSet model is excellent at the task which takes a set of objects and reduces them into single object. Predicting Digits Sum from DeepSet Model Variational Graph Autoencoder (VGAE) is a unsupervised generative model. It takes node features and graph structure and predicts the edge link in the graph. A link preidction task is defined for this model. Variational Graph Autoencoder Graph Embedding Through Node2vec Model Graph Embedding Through Node2vec Model"},{"id":201,"pagetitle":"Predicting Digits Sum from DeepSet Model","title":"Predicting Digits Sum from DeepSet Model","ref":"/geometricflux/stable/tutorials/examples/deepset/#Predicting-Digits-Sum-from-DeepSet-Model","content":" Predicting Digits Sum from DeepSet Model Digits sum is a task of summing up digits in images or text. This example demonstrates summing up digits in arbitrary number of MNIST images. To accomplish such task, DeepSet model is suitable for this task. DeepSet model is excellent at the task which takes a set of objects and reduces them into single object."},{"id":202,"pagetitle":"Predicting Digits Sum from DeepSet Model","title":"Step 1: Load MNIST Dataset","ref":"/geometricflux/stable/tutorials/examples/deepset/#Step-1:-Load-MNIST-Dataset","content":" Step 1: Load MNIST Dataset Since a DeepSet model predicts the summation from a set of images, we have to prepare training dataset composed of a random-sized set of images and a summed result. First, the whole dataset is loaded from MLDatasets.jl and then shuffled before generating training dataset. train_data, test_data = MNIST(:train), MNIST(:test)\ntrain_X, train_y = shuffle_data(train_data.features, train_data.targets)\ntest_X, test_y = shuffle_data(test_data.features, test_data.targets) The  generate_featuredgraphs  here generates a set of pairs which contains a  FeaturedGraph  and a summed number for prediction target. In a  FeaturedGraph , an arbitrary number of MNIST images are collected as node features and corresponding nodes are collected in a graph without edges. train_data = generate_featuredgraphs(train_X, train_y, num_train_examples, 1:train_max_length) num_train_examples  is the parameter for assigning how many training example to generate.  1:train_max_length  specifies the range of number of images to contained in one example."},{"id":203,"pagetitle":"Predicting Digits Sum from DeepSet Model","title":"Step 2: Build a DeepSet model","ref":"/geometricflux/stable/tutorials/examples/deepset/#Step-2:-Build-a-DeepSet-model","content":" Step 2: Build a DeepSet model A DeepSet takes a set of objects and outputs single object. To make a model accept a set of objects, the model input must be invariant to permutation. The DeepSet model is simply composed of two parts:  $\\phi$  network and  $\\rho$  network.  \\[Z = \\rho ( \\sum_{x_i \\in \\mathcal{V}} \\phi (x_i) )\\] $\\phi$  network embeds every images and they are summed up to be a single embedding. Permutation invariance comes from the use of summation. In general, a commutative binary operator can be used to reduce a set of embeddings into one embedding. Finally,  $\\rho$  network decodes the embedding to a number. ϕ = Chain(\n    Dense(args.input_dim, args.hidden_dims[1], tanh),\n    Dense(args.hidden_dims[1], args.hidden_dims[2], tanh),\n    Dense(args.hidden_dims[2], args.hidden_dims[3], tanh),\n)\nρ = Dense(args.hidden_dims[3], args.target_dim)\nmodel = DeepSet(ϕ, ρ) |> device"},{"id":204,"pagetitle":"Predicting Digits Sum from DeepSet Model","title":"Step 3: Loss Functions","ref":"/geometricflux/stable/tutorials/examples/deepset/#Step-3:-Loss-Functions","content":" Step 3: Loss Functions Mean absolute error is used as the loss function. Since the model outputs a  FeaturedGraph , the prediction is placed as a global feature in  FeaturedGraph . function model_loss(model, batch)\n    ŷ = vcat(map(x -> global_feature(model(x[1])), batch)...)\n    y = vcat(map(x -> x[2], batch)...)\n    return mae(ŷ, y)\nend"},{"id":205,"pagetitle":"Predicting Digits Sum from DeepSet Model","title":"Step 4: Training DeepSet Model","ref":"/geometricflux/stable/tutorials/examples/deepset/#Step-4:-Training-DeepSet-Model","content":" Step 4: Training DeepSet Model # optimizer\nopt = ADAM(args.η)\n\n# parameters\nps = Flux.params(model)\n\n# training\n@info \"Start Training, total $(args.epochs) epochs\"\nfor epoch = 1:args.epochs\n    @info \"Epoch $(epoch)\"\n\n    for batch in train_loader\n        batch = batch |> device\n        train_loss, back = Flux.pullback(() -> model_loss(model, batch), ps)\n        test_loss = model_loss(model, test_loader, device)\n        grad = back(1f0)\n        Flux.Optimise.update!(opt, ps, grad)\n    end\nend For a complete example, please check  examples/digitsum_deepsets.jl . This page was generated using  DemoCards.jl ."},{"id":208,"pagetitle":"Graph Attention Network","title":"Graph Attention Network","ref":"/geometricflux/stable/tutorials/examples/gat/#Graph-Attention-Network","content":" Graph Attention Network Graph attention network (GAT) belongs to the message-passing network family, and it queries node feature over its neighbor features and generates result as layer output."},{"id":209,"pagetitle":"Graph Attention Network","title":"Step 1: Load Dataset","ref":"/geometricflux/stable/tutorials/examples/gat/#Step-1:-Load-Dataset","content":" Step 1: Load Dataset We load dataset from Planetoid dataset. Here cora dataset is used. data = dataset[1].node_data\nX, y = data.features, onehotbatch(data.targets, 1:7)\ntrain_idx, test_idx = data.train_mask, data.val_mask"},{"id":210,"pagetitle":"Graph Attention Network","title":"Step 2: Batch up Features and Labels","ref":"/geometricflux/stable/tutorials/examples/gat/#Step-2:-Batch-up-Features-and-Labels","content":" Step 2: Batch up Features and Labels Just batch up features as usual. s, t = dataset[1].edge_index\ng = Graphs.Graph(dataset[1].num_nodes)\nfor (i, j) in zip(s, t)\n    Graphs.add_edge!(g, i, j)\nend\n\nadd_all_self_loops!(g)\nfg = FeaturedGraph(g)\ntrain_X, train_y = repeat(X, outer=(1,1,train_repeats)), repeat(y, outer=(1,1,train_repeats))\ntrain_loader = DataLoader((train_X, train_y), batchsize=batch_size, shuffle=true) Notably, self loop for all nodes are needed for GAT model."},{"id":211,"pagetitle":"Graph Attention Network","title":"Step 3: Build a GAT model","ref":"/geometricflux/stable/tutorials/examples/gat/#Step-3:-Build-a-GAT-model","content":" Step 3: Build a GAT model model = Chain(\n    WithGraph(fg, GATConv(args.input_dim=>args.hidden_dim, heads=args.heads)),\n    Dropout(0.6),\n    WithGraph(fg, GATConv(args.hidden_dim*args.heads=>args.target_dim, heads=args.heads, concat=false)),\n) |> device To note that a  GATConv  with  concat=true  will accumulates  heads  onto feature dimension. Thus, in the next layer, we should use  args.hidden_dim*args.heads . In the final layer of a network, a  GATConv  layer should be assigned with  concat=false  to average over each heads."},{"id":212,"pagetitle":"Graph Attention Network","title":"Step 4: Loss Functions and Accuracy","ref":"/geometricflux/stable/tutorials/examples/gat/#Step-4:-Loss-Functions-and-Accuracy","content":" Step 4: Loss Functions and Accuracy Cross entropy loss is used as loss function and accuracy is used to evaluate the model. model_loss(model, X, y, idx) =\n    logitcrossentropy(model(X)[:,idx,:], y[:,idx,:]) accuracy(model, X::AbstractArray, y::AbstractArray, idx) =\n    mean(onecold(softmax(cpu(model(X))[:,idx,:])) .== onecold(cpu(y)[:,idx,:])"},{"id":213,"pagetitle":"Graph Attention Network","title":"Step 5: Training GAT Model","ref":"/geometricflux/stable/tutorials/examples/gat/#Step-5:-Training-GAT-Model","content":" Step 5: Training GAT Model # ADAM optimizer\nopt = ADAM(args.η)\n\n# parameters\nps = Flux.params(model)\n\n# training\n@info \"Start Training, total $(args.epochs) epochs\"\nfor epoch = 1:args.epochs\n    @info \"Epoch $(epoch)\"\n\n    for (X, y) in train_loader\n        X, y, device_idx = X |> device, y |> device, train_idx |> device\n        loss, back = Flux.pullback(() -> model_loss(model, X, y, device_idx), ps)\n        train_acc = accuracy(model, train_loader, device, train_idx)\n        test_acc = accuracy(model, test_loader, device, test_idx)\n        grad = back(1f0)\n        Flux.Optimise.update!(opt, ps, grad)\n    end\nend For a complete example, please check  examples/gat.jl . This page was generated using  DemoCards.jl ."},{"id":216,"pagetitle":"GCN with Static Graph","title":"GCN with Static Graph","ref":"/geometricflux/stable/tutorials/examples/gcn_static_graph/#GCN-with-Static-Graph","content":" GCN with Static Graph In the tutorial for semi-supervised learning with GCN, variable graphs are provided to GNN from  FeaturedGraph , which contains a graph and node features. Each  FeaturedGraph  object can contain different graph and different node features, and can be train on the same GNN model. However, variable graph doesn't have the proper form of graph structure with respect to GNN layers and this lead to inefficient training/inference process. Static graph strategy can be used to train a GNN model with the same graph structure in GeometricFlux."},{"id":217,"pagetitle":"GCN with Static Graph","title":"Static Graph","ref":"/geometricflux/stable/tutorials/examples/gcn_static_graph/#Static-Graph","content":" Static Graph A static graph is given to a layer by  WithGraph  syntax.  WithGraph  wrap a  FeaturedGraph  object and a GNN layer as first and second arguments, respectively. fg = FeaturedGraph(graph)\nWithGraph(fg, GCNConv(1024=>256, relu)) This way, we can customize by binding different graph to certain layer and the layer will specialize graph to a required form. For example, a  GCNConv  layer requires graph in the form of normalized adjacency matrix. Once the graph is bound to a  GCNConv  layer, it transforms graph into normalized adjacency matrix and stores in  WithGraph  object. It accelerates training or inference by avoiding calculating transformations. The features in  FeaturedGraph  object in  WithGraph  are not used in any layer or model training or inference."},{"id":218,"pagetitle":"GCN with Static Graph","title":"Array in, Array out","ref":"/geometricflux/stable/tutorials/examples/gcn_static_graph/#Array-in,-Array-out","content":" Array in, Array out With this approach, a GNN layer accepts features in array. It takes an array as input and outputs array. Thus, a GNN layer wrapped with  WithGraph  should accept a feature array, just like regular deep learning model."},{"id":219,"pagetitle":"GCN with Static Graph","title":"Batch Learning","ref":"/geometricflux/stable/tutorials/examples/gcn_static_graph/#Batch-Learning","content":" Batch Learning Since features are in the form of array, they can be batched up for batched learning. We will demonstrate how to achieve these goals."},{"id":220,"pagetitle":"GCN with Static Graph","title":"Step 1: Load Dataset","ref":"/geometricflux/stable/tutorials/examples/gcn_static_graph/#Step-1:-Load-Dataset","content":" Step 1: Load Dataset Different from loading datasets in semi-supervised learning example, we use  alldata  for supervised learning here and  padding=true  is added in order to padding features from partial nodes to pseudo-full nodes. A padded features contains zeros in the nodes that are not supposed to be train on. data = dataset[1].node_data\nX, y = data.features, onehotbatch(data.targets, 1:7)\ntrain_idx, test_idx = data.train_mask, data.val_mask\ntrain_X, train_y = repeat(X, outer=(1,1,train_repeats)), repeat(y, outer=(1,1,train_repeats)) We need graph and node indices for training as well. s, t = dataset[1].edge_index\ng = Graphs.Graph(dataset[1].num_nodes)\nfor (i, j) in zip(s, t)\n    Graphs.add_edge!(g, i, j)\nend\nfg = FeaturedGraph(g)"},{"id":221,"pagetitle":"GCN with Static Graph","title":"Step 2: Batch up Features and Labels","ref":"/geometricflux/stable/tutorials/examples/gcn_static_graph/#Step-2:-Batch-up-Features-and-Labels","content":" Step 2: Batch up Features and Labels In order to make batch learning available, we separate graph and node features. We don't subgraph here. Node features are batched up by repeating node features here for demonstration, since planetoid dataset doesn't have batched settings. Different repeat numbers can be specified by  train_repeats  and  train_repeats . train_loader = DataLoader((train_X, train_y), batchsize=batch_size, shuffle=true)"},{"id":222,"pagetitle":"GCN with Static Graph","title":"Step 3: Build a GCN model","ref":"/geometricflux/stable/tutorials/examples/gcn_static_graph/#Step-3:-Build-a-GCN-model","content":" Step 3: Build a GCN model Here comes to building a GCN model. We build a model as building a regular Flux model but just wrap  GCNConv  layer with  WithGraph . model = Chain(\n    WithGraph(fg, GCNConv(args.input_dim=>args.hidden_dim, relu)),\n    Dropout(0.5),\n    WithGraph(fg, GCNConv(args.hidden_dim=>args.target_dim)),\n)"},{"id":223,"pagetitle":"GCN with Static Graph","title":"Step 4: Loss Functions and Accuracy","ref":"/geometricflux/stable/tutorials/examples/gcn_static_graph/#Step-4:-Loss-Functions-and-Accuracy","content":" Step 4: Loss Functions and Accuracy Almost all codes are the same as in semi-supervised learning example, except that indices for subgraphing are needed to get partial features out for calculating loss. l2norm(x) = sum(abs2, x)\n\nfunction model_loss(model, λ, X, y, idx)\n    loss = logitcrossentropy(model(X)[:,idx,:], y[:,idx,:])\n    loss += λ*sum(l2norm, Flux.params(model[1]))\n    return loss\nend And the accuracy measurement also needs indices. function accuracy(model, X::AbstractArray, y::AbstractArray, idx)\n    return mean(onecold(softmax(cpu(model(X))[:,idx,:])) .== onecold(cpu(y)[:,idx,:]))\nend\n\naccuracy(model, loader::DataLoader, device, idx) = mean(accuracy(model, X |> device, y |> device, idx) for (X, y) in loader)"},{"id":224,"pagetitle":"GCN with Static Graph","title":"Step 5: Training GCN Model","ref":"/geometricflux/stable/tutorials/examples/gcn_static_graph/#Step-5:-Training-GCN-Model","content":" Step 5: Training GCN Model train_loader, test_loader, fg, train_idx, test_idx = load_data(:cora, args.batch_size)\n\n# optimizer\nopt = ADAM(args.η)\n\n# parameters\nps = Flux.params(model)\n\n# training\ntrain_steps = 0\n@info \"Start Training, total $(args.epochs) epochs\"\nfor epoch = 1:args.epochs\n    @info \"Epoch $(epoch)\"\n\n    for (X, y) in train_loader\n        X, y, device_idx = X |> device, y |> device, train_idx |> device\n        grad = gradient(() -> model_loss(model, args.λ, X, y, device_idx), ps)\n        Flux.Optimise.update!(opt, ps, grad)\n        train_steps += 1\n    end\nend Now we could just train the GCN model directly! This page was generated using  DemoCards.jl ."},{"id":227,"pagetitle":"Graph Embedding Through Node2vec Model","title":"Graph Embedding Through Node2vec Model","ref":"/geometricflux/stable/tutorials/examples/graph_embedding/#graph_embedding","content":" Graph Embedding Through Node2vec Model"},{"id":228,"pagetitle":"Graph Embedding Through Node2vec Model","title":"Graph Embedding Through Node2vec Model","ref":"/geometricflux/stable/tutorials/examples/graph_embedding/#Graph-Embedding-Through-Node2vec-Model","content":" Graph Embedding Through Node2vec Model This page was generated using  DemoCards.jl ."},{"id":231,"pagetitle":"Semi-supervised Learning with Graph Convolution Networks (GCN)","title":"Semi-supervised Learning with Graph Convolution Networks (GCN)","ref":"/geometricflux/stable/tutorials/examples/semisupervised_gcn/#Semi-supervised-Learning-with-Graph-Convolution-Networks-(GCN)","content":" Semi-supervised Learning with Graph Convolution Networks (GCN) Graph convolution networks (GCN) have been considered as the first step to graph neural networks (GNN). This example will go through how to train a vanilla GCN."},{"id":232,"pagetitle":"Semi-supervised Learning with Graph Convolution Networks (GCN)","title":"Semi-supervised Learning in Graph Neural Networks","ref":"/geometricflux/stable/tutorials/examples/semisupervised_gcn/#Semi-supervised-Learning-in-Graph-Neural-Networks","content":" Semi-supervised Learning in Graph Neural Networks The semi-supervised learning task defines a learning by given features and labels for only partial nodes in a graph. We train features and labels for partial nodes, and test the model for another partial nodes in graph."},{"id":233,"pagetitle":"Semi-supervised Learning with Graph Convolution Networks (GCN)","title":"Node Classification task","ref":"/geometricflux/stable/tutorials/examples/semisupervised_gcn/#Node-Classification-task","content":" Node Classification task In this task, we learn a node classification task which learns a model to predict labels for each node in a graph. In GCN network, node features are given and the model outputs node labels."},{"id":234,"pagetitle":"Semi-supervised Learning with Graph Convolution Networks (GCN)","title":"Step 1: Load Dataset","ref":"/geometricflux/stable/tutorials/examples/semisupervised_gcn/#Step-1:-Load-Dataset","content":" Step 1: Load Dataset GeometricFlux provides planetoid dataset in  GeometricFlux.Datasets , which is provided by GraphMLDatasets. Planetoid dataset has three sub-datasets: Cora, Citeseer, PubMed. We demonstrate Cora dataset in this example.  traindata  provides the functionality for loading training data from various kinds of datasets. Dataset can be specified by the first argument, and the second for sub-datasets. using GeometricFlux.Datasets\n\ntrain_X, train_y = traindata(Planetoid(), :cora) traindata  returns a pre-defined training features and labels. These features are node features. train_X, train_y = map(x->Matrix(x), traindata(Planetoid(), :cora)) We can load graph from  graphdata , and the graph is preprocessed into  SimpleGraph  type, which is provided by Graphs. g = graphdata(Planetoid(), :cora)\ntrain_idx = train_indices(Planetoid(), :cora) We need node indices to index a subgraph from original graph.  train_indices  gives node indices for training."},{"id":235,"pagetitle":"Semi-supervised Learning with Graph Convolution Networks (GCN)","title":"Step 2: Wrapping Graph and Features into  FeaturedGraph","ref":"/geometricflux/stable/tutorials/examples/semisupervised_gcn/#Step-2:-Wrapping-Graph-and-Features-into-FeaturedGraph","content":" Step 2: Wrapping Graph and Features into  FeaturedGraph FeaturedGraph  is a container for holding a graph, node features, edge features and global features. It is provided by GraphSignals. To wrap graph and node features into  FeaturedGraph , graph  g  should be placed as the first argument and  nf  is to specify node features. using GraphSignals\n\nFeaturedGraph(g, nf=train_X) If we want to get a subgraph from a  FeaturedGraph  object, we call  subgraph  and provide node indices  train_idx  as second argument. subgraph(FeaturedGraph(g, nf=train_X), train_idx)"},{"id":236,"pagetitle":"Semi-supervised Learning with Graph Convolution Networks (GCN)","title":"Step 3: Build a GCN model","ref":"/geometricflux/stable/tutorials/examples/semisupervised_gcn/#Step-3:-Build-a-GCN-model","content":" Step 3: Build a GCN model A GCn model is composed of two layers of  GCNConv  and the activation function for first layer is  relu . In the middle, a  Dropout  layer is placed. We need a  GraphParallel  to integrate with regular Flux layer, and it specifies node features go to  node_layer=Dropout(0.5) . model = Chain(\n    GCNConv(input_dim=>hidden_dim, relu),\n    GraphParallel(node_layer=Dropout(0.5)),\n    GCNConv(hidden_dim=>target_dim),\n    node_feature,\n) Since the model input is a  FeaturedGraph  object, the model output a  FeaturedGraph  object as well. In the end of model, we get node features out from a  FeaturedGraph  object using  node_feature ."},{"id":237,"pagetitle":"Semi-supervised Learning with Graph Convolution Networks (GCN)","title":"Step 4: Loss Functions and Accuracy","ref":"/geometricflux/stable/tutorials/examples/semisupervised_gcn/#Step-4:-Loss-Functions-and-Accuracy","content":" Step 4: Loss Functions and Accuracy Then, since it is a node classification task, we define the model loss by  logitcrossentropy , and a L2 regularization is used. In the vanilla GCN, only first layer is applied to L2 regularization and can be adjusted by hyperparameter  λ . l2norm(x) = sum(abs2, x)\n\nfunction model_loss(model, λ, batch)\n    loss = 0.f0\n    for (x, y) in batch\n        loss += logitcrossentropy(model(x), y)\n        loss += λ*sum(l2norm, Flux.params(model[1]))\n    end\n    return loss\nend Accuracy for a batch and for data loader are provided. function accuracy(model, batch::AbstractVector)\n    return mean(mean(onecold(softmax(cpu(model(x)))) .== onecold(cpu(y))) for (x, y) in batch)\nend\n\naccuracy(model, loader::DataLoader, device) = mean(accuracy(model, batch |> device) for batch in loader)"},{"id":238,"pagetitle":"Semi-supervised Learning with Graph Convolution Networks (GCN)","title":"Step 5: Training GCN Model","ref":"/geometricflux/stable/tutorials/examples/semisupervised_gcn/#Step-5:-Training-GCN-Model","content":" Step 5: Training GCN Model We train the model with the same process as training a Flux model. train_loader, test_loader = load_data(:cora, args.batch_size)\n\n# optimizer\nopt = ADAM(args.η)\n    \n# parameters\nps = Flux.params(model)\n\n# training\ntrain_steps = 0\n@info \"Start Training, total $(args.epochs) epochs\"\nfor epoch = 1:args.epochs\n    @info \"Epoch $(epoch)\"\n\n    for batch in train_loader\n        grad = gradient(() -> model_loss(model, args.λ, batch |> device), ps)\n        Flux.Optimise.update!(opt, ps, grad)\n        train_steps += 1\n    end\nend So far, we complete a basic tutorial for training a GCN model! For the complete example, please check the script  examples/semisupervised_gcn.jl ."},{"id":239,"pagetitle":"Semi-supervised Learning with Graph Convolution Networks (GCN)","title":"Acceleration by Pre-computing Normalized Adjacency Matrix","ref":"/geometricflux/stable/tutorials/examples/semisupervised_gcn/#Acceleration-by-Pre-computing-Normalized-Adjacency-Matrix","content":" Acceleration by Pre-computing Normalized Adjacency Matrix The training process can be slow in this example. Since we place the graph and features together in  FeaturedGraph  object,  GCNConv  will need to compute a normalized adjacency matrix in the training process. This behavior will lead to long training time. We can accelerate training process by pre-compute normalized adjacency matrix for all  FeaturedGraph  objects. To do so, we can call the following function and it will compute normalized adjacency matrix for  fg  before training. This will reduce the training time. GraphSignals.normalized_adjacency_matrix!(fg) Since the normalized adjacency matrix is used in  GCNConv , we could pre-compute normalized adjacency matrix for it. If a layer doesn't require a normalized adjacency matrix, this step will lead to error. This page was generated using  DemoCards.jl ."},{"id":242,"pagetitle":"Variational Graph Autoencoder","title":"Variational Graph Autoencoder","ref":"/geometricflux/stable/tutorials/examples/vgae/#Variational-Graph-Autoencoder","content":" Variational Graph Autoencoder Variational Graph Autoencoder (VGAE) is a unsupervised generative model. It takes node features and graph structure and predicts the edge link in the graph. A link preidction task is defined for this model."},{"id":243,"pagetitle":"Variational Graph Autoencoder","title":"Step 1: Load Dataset","ref":"/geometricflux/stable/tutorials/examples/vgae/#Step-1:-Load-Dataset","content":" Step 1: Load Dataset We load dataset from Planetoid dataset. Here cora dataset is used. data = dataset[1].node_data\nX = data.features\ntrain_X = repeat(X, outer=(1, 1, train_repeats)) Notably, a link prediction task will output a graph in the form of adjacency matrix, so an adjacency matrix is needed as label for this task. s, t = dataset[1].edge_index\ng = Graphs.Graph(dataset[1].num_nodes)\nfor (i, j) in zip(s, t)\n    Graphs.add_edge!(g, i, j)\nend\nfg = FeaturedGraph(g)\nA = GraphSignals.adjacency_matrix(fg)"},{"id":244,"pagetitle":"Variational Graph Autoencoder","title":"Step 2: Batch up Features and Labels","ref":"/geometricflux/stable/tutorials/examples/vgae/#Step-2:-Batch-up-Features-and-Labels","content":" Step 2: Batch up Features and Labels Just batch up features as usual. loader = DataLoader((train_X, train_y), batchsize=batch_size, shuffle=true)"},{"id":245,"pagetitle":"Variational Graph Autoencoder","title":"Step 3: Build a VGAE model","ref":"/geometricflux/stable/tutorials/examples/vgae/#Step-3:-Build-a-VGAE-model","content":" Step 3: Build a VGAE model A VGAE model is composed of an encoder and a decoder. A  VariationalGraphEncoder  is used as an graph encoder and it contains a neural network to encode node features. A  InnerProductDecoder  is the decoder to predict links in a graph. Actually, it gives an adjacency matrix. Finally, we build  VGAE  model with encoder and decoder. encoder = VariationalGraphEncoder(\n    WithGraph(fg, GCNConv(args.input_dim=>args.h_dim, relu)),\n    WithGraph(fg, GCNConv(args.h_dim=>args.z_dim)),\n    WithGraph(fg, GCNConv(args.h_dim=>args.z_dim)),\n    args.z_dim\n)\n\ndecoder = InnerProductDecoder(σ)\n\nmodel = VGAE(encoder, decoder) |> device"},{"id":246,"pagetitle":"Variational Graph Autoencoder","title":"Step 4: Loss Functions and Link Prediction","ref":"/geometricflux/stable/tutorials/examples/vgae/#Step-4:-Loss-Functions-and-Link-Prediction","content":" Step 4: Loss Functions and Link Prediction Since a VGAE is a VAE model, its loss function is composed of a KL divergence and a log P. function kldivergence(model, X::AbstractArray{T}) where {T}\n    μ̂, logσ̂ = GeometricFlux.summarize(model.encoder, X)\n    return -T(0.5) * sum(one(T) .+ T(2).*logσ̂ .- μ̂.^2 .- exp.(T(2).*logσ̂))\nend\n\nfunction logp(model, X, Y)\n    Z = model.encoder(X)\n    return -logitbinarycrossentropy(model.decoder(Z), Y)\nend\n\nfunction model_loss(model, X, Y, β)\n    kl_q_p = kldivergence(model, X)\n    logp_y_z = logp(model, X, Y)\n    return -logp_y_z + β*kl_q_p\nend Precision metric is used to measure the existence of edges to be predicted from a model. function precision(model, X::AbstractArray, A::AbstractArray)\n    ŷ = cpu(Flux.flatten(model(X))) .≥ 0.5\n    y = cpu(Flux.flatten(A))\n    return sum(y .* ŷ) / sum(ŷ)\nend"},{"id":247,"pagetitle":"Variational Graph Autoencoder","title":"Step 5: Training VGAE Model","ref":"/geometricflux/stable/tutorials/examples/vgae/#Step-5:-Training-VGAE-Model","content":" Step 5: Training VGAE Model # ADAM optimizer\nopt = ADAM(args.η)\n\n# parameters\nps = Flux.params(model)\n\n# training\n@info \"Start Training, total $(args.epochs) epochs\"\nfor epoch = 1:args.epochs\n    @info \"Epoch $(epoch)\"\n\n    for (X, Â) in loader\n        X, Â = X |> device, Â |> device\n        loss, back = Flux.pullback(() -> model_loss(model, X, Â, args.β), ps)\n        prec = precision(model, loader, device)\n        grad = back(1f0)\n        Flux.Optimise.update!(opt, ps, grad)\n    end\nend For a complete example, please check  examples/vgae.jl . This page was generated using  DemoCards.jl ."},{"id":250,"pagetitle":"Home","title":"GraphSignals","ref":"/graphsignals/stable/#GraphSignals","content":" GraphSignals GraphSignals is aim to provide a general data structure, which is composed of a graph and graph signals, in order to support a graph neural network library, specifically, GeometricFlux.jl. The concept of graph is used ubiquitously in several fields, including computer science, social science, biological science and neural science. GraphSignals provides graph signals attached to a graph as a whole for training or inference a graph neural network. Some characteristics of this package are listed: Graph signals can be node features, edge features or global features, which are all general arrays. Graph Laplacian and related matrices are supported to calculated from a general data structure. Support graph representations from JuliaGraphs."},{"id":251,"pagetitle":"Home","title":"Example","ref":"/graphsignals/stable/#Example","content":" Example FeaturedGraph  supports various graph representations. It supports graph in adjacency matrix. julia> adjm = [0 1 1 1;\n               1 0 1 0;\n               1 1 0 1;\n               1 0 1 0];\n\njulia> fg = FeaturedGraph(adjm)\nFeaturedGraph(\n\tUndirected graph with (#V=4, #E=5) in adjacency matrix,\n) It also supports graph in adjacency list. julia> adjl = [\n               [2, 3, 4],\n               [1, 3],\n               [1, 2, 4],\n               [1, 3]\n               ];\n\njulia> fg = FeaturedGraph(adjl)\nFeaturedGraph(\n\tUndirected graph with (#V=4, #E=5) in adjacency matrix,\n) It supports  SimpleGraph  from LightGraphs and convert adjacency matrix into a Laplacian matrix as well. julia> using LightGraphs\n\njulia> N = 4\n4\n\njulia> ug = SimpleGraph(N)\n{4, 0} undirected simple Int64 graph\n\njulia> add_edge!(ug, 1, 2); add_edge!(ug, 1, 3); add_edge!(ug, 1, 4);\n\njulia> add_edge!(ug, 2, 3); add_edge!(ug, 3, 4);\n\njulia> fg = FeaturedGraph(ug)\nFeaturedGraph(\n\tUndirected graph with (#V=4, #E=5) in adjacency matrix,\n)\n\njulia> laplacian_matrix!(fg)\nFeaturedGraph(\n\tUndirected graph with (#V=4, #E=5) in Laplacian matrix,\n) Features can be attached to it. julia> N = 4\n4\n\njulia> E = 5\n5\n\njulia> nf = rand(3, N);\n\njulia> ef = rand(5, E);\n\njulia> gf = rand(7);\n\njulia> fg = FeaturedGraph(ug, nf=nf, ef=ef, gf=gf)\nFeaturedGraph(\n\tUndirected graph with (#V=4, #E=5) in adjacency matrix,\n\tNode feature:\tℝ^3 <Matrix{Float64}>,\n\tEdge feature:\tℝ^5 <Matrix{Float64}>,\n\tGlobal feature:\tℝ^7 <Vector{Float64}>,\n) If there are mismatched node features attached to it, a  DimensionMismatch  is throw out and hint user. julia> nf = rand(3, 7);\n\njulia> fg = FeaturedGraph(ug, nf=nf)\nERROR: DimensionMismatch(\"number of nodes must match between graph (4) and node features (7)\")"},{"id":254,"pagetitle":"FeaturedGraph","title":"FeaturedGraph","ref":"/graphsignals/stable/manual/featuredgraph/#FeaturedGraph","content":" FeaturedGraph"},{"id":255,"pagetitle":"FeaturedGraph","title":"Construct a FeaturedGraph and graph representations","ref":"/graphsignals/stable/manual/featuredgraph/#Construct-a-FeaturedGraph-and-graph-representations","content":" Construct a FeaturedGraph and graph representations A  FeaturedGraph  is aimed to represent a composition of graph representation and graph signals. A graph representation is required to construct a  FeaturedGraph  object. Graph representation can be accepted in several forms: adjacency matrix, adjacency list or graph representation provided from JuliaGraphs. julia> adj = [0 1 1;\n              1 0 1;\n              1 1 0]\n3×3 Matrix{Int64}:\n 0  1  1\n 1  0  1\n 1  1  0\n\njulia> FeaturedGraph(adj)\nFeaturedGraph(\n\tUndirected graph with (#V=3, #E=3) in adjacency matrix,\n) Currently,  SimpleGraph  and  SimpleDiGraph  from LightGraphs.jl,  SimpleWeightedGraph  and  SimpleWeightedDiGraph  from SimpleWeightedGraphs.jl, as well as  MetaGraph  and  MetaDiGraph  from MetaGraphs.jl are supported. If a graph representation is not given, a  FeaturedGraph  object will be regarded as a  NullGraph . A  NullGraph  object is just used as a special case of  FeaturedGraph  to represent a null object. julia> FeaturedGraph()\nNullGraph()"},{"id":256,"pagetitle":"FeaturedGraph","title":"FeaturedGraph constructors","ref":"/graphsignals/stable/manual/featuredgraph/#FeaturedGraph-constructors","content":" FeaturedGraph constructors Missing docstring. Missing docstring for  NullGraph() . Check Documenter's build log for details."},{"id":257,"pagetitle":"FeaturedGraph","title":"GraphSignals.FeaturedGraph","ref":"/graphsignals/stable/manual/featuredgraph/#GraphSignals.FeaturedGraph","content":" GraphSignals.FeaturedGraph  —  Type FeaturedGraph(g, [mt]; directed=:auto, nf, ef, gf, pf=nothing,\n    T, N, E, with_batch=false) A type representing a graph structure and storing also arrays that contain features associated to nodes, edges, and the whole graph. A  FeaturedGraph  can be constructed out of different objects  g  representing the connections inside the graph. When constructed from another featured graph  fg , the internal graph representation is preserved and shared. Arguments g : Data representing the graph topology. Possible type are An adjacency matrix. An adjacency list. A Graphs' graph, i.e.  SimpleGraph ,  SimpleDiGraph  from Graphs, or  SimpleWeightedGraph ,    SimpleWeightedDiGraph  from SimpleWeightedGraphs. An  AbstractFeaturedGraph  object. mt::Symbol : Matrix type for  g  in matrix form. if  graph  is in matrix form,  mt  is   recorded as one of  :adjm ,  :normedadjm ,  :laplacian ,  :normalized  or  :scaled . directed : It specify that direction of a graph. It can be  :auto ,  :directed  and    :undirected . Default value is  :auto , which infers direction automatically. nf : Node features. ef : Edge features. gf : Global features. pf : Positional features. If  nothing  is given, positional encoding is turned off. If an   array is given, positional encoding is assigned as given array. If  :auto  is given,   positional encoding is generated automatically for node features and  with_batch  is considered. T : It specifies the element type of graph. Default value is the element type of  g . N : Number of nodes for  g . E : Number of edges for  g . with_batch::Bool : Consider last dimension of all features as batch dimension. Usage using GraphSignals, CUDA\n\n# Construct from adjacency list representation\ng = [[2,3], [1,4,5], [1], [2,5], [2,4]]\nfg = FeaturedGraph(g)\n\n# Number of nodes and edges\nnv(fg)  # 5\nne(fg)  # 10\n\n# From a Graphs' graph\nfg = FeaturedGraph(erdos_renyi(100, 20))\n\n# Copy featured graph while also adding node features\nfg = FeaturedGraph(fg, nf=rand(100, 5))\n\n# Send to gpu\nfg = fg |> cu See also  graph ,  node_feature ,  edge_feature , and  global_feature . source"},{"id":258,"pagetitle":"FeaturedGraph","title":"Graph Signals","ref":"/graphsignals/stable/manual/featuredgraph/#Graph-Signals","content":" Graph Signals Graph signals is a collection of any signals defined on a graph. Graph signals can be the signals related to vertex, edges or graph itself. If a vertex signal is given, it is recorded as a node feature in  FeaturedGraph . A node feature is stored as the form of generic array, of which type is  AbstractArray . A node feature can be indexed by the node index, which is the same index for given graph. Node features can be optionally given in construction of a  FeaturedGraph . julia> fg = FeaturedGraph(adj, nf=rand(5, 3))\nFeaturedGraph(\n\tUndirected graph with (#V=3, #E=3) in adjacency matrix,\n\tNode feature:\tℝ^5 <Matrix{Float64}>,\n)\n\njulia> has_node_feature(fg)\ntrue\n\njulia> node_feature(fg)\n5×3 Matrix{Float64}:\n 0.534928  0.719566  0.952673\n 0.395465  0.268515  0.335446\n 0.79428   0.18623   0.454377\n 0.530675  0.402474  0.00920068\n 0.642556  0.719674  0.772497 Users check node/edge/graph features are available by  has_node_feature ,  has_edge_feature  and  has_global_feature , respectively, and fetch these features by  node_feature ,  edge_feature  and  global_feature ."},{"id":259,"pagetitle":"FeaturedGraph","title":"Getter methods","ref":"/graphsignals/stable/manual/featuredgraph/#Getter-methods","content":" Getter methods"},{"id":260,"pagetitle":"FeaturedGraph","title":"GraphSignals.graph","ref":"/graphsignals/stable/manual/featuredgraph/#GraphSignals.graph","content":" GraphSignals.graph  —  Function graph(fg) Get referenced graph in  fg . Arguments fg::AbstractFeaturedGraph : A concrete object of  AbstractFeaturedGraph  type. source"},{"id":261,"pagetitle":"FeaturedGraph","title":"GraphSignals.node_feature","ref":"/graphsignals/stable/manual/featuredgraph/#GraphSignals.node_feature","content":" GraphSignals.node_feature  —  Function node_feature(fg) Get node feature attached to  fg . Arguments fg::AbstractFeaturedGraph : A concrete object of  AbstractFeaturedGraph  type. source"},{"id":262,"pagetitle":"FeaturedGraph","title":"GraphSignals.edge_feature","ref":"/graphsignals/stable/manual/featuredgraph/#GraphSignals.edge_feature","content":" GraphSignals.edge_feature  —  Function edge_feature(fg) Get edge feature attached to  fg . Arguments fg::AbstractFeaturedGraph : A concrete object of  AbstractFeaturedGraph  type. source"},{"id":263,"pagetitle":"FeaturedGraph","title":"GraphSignals.global_feature","ref":"/graphsignals/stable/manual/featuredgraph/#GraphSignals.global_feature","content":" GraphSignals.global_feature  —  Function global_feature(fg) Get global feature attached to  fg . Arguments fg::AbstractFeaturedGraph : A concrete object of  AbstractFeaturedGraph  type. source"},{"id":264,"pagetitle":"FeaturedGraph","title":"Check methods","ref":"/graphsignals/stable/manual/featuredgraph/#Check-methods","content":" Check methods"},{"id":265,"pagetitle":"FeaturedGraph","title":"GraphSignals.has_graph","ref":"/graphsignals/stable/manual/featuredgraph/#GraphSignals.has_graph","content":" GraphSignals.has_graph  —  Function has_graph(fg) Check if  graph  is available or not for  fg . Arguments fg::AbstractFeaturedGraph : A concrete object of  AbstractFeaturedGraph  type. source"},{"id":266,"pagetitle":"FeaturedGraph","title":"GraphSignals.has_node_feature","ref":"/graphsignals/stable/manual/featuredgraph/#GraphSignals.has_node_feature","content":" GraphSignals.has_node_feature  —  Function has_node_feature(::AbstractFeaturedGraph) Check if  node_feature  is available or not for  fg . Arguments fg::AbstractFeaturedGraph : A concrete object of  AbstractFeaturedGraph  type. source"},{"id":267,"pagetitle":"FeaturedGraph","title":"GraphSignals.has_edge_feature","ref":"/graphsignals/stable/manual/featuredgraph/#GraphSignals.has_edge_feature","content":" GraphSignals.has_edge_feature  —  Function has_edge_feature(::AbstractFeaturedGraph) Check if  edge_feature  is available or not for  fg . Arguments fg::AbstractFeaturedGraph : A concrete object of  AbstractFeaturedGraph  type. source"},{"id":268,"pagetitle":"FeaturedGraph","title":"GraphSignals.has_global_feature","ref":"/graphsignals/stable/manual/featuredgraph/#GraphSignals.has_global_feature","content":" GraphSignals.has_global_feature  —  Function has_global_feature(::AbstractFeaturedGraph) Check if  global_feature  is available or not for  fg . Arguments fg::AbstractFeaturedGraph : A concrete object of  AbstractFeaturedGraph  type. source"},{"id":269,"pagetitle":"FeaturedGraph","title":"Graph properties","ref":"/graphsignals/stable/manual/featuredgraph/#Graph-properties","content":" Graph properties FeaturedGraph  is itself a graph, so we can query some graph properties from a  FeaturedGraph . julia> nv(fg)\n3\n\njulia> ne(fg)\n3\n\njulia> is_directed(fg)\nfalse Users can query number of vertex and number of edge by  nv  and  ne , respectively.  is_directed  checks if the underlying graph is a directed graph or not."},{"id":270,"pagetitle":"FeaturedGraph","title":"Graph-related APIs","ref":"/graphsignals/stable/manual/featuredgraph/#Graph-related-APIs","content":" Graph-related APIs Missing docstring. Missing docstring for  nv . Check Documenter's build log for details. Missing docstring. Missing docstring for  ne . Check Documenter's build log for details. Missing docstring. Missing docstring for  is_directed . Check Documenter's build log for details."},{"id":271,"pagetitle":"FeaturedGraph","title":"Pass  FeaturedGraph  to CUDA","ref":"/graphsignals/stable/manual/featuredgraph/#Pass-FeaturedGraph-to-CUDA","content":" Pass  FeaturedGraph  to CUDA Passing a  FeaturedGraph  to CUDA is easy. Just pipe a  FeaturedGraph  object to  gpu  provided by Flux. julia> using Flux\n\njulia> fg = fg |> gpu\nFeaturedGraph(\n\tUndirected graph with (#V=3, #E=3) in adjacency matrix,\n\tNode feature:\tℝ^5 <CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}>,\n)"},{"id":272,"pagetitle":"FeaturedGraph","title":"Linear algebra for  FeaturedGraph","ref":"/graphsignals/stable/manual/featuredgraph/#Linear-algebra-for-FeaturedGraph","content":" Linear algebra for  FeaturedGraph FeaturedGraph  supports the calculation of graph Laplacian matrix in inplace manner. julia> fg = FeaturedGraph(adj, nf=rand(5, 3))\nFeaturedGraph(\n\tUndirected graph with (#V=3, #E=3) in adjacency matrix,\n\tNode feature:\tℝ^5 <Matrix{Float64}>,\n)\n\njulia> laplacian_matrix!(fg)\nFeaturedGraph(\n\tUndirected graph with (#V=3, #E=3) in Laplacian matrix,\n\tNode feature:\tℝ^5 <Matrix{Float64}>,\n)\n\njulia> laplacian_matrix(fg)\n3×3 SparseArrays.SparseMatrixCSC{Int64, Int64} with 9 stored entries:\n -2   1   1\n  1  -2   1\n  1   1  -2 laplacian_matrix!  mutates the adjacency matrix into a Laplacian matrix in a  FeaturedGraph  object and the Laplacian matrix can be fetched by  laplacian_matrix . The Laplacian matrix is cached in a  FeaturedGraph  object and can be passed to a graph neural network model for training or inference. This way reduces the calculation overhead for Laplacian matrix during the training process. FeaturedGraph  supports not only Laplacian matrix, but also normalized Laplacian matrix and scaled Laplacian matrix calculation."},{"id":273,"pagetitle":"FeaturedGraph","title":"Inplaced linear algebraic APIs","ref":"/graphsignals/stable/manual/featuredgraph/#Inplaced-linear-algebraic-APIs","content":" Inplaced linear algebraic APIs Missing docstring. Missing docstring for  laplacian_matrix! . Check Documenter's build log for details. Missing docstring. Missing docstring for  normalized_laplacian! . Check Documenter's build log for details. Missing docstring. Missing docstring for  scaled_laplacian! . Check Documenter's build log for details."},{"id":274,"pagetitle":"FeaturedGraph","title":"Linear algebraic APIs","ref":"/graphsignals/stable/manual/featuredgraph/#Linear-algebraic-APIs","content":" Linear algebraic APIs Non-inplaced APIs returns a vector or a matrix directly. Missing docstring. Missing docstring for  adjacency_matrix . Check Documenter's build log for details. Missing docstring. Missing docstring for  degrees . Check Documenter's build log for details. Missing docstring. Missing docstring for  degree_matrix . Check Documenter's build log for details."},{"id":275,"pagetitle":"FeaturedGraph","title":"Graphs.LinAlg.laplacian_matrix","ref":"/graphsignals/stable/manual/featuredgraph/#Graphs.LinAlg.laplacian_matrix","content":" Graphs.LinAlg.laplacian_matrix  —  Function laplacian_matrix(g, [T=eltype(g)]; dir=:out) Laplacian matrix of graph  g , defined as \\[D - A\\] where  $D$  is degree matrix and  $A$  is adjacency matrix from  g . Arguments g : Should be a adjacency matrix,  FeaturedGraph ,  SimpleGraph ,  SimpleDiGraph  (from Graphs)   or  SimpleWeightedGraph ,  SimpleWeightedDiGraph  (from SimpleWeightedGraphs). T : The element type of result degree vector. The default type is the element type of  g . dir::Symbol : The way to calculate degree of a graph  g  regards its directions.   Should be  :in ,  :out , or  :both . source"},{"id":276,"pagetitle":"FeaturedGraph","title":"GraphSignals.normalized_laplacian","ref":"/graphsignals/stable/manual/featuredgraph/#GraphSignals.normalized_laplacian","content":" GraphSignals.normalized_laplacian  —  Function normalized_laplacian(g, [T=float(eltype(g))]; dir=:both, selfloop=false) Normalized Laplacian matrix of graph  g , defined as \\[I - D^{-\\frac{1}{2}} \\tilde{A} D^{-\\frac{1}{2}}\\] where  $D$  is degree matrix and  $\\tilde{A}$  is adjacency matrix w/o self loop from  g . Arguments g : Should be a adjacency matrix,  FeaturedGraph ,  SimpleGraph ,  SimpleDiGraph  (from Graphs)   or  SimpleWeightedGraph ,  SimpleWeightedDiGraph  (from SimpleWeightedGraphs). T : The element type of result degree vector. The default type is the element type of  g . dir::Symbol : The way to calculate degree of a graph  g  regards its directions.   Should be  :in ,  :out , or  :both . selfloop::Bool : Adding self loop to  $\\tilde{A}$  or not. source"},{"id":277,"pagetitle":"FeaturedGraph","title":"GraphSignals.scaled_laplacian","ref":"/graphsignals/stable/manual/featuredgraph/#GraphSignals.scaled_laplacian","content":" GraphSignals.scaled_laplacian  —  Function scaled_laplacian(g, [T=float(eltype(g))]) Scaled Laplacien matrix of graph  g , defined as \\[\\hat{L} = \\frac{2}{\\lambda_{max}} \\tilde{L} - I\\] where  $\\tilde{L}$  is the normalized Laplacian matrix. Arguments g : Should be a adjacency matrix,  FeaturedGraph ,  SimpleGraph ,  SimpleDiGraph  (from Graphs)   or  SimpleWeightedGraph ,  SimpleWeightedDiGraph  (from SimpleWeightedGraphs). T : The element type of result degree vector. The default type is the element type of  g . source"},{"id":280,"pagetitle":"Sparse graph strucutre","title":"Sparse graph Strucutre","ref":"/graphsignals/stable/manual/sparsegraph/#Sparse-graph-Strucutre","content":" Sparse graph Strucutre"},{"id":281,"pagetitle":"Sparse graph strucutre","title":"The need of graph structure","ref":"/graphsignals/stable/manual/sparsegraph/#The-need-of-graph-structure","content":" The need of graph structure Graph convolution can be classified into spectral-based graph convolution and spatial-based graph convolution. Spectral-based graph convolution relys on the algebaric operations, including  + ,  - ,  * , which are applied to features with graph structure. Spatial-based graph convolution relys on the indexing operations, since spatial-based graph convolution always indexes the neighbors of vertex. A graph structure can be use under two view point of a part of algebaric operations or an indexing structure. Message-passing neural network requires to access neighbor information for each vertex. Messages are passed from a vertex's neighbors to itself. A efficient indexing data structure is required to access incident edges or neighbor vertices from a specific vertex."},{"id":282,"pagetitle":"Sparse graph strucutre","title":"SparseGraph","ref":"/graphsignals/stable/manual/sparsegraph/#SparseGraph","content":" SparseGraph SparseGraph is implemented with sparse matrix. It is built on top of built-in sparse matrix,  SparseMatrixCSC .  SparseMatrixCSC  can be used as a regular matrix and performs algebaric operations with matrix or vectors. To benefit message-passing scheme, making a graph structure as an indexing structure is important. A well-designed indexing structure is made to leverage the sparse format of  SparseMatrixCSC , which is in CSC format. CSC format stores sparse matrix in a highly compressed manner. Comparing to traditional COO format, CSC format compresses the column indices into column pointers. All values are stored in single vector. If we want to index the sparse matrix  A , the row indices can be fetched by  rowvals[colptr[j]:(colptr[j+1]-1)]  and the non-zero values can be indexed by  nzvals[colptr[j]:(colptr[j+1]-1)] . The edge indices are designed in the same manner  edges[colptr[j]:(colptr[j+1]-1)] . This way matches the need of indexing neighbors of vertex. This makes neighbor indices or values close together. It takes  $O(1)$  to get negihbor indices, instead of searching neighbor in  $O(N)$ . Thus,  SparseGraph  takes both advantages of both algebaric operations and indexing operations."},{"id":283,"pagetitle":"Sparse graph strucutre","title":"Create  SparseGraph","ref":"/graphsignals/stable/manual/sparsegraph/#Create-SparseGraph","content":" Create  SparseGraph SparseGraph  accepts adjacency matrix, adjacency list, and almost all graphs defined in JuliaGraphs. julia> using GraphSignals, LightGraphs\n\njulia> ug = SimpleGraph(4)\n{4, 0} undirected simple Int64 graph\n\njulia> add_edge!(ug, 1, 2); add_edge!(ug, 1, 3); add_edge!(ug, 1, 4);\n\njulia> add_edge!(ug, 2, 3); add_edge!(ug, 3, 4);\n\njulia> sg = SparseGraph(ug)\nSparseGraph(#V=4, #E=5) The indexed adjacency list is a list of list strucutre. The inner list consists of a series of tuples containing a vertex index and a edge index, respectively."},{"id":284,"pagetitle":"Sparse graph strucutre","title":"Operate  SparseGraph  as graph","ref":"/graphsignals/stable/manual/sparsegraph/#Operate-SparseGraph-as-graph","content":" Operate  SparseGraph  as graph It supports basic graph APIs for querying graph information, including number of vertices  nv  and number of edges  ne . julia> is_directed(sg)\nfalse\n\njulia> nv(sg)\n4\n\njulia> ne(sg)\n5\n\njulia> eltype(sg)\nInt64\n\njulia> has_vertex(sg, 3)\ntrue\n\njulia> has_edge(sg, 1, 2)\ntrue We can compare two graph structure if they are equivalent or not. julia> adjm = [0 1 1 1; 1 0 1 0; 1 1 0 1; 1 0 1 0]\n4×4 Matrix{Int64}:\n 0  1  1  1\n 1  0  1  0\n 1  1  0  1\n 1  0  1  0\n\njulia> sg2 = SparseGraph(adjm, false)\nSparseGraph(#V=4, #E=5)\n\njulia> sg == sg2\ntrue We can also iterate over edges. julia> for (i, e) in edges(sg)\n           println(\"edge index: \", i, \", edge: \", e)\n       end\nedge index: 1, edge: (2, 1)\nedge index: 2, edge: (3, 1)\nedge index: 3, edge: (3, 2)\nedge index: 4, edge: (4, 1)\nedge index: 5, edge: (4, 3) Edge index is the index for each edge. It is used to index edge features."},{"id":285,"pagetitle":"Sparse graph strucutre","title":"Indexing operations","ref":"/graphsignals/stable/manual/sparsegraph/#Indexing-operations","content":" Indexing operations To get neighbors of a specified vertex,  neighbors  is used by passing a  SparseGraph  object and a vertex index. A vector of neighbor vertex index is returned. julia> neighbors(sg, 1)\n3-element view(::Vector{Int64}, 1:3) with eltype Int64:\n 2\n 3\n 4 To get incident edges of a specified vertex,  incident_edges  can be used and it will return edge indices. julia> incident_edges(sg, 1)\n3-element view(::Vector{Int64}, 1:3) with eltype Int64:\n 1\n 2\n 4 An edge index can be fetched by querying an edge, for example, edge  (1, 2)  and edge  (2, 1)  refers to the same edge with index  1 . julia> edge_index(sg, 1, 2)\n1\n\njulia> edge_index(sg, 2, 1)\n1 One can have the opportunity to index the underlying sparse matrix. julia> sg[1, 2]\n1\n\njulia> sg[2, 1]\n1"},{"id":286,"pagetitle":"Sparse graph strucutre","title":"Aggregate over neighbors","ref":"/graphsignals/stable/manual/sparsegraph/#Aggregate-over-neighbors","content":" Aggregate over neighbors In message-passing scheme, it is always to aggregate node features or edge feature from neighbors. For convention,  edge_scatter  and  neighbor_scatter  are used to apply aggregate operations over edge features or neighbor vertex features. The actual aggregation is supported by  scatter  operations. julia> nf = rand(10, 4);\n\njulia> neighbor_scatter(+, nf, sg)\n10×4 Matrix{Float64}:\n 1.54937   1.03974   1.72926  1.03974\n 1.38554   0.775991  1.34106  0.775991\n 1.13192   0.424888  1.34657  0.424888\n 2.23452   1.63226   2.436    1.63226\n 0.815662  0.718865  1.25237  0.718865\n 2.35763   1.42174   2.26442  1.42174\n 1.94051   1.44812   1.71694  1.44812\n 1.83641   1.89104   1.80857  1.89104\n 2.43027   1.92217   2.37003  1.92217\n 1.58177   1.16149   1.87467  1.16149 For example,  neighbor_scatter  aggregates node features  nf  via neighbors in graph  sg  by  +  operation. julia> ef = rand(9, 5);\n\njulia> edge_scatter(+, ef, sg)\n9×4 Matrix{Float64}:\n 2.22577  0.967172  1.92781   1.92628\n 1.4842   1.20605   2.30014   0.849819\n 2.20728  1.01527   0.899094  1.35062\n 1.09119  0.589925  1.62597   1.51175\n 1.42288  1.63764   1.23445   0.693258\n 1.57561  0.926591  1.72599   0.690108\n 1.68402  0.544808  1.58687   1.70676\n 1.10908  1.0898    1.05256   0.508157\n 2.33764  1.26419   1.87927   1.11151 Or,  edge_scatter  aggregates edge features  ef  via incident edges in graph  sg  by  +  operation."},{"id":287,"pagetitle":"Sparse graph strucutre","title":"SparseGraph  APIs","ref":"/graphsignals/stable/manual/sparsegraph/#SparseGraph-APIs","content":" SparseGraph  APIs"},{"id":288,"pagetitle":"Sparse graph strucutre","title":"GraphSignals.SparseGraph","ref":"/graphsignals/stable/manual/sparsegraph/#GraphSignals.SparseGraph","content":" GraphSignals.SparseGraph  —  Type SparseGraph(A, directed, [T]) A sparse graph structure represents by sparse matrix. A directed graph is represented by a sparse matrix, of which column index as source node index and row index as sink node index. Arguments A : Adjacency matrix. directed : If this is a directed graph or not. T : Element type for  SparseGraph . source"},{"id":289,"pagetitle":"Sparse graph strucutre","title":"Graphs.neighbors","ref":"/graphsignals/stable/manual/sparsegraph/#Graphs.neighbors","content":" Graphs.neighbors  —  Function neighbors(sg, i) Return the neighbors of vertex  i  in sparse graph  sg . Arguments sg::SparseGraph : sparse graph to query. i : vertex index. source"},{"id":290,"pagetitle":"Sparse graph strucutre","title":"GraphSignals.incident_edges","ref":"/graphsignals/stable/manual/sparsegraph/#GraphSignals.incident_edges","content":" GraphSignals.incident_edges  —  Function incident_edges(sg, i) Return the edges incident to vertex  i  in sparse graph  sg . Arguments sg::SparseGraph : sparse graph to query. i : vertex index. source Missing docstring. Missing docstring for  neighbor_scatter . Check Documenter's build log for details. Missing docstring. Missing docstring for  edge_scatter . Check Documenter's build log for details."},{"id":291,"pagetitle":"Sparse graph strucutre","title":"Internals","ref":"/graphsignals/stable/manual/sparsegraph/#Internals","content":" Internals In the design of  SparseGraph , it resolve the problem of indexing edge features. For a graph, edge is represented in  (i, j)  and edge features are considered as a matrix  ef  with edge number in its column. The problem is to unifiedly fetch corresponding edge feature  ef[:, k]  for edge  (i, j)  over directed and undirected graph. To resolve this issue, edge index is set to be the unique index for each edge. Further,  aggregate_index  is designed to generate indices for aggregating from neighbor nodes or incident edges. Conclusively, it provides the core operations needed in message-passing scheme."}]